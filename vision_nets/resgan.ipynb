{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. GAN with resblock backbone \n",
    "2. cGAN with projection\n",
    "\n",
    "\n",
    "## todo\n",
    "\n",
    "+ does projection cGAN work with continuous conditioned variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import spectral_norm\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as tv_transforms\n",
    "import torchvision.utils as tv_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import spectral_norm\n",
    "\n",
    "\n",
    "class ConditionalBatchNorm2d(nn.Module):\n",
    "    \"\"\" https://github.com/pytorch/pytorch/issues/8985#issuecomment-405080775\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(ConditionalBatchNorm2d, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.bn = nn.BatchNorm2d(num_features, affine=False)\n",
    "        self.embed = nn.Embedding(num_classes, num_features * 2)\n",
    "        self.embed.weight.data[:, :num_features].normal_(1, 0.02)  # Initialise scale at N(1, 0.02)\n",
    "        self.embed.weight.data[:, num_features:].zero_()  # Initialise bias at 0\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        out = self.bn(x)\n",
    "        gamma, beta = self.embed(c.view(-1)).chunk(2, 1)\n",
    "        out = gamma.view(-1, self.num_features, 1, 1) * out + beta.view(-1, self.num_features, 1, 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "def apply_sn(module, use_sn):\n",
    "    if use_sn:\n",
    "        return spectral_norm(module)\n",
    "    else:\n",
    "        return module\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, dilation=1, use_sn=False):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    module = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, bias=False, dilation=dilation)\n",
    "    return apply_sn(module, use_sn)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1, use_sn=False):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    module = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "    return apply_sn(module, use_sn)\n",
    "\n",
    "\n",
    "class UpsampleConv(nn.Module):\n",
    "    \"\"\" Upsample then Convolution. Better than ConvTranspose2d\n",
    "            https://distill.pub/2016/deconv-checkerboard/\n",
    "            https://github.com/pytorch/examples/blob/master/fast_neural_style/neural_style/transformer_net.py\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, scale_factor, use_sn=False):\n",
    "        super(UpsampleConv, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.pad  = torch.nn.ReflectionPad2d(kernel_size // 2)        \n",
    "        self.conv = apply_sn(torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride), use_sn)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, mode='nearest', scale_factor=self.scale_factor)\n",
    "        x = self.pad(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "def deconv3x3(in_planes, out_planes, stride=1, dilation=1, use_sn=False):\n",
    "    return UpsampleConv(in_planes, out_planes, kernel_size=3, stride=1, scale_factor=2, use_sn=use_sn)\n",
    "\n",
    "\n",
    "def deconv1x1(in_planes, out_planes, stride=1, dilation=1, use_sn=False):\n",
    "    return UpsampleConv(in_planes, out_planes, kernel_size=1, stride=1, scale_factor=2, use_sn=use_sn)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\" Pre-activation Residual Block\n",
    "            BN, nonlinearity, conv3x3, BN, nonlinearity, conv3x3\n",
    "\n",
    "    References:\n",
    "        https://arxiv.org/abs/1512.03385\n",
    "        http://torch.ch/blog/2016/02/04/resnets.html\n",
    "\n",
    "        ResBlock\n",
    "            https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "        WGAN-GP resnet architecture\n",
    "            https://github.com/igul222/improved_wgan_training/blob/fa66c574a54c4916d27c55441d33753dcc78f6bc/gan_cifar_resnet.py#L159\n",
    "            Generator: BN, ReLU, conv3x3, Tanh -> out\n",
    "            PreactivationResblock: https://github.com/igul222/improved_wgan_training/blob/master/gan_64x64.py\n",
    "        SNGAN/Projection cGAN architecture\n",
    "            https://github.com/pfnet-research/sngan_projection/blob/master/dis_models/resblocks.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, \n",
    "                 resample=None,\n",
    "                 norm_layer=nn.Identity,\n",
    "                 nonlinearity=nn.ReLU(),\n",
    "                 resblk_1st=False,\n",
    "                 use_sn=False):\n",
    "        \"\"\"\n",
    "            resample\n",
    "                \\in {None, 'up', 'dn'}\n",
    "            norm_layer\n",
    "                \\in {nn.Identity, nn.BatchNorm2d}\n",
    "            nonlinearity\n",
    "                either \n",
    "                    nn.ReLU(inplace=True)\n",
    "                    nn.LeakyReLU(slope=0.2)\n",
    "            resblk_1st\n",
    "                if True, no nonlinearity before first `conv_1`\n",
    "            use_sn\n",
    "                Apply spectral normalization for each linear/conv layers\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.identity_shortcut = (in_channels == out_channels and resample == None)\n",
    "        \n",
    "        if   resample == 'dn':\n",
    "            residual_conv_resample = conv3x3(in_channels, out_channels, 2, use_sn=use_sn)\n",
    "            shortcut_conv_resample = conv1x1(in_channels, out_channels, 2, use_sn=use_sn)\n",
    "        elif resample == 'up':\n",
    "            residual_conv_resample = deconv3x3(in_channels, out_channels, use_sn=use_sn)\n",
    "            shortcut_conv_resample = deconv1x1(in_channels, out_channels, use_sn=use_sn)\n",
    "        else:\n",
    "            residual_conv_resample = conv3x3(in_channels, out_channels, 1, use_sn=use_sn)\n",
    "            shortcut_conv_resample = conv1x1(in_channels, out_channels, 1, use_sn=use_sn)\n",
    "\n",
    "        self.residual_normalization_1 = norm_layer(in_channels)\n",
    "        self.residual_nonlinearity_1 = nn.Identity() if resblk_1st else nonlinearity\n",
    "        self.residual_conv_1 = residual_conv_resample\n",
    "        self.residual_normalization_2 = norm_layer(out_channels)\n",
    "        self.residual_nonlinearity_2 = nonlinearity\n",
    "        self.residual_conv_2 = conv3x3(out_channels, out_channels, use_sn=use_sn)\n",
    "        \n",
    "        if not self.identity_shortcut:\n",
    "            self.shortcut_normalization_1 = norm_layer(in_channels)\n",
    "            self.shortcut_conv_1 = shortcut_conv_resample\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        identity = x\n",
    "        \n",
    "        if self.identity_shortcut:\n",
    "            s = identity\n",
    "        else:\n",
    "            s = self.shortcut_normalization_1(identity)\n",
    "            s = self.shortcut_conv_1(s)\n",
    "            \n",
    "        x = self.residual_normalization_1(x)\n",
    "        x = self.residual_nonlinearity_1(x)\n",
    "        x = self.residual_conv_1(x)\n",
    "        x = self.residual_normalization_2(x)\n",
    "        x = self.residual_nonlinearity_2(x)\n",
    "        x = self.residual_conv_2(x)\n",
    "        \n",
    "        return x + s\n",
    "    \n",
    "    \n",
    "    \n",
    "class ConditionalResidualBlock(ResidualBlock):\n",
    "    \"\"\" Residual block w/ categorical conditional BatchNorm2d\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 resample=None,\n",
    "                 norm_layer=nn.BatchNorm2d,\n",
    "                 nonlinearity=nn.ReLU(inplace=True),\n",
    "                 resblk_1st=False,\n",
    "                 use_sn=False):\n",
    "        \"\"\"\n",
    "            norm_layer\n",
    "                initialize w/ num_features\n",
    "        \"\"\"\n",
    "        \n",
    "        super(ConditionalResidualBlock, self).__init__(\n",
    "            in_channels, out_channels,\n",
    "            resample = resample,\n",
    "            norm_layer = norm_layer,\n",
    "            nonlinearity = nonlinearity,\n",
    "            resblk_1st = resblk_1st,\n",
    "            use_sn = use_sn)\n",
    "        \n",
    "    def forward(self, x, c):\n",
    "        \n",
    "        identity = x\n",
    "        \n",
    "        if self.identity_shortcut:\n",
    "            s = identity\n",
    "        else:\n",
    "            s = self.shortcut_normalization_1(identity, c)\n",
    "            s = self.shortcut_conv_1(s)\n",
    "            \n",
    "        x = self.residual_normalization_1(x, c)\n",
    "        x = self.residual_nonlinearity_1(x)\n",
    "        x = self.residual_conv_1(x)\n",
    "        x = self.residual_normalization_2(x, c)\n",
    "        x = self.residual_nonlinearity_2(x)\n",
    "        x = self.residual_conv_2(x)\n",
    "        \n",
    "        return x + s\n",
    "        \n",
    "        \n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, conv_channels, conv_upsample,\n",
    "                 resblk_cls = ResidualBlock,\n",
    "                 norm_layer = nn.BatchNorm2d,\n",
    "                 dim_z = 128,\n",
    "                 im_channels = 3):\n",
    "        \"\"\"\n",
    "            conv_channels\n",
    "                [1024, 1024, 512, 256, 128, 64]\n",
    "                     c1    c2   c3   c4   c5\n",
    "            conv_upsample\n",
    "                4x4 -> 128x128    [True, True, True, True, True]\n",
    "                4x4 -> 64x64      [True, True, True, True]\n",
    "                4x4 -> 32x32      [True, True, True]\n",
    "            im_channnels\n",
    "                3 for color image\n",
    "                1 for grayscale image\n",
    "            num_classes\n",
    "                if not None, use conditional batchnorm\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        n_convs = len(conv_channels) - 1\n",
    "        assert(n_convs > 0)\n",
    "        assert(n_convs == len(conv_upsample))\n",
    "        \n",
    "        self.n_convs = n_convs\n",
    "        self.bottom_width = 4\n",
    "        self.nonlinearity = nn.ReLU()\n",
    "        \n",
    "        self.linear = nn.Linear(dim_z, (self.bottom_width**2) * conv_channels[0])\n",
    "        \n",
    "        for i in range(n_convs):\n",
    "            upsample = conv_upsample[i]\n",
    "            self.add_module(\n",
    "                f'residual_block_{i}',\n",
    "                resblk_cls(conv_channels[i], conv_channels[i+1],\n",
    "                           resample = \"up\" if upsample else None,\n",
    "                           norm_layer = norm_layer,\n",
    "                           nonlinearity = self.nonlinearity))\n",
    "        \n",
    "        self.normalization_final = norm_layer(conv_channels[-1])\n",
    "        self.conv_final = conv3x3(conv_channels[-1], im_channels)\n",
    "        self.nonlinearity_final = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # bottom_width = 4\n",
    "        # conv_channels = [1024, 1024, 512, 256, 128, 64]\n",
    "        # conv_upsample = [True, True, True, True, True]\n",
    "        # im_channnels = 3\n",
    "        #\n",
    "        # 128\n",
    "        x = self.linear(x)\n",
    "        x = x.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n",
    "        # 1024x4x4\n",
    "        for i in range(self.n_convs):\n",
    "            x = getattr(self, f'residual_block_{i}')(x)\n",
    "        # 64x128x128\n",
    "        x = self.normalization_final(x)\n",
    "        x = self.nonlinearity(x)\n",
    "        x = self.conv_final(x)\n",
    "        x = self.nonlinearity_final(x)\n",
    "        # 3x128x128\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class ConditionalGenerator(Generator):\n",
    "    \n",
    "    def __init__(self, conv_channels, conv_upsample, num_classes,\n",
    "                 dim_z = 128,\n",
    "                 im_channels = 3):\n",
    "        \"\"\"\n",
    "            norm_layer = lambda num_features: ConditionalBatchNorm2d(num_features, num_classes)\n",
    "        \"\"\" \n",
    "        resblk_cls = ConditionalResidualBlock\n",
    "        norm_layer = lambda num_features: ConditionalBatchNorm2d(num_features, num_classes)\n",
    "        \n",
    "        super(ConditionalGenerator, self).__init__(conv_channels, conv_upsample,\n",
    "            resblk_cls = resblk_cls,\n",
    "            norm_layer = norm_layer,\n",
    "            dim_z = dim_z,\n",
    "            im_channels = im_channels)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        \"\"\" x    batch_size x im_channels x h x w\n",
    "            c    batch_size\n",
    "        \"\"\"\n",
    "        c = c.view(-1)\n",
    "        #\n",
    "        # 128\n",
    "        x = self.linear(x)\n",
    "        x = x.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n",
    "        # 1024x4x4\n",
    "        for i in range(self.n_convs):\n",
    "            x = getattr(self, f'residual_block_{i}')(x, c)\n",
    "        # 64x128x128\n",
    "        x = self.normalization_final(x, c)\n",
    "        x = self.nonlinearity(x)\n",
    "        x = self.conv_final(x)\n",
    "        x = self.nonlinearity_final(x)\n",
    "        # 3x128x128\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, conv_channels, conv_dnsample, use_sn=True):\n",
    "        \"\"\"\n",
    "            conv_channels\n",
    "                [3, 64, 128, 256, 512, 1024, 1024]\n",
    "                  c1  c2   c3   c4   c5    c6\n",
    "            conv_dnsample\n",
    "                128x128 -> 4x4    [True, True, True, True, True, False]\n",
    "                64x64 -> 4x4      [True, True, True, True]\n",
    "                32x32 -> 4x4      [True, True, True]\n",
    "                \n",
    "        Projection cGAN \n",
    "            conv_channels = [3, 64, 128, 256, 512, 1024, 1024]\n",
    "            conv_dnsample = [True, True, True, True, True, False]\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        n_convs = len(conv_channels) - 1\n",
    "        assert(n_convs > 0)\n",
    "        assert(n_convs == len(conv_dnsample))\n",
    "        \n",
    "        self.nonlinearity = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.residual_blocks = nn.Sequential()\n",
    "        for i in range(n_convs):\n",
    "            downsample = conv_dnsample[i]\n",
    "            self.residual_blocks.add_module(f'residual_block_{i}',\n",
    "                  ResidualBlock(conv_channels[i], conv_channels[i+1],\n",
    "                                resample = \"dn\" if downsample else None,\n",
    "                                norm_layer = nn.Identity,\n",
    "                                nonlinearity = self.nonlinearity,\n",
    "                                resblk_1st = True if i == 0 else False,\n",
    "                                use_sn = use_sn))\n",
    "\n",
    "        self.linear = apply_sn(nn.Linear(conv_channels[-1], 1), use_sn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # conv_channels = [3, 64, 128, 256, 512, 1024, 1024]\n",
    "        # conv_dnsample = [True, True, True, True, True]\n",
    "        #\n",
    "        # 3x128x128\n",
    "        x = self.residual_blocks(x)\n",
    "        # 1024x4x4\n",
    "        x = self.nonlinearity(x)\n",
    "        x = torch.sum(x, dim=(2,3))   # (global sum pooling)\n",
    "        # 1024\n",
    "        x = self.linear(x)\n",
    "        # 1\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class ConditionalDiscriminator(Discriminator):\n",
    "    \n",
    "    def __init__(self, conv_channels, conv_dnsample, num_classes, use_sn=True):\n",
    "        super(ConditionalDiscriminator, self).__init__(conv_channels, conv_dnsample, use_sn=use_sn)\n",
    "        \n",
    "        self.c_embed = apply_sn(nn.Embedding(num_classes, conv_channels[-1]), use_sn)\n",
    "        \n",
    "    def forward(self, x, c):\n",
    "        \"\"\" x    batch_size x im_channels x h x w\n",
    "            c    batch_size\n",
    "        \"\"\"\n",
    "        c = c.view(-1)\n",
    "        # conv_channels = [3, 64, 128, 256, 512, 1024, 1024]\n",
    "        # conv_dnsample = [True, True, True, True, True]\n",
    "        #\n",
    "        # 3x128x128\n",
    "        x = self.residual_blocks(x)\n",
    "        # 1024x4x4\n",
    "        x = self.nonlinearity(x)\n",
    "        x = torch.sum(x, dim=(2,3))   # (global sum pooling)\n",
    "        # 1024\n",
    "        \n",
    "        # sigmoid^-1(p(real/fake|x,c)) =\n",
    "        #     log(p_data(x)/p_model(x)) + \n",
    "        #     log(p_data(c|x)/p_model(c|x))\n",
    "        x = self.linear(x) + \\\n",
    "            torch.sum(self.c_embed(c) * x, dim=1, keepdim=True)\n",
    "        # 1\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "## resblock\n",
    "##############################\n",
    "\n",
    "blk = ResidualBlock(16, 32, resblk_1st=True)\n",
    "\n",
    "# print(blk)\n",
    "\n",
    "x = torch.empty((32, 16, 28, 28))\n",
    "out = blk(x)\n",
    "\n",
    "print(x.shape, out.shape)\n",
    "\n",
    "##############################\n",
    "## G\n",
    "##############################\n",
    "\n",
    "dim_z = 50\n",
    "conv_channels = [256, 256, 128, 64]\n",
    "conv_upsample = [True, True, True]\n",
    "G = Generator(conv_channels, conv_upsample, dim_z = dim_z, im_channels = 1)\n",
    "\n",
    "# print(G)\n",
    "\n",
    "x = torch.empty((32, dim_z))\n",
    "out = G(x)\n",
    "\n",
    "print(x.shape, out.shape)\n",
    "\n",
    "##############################\n",
    "## D\n",
    "##############################\n",
    "\n",
    "conv_channels = [3, 64, 128, 256]\n",
    "conv_dnsample = [True, True, True]\n",
    "D = Discriminator(conv_channels, conv_dnsample)\n",
    "\n",
    "# print(D)\n",
    "\n",
    "x = torch.empty((50, 3, 32, 32))\n",
    "out = D(x)\n",
    "\n",
    "print(x.shape, out.shape)\n",
    "\n",
    "\n",
    "##############################\n",
    "## conditional D\n",
    "##############################\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "conv_channels = [3, 64, 128, 256]\n",
    "conv_dnsample = [True, True, True]\n",
    "D = ConditionalDiscriminator(conv_channels, conv_dnsample, num_classes)\n",
    "\n",
    "\n",
    "x = torch.empty((50, 3, 32, 32))\n",
    "c = torch.empty((50, 1), dtype=torch.long).random_(0, 10)\n",
    "out = D(x, c)\n",
    "\n",
    "print(x.shape, out.shape)\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "## conditional G\n",
    "##############################\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "dim_z = 50\n",
    "conv_channels = [256, 256, 128, 64]\n",
    "conv_upsample = [True, True, True]\n",
    "G = ConditionalGenerator(conv_channels, conv_dnsample, num_classes, dim_z=dim_z)\n",
    "\n",
    "\n",
    "x = torch.empty((50, dim_z))\n",
    "c = torch.empty((50,), dtype=torch.long).random_(0, 10)\n",
    "out = G(x, c)\n",
    "\n",
    "print(x.shape, out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'resgan_jpt'\n",
    "seed = 0\n",
    "gpu_id = '2'\n",
    "image_size = 32\n",
    "dim_z = 50\n",
    "batch_size = 32\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "n_epochs = 20\n",
    "figure_root = 'figures/resgan_jpt'\n",
    "log_interval = 100\n",
    "use_sn = False\n",
    "n_workers = 8\n",
    "batch_size = 32\n",
    "\n",
    "conditional_G = False\n",
    "conditional_D = True\n",
    "num_classes = 10\n",
    "include_c_in_z= True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(figure_root, exist_ok=True)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_id\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    MNIST(\n",
    "        root='./data', download=True, train=True, transform=tv_transforms.Compose([\n",
    "            tv_transforms.Resize(image_size),\n",
    "            tv_transforms.ToTensor(),\n",
    "            tv_transforms.Normalize((0.5,), (0.5,)),\n",
    "        ])),\n",
    "    batch_size=batch_size, shuffle=True, num_workers=n_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_channels = [256, 256, 128, 64]\n",
    "conv_upsample = [True, True, True]\n",
    "\n",
    "conv_channels = [1, 64, 128, 256]\n",
    "conv_dnsample = [True, True, True]\n",
    "\n",
    "if conditional_G:\n",
    "    G = ConditionalGenerator(conv_channels, conv_upsample, num_classes=10, dim_z=dim_z, im_channels=1).to(device)\n",
    "else:\n",
    "    G = Generator(conv_channels, conv_upsample, dim_z=dim_z, im_channels = 1).to(device)\n",
    "    \n",
    "if conditional_D:\n",
    "    D = ConditionalDiscriminator(conv_channels, conv_dnsample, num_classes, use_sn=use_sn).to(device)\n",
    "else:\n",
    "    D = Discriminator(conv_channels, conv_dnsample, use_sn=use_sn).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "fixed_z = torch.randn(100, dim_z, device=device)\n",
    "fixed_c = torch.arange(10).repeat(10).to(device)\n",
    "\n",
    "if include_c_in_z:\n",
    "    fixed_z[:, -10:] = F.one_hot(fixed_c, 10)\n",
    "\n",
    "# label flipping helps with training G!\n",
    "real_label = 0\n",
    "fake_label = 1\n",
    "\n",
    "optimizerD = torch.optim.Adam(D.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(G.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    for it, (x_real, c_real) in enumerate(train_loader):\n",
    "\n",
    "        # batch_size for last batch might be different ...\n",
    "        batch_size = x_real.size(0)\n",
    "        real_labels = torch.full((batch_size, 1), real_label, device=device)\n",
    "        fake_labels = torch.full((batch_size, 1), fake_label, device=device)\n",
    "\n",
    "        ##############################################################\n",
    "        # Update Discriminator: Maximize E[log(D(x))] + E[log(1 - D(G(z)))]\n",
    "        ##############################################################\n",
    "\n",
    "        D.zero_grad()\n",
    "\n",
    "        # a minibatch of samples from data distribution\n",
    "        x_real, c_real = x_real.to(device), c_real.to(device)\n",
    "\n",
    "        y = D(x_real, c_real) if conditional_D else D(x_real)\n",
    "        loss_D_real = criterion(y, real_labels)\n",
    "        loss_D_real.backward()\n",
    "\n",
    "        # a minibatch of samples from the model distribution\n",
    "        z = torch.randn(batch_size, dim_z, device=device)\n",
    "        c_fake = torch.empty(batch_size, dtype=torch.long).random_(0, num_classes).to(device)\n",
    "\n",
    "        if include_c_in_z:\n",
    "            z[:, -num_classes:] = F.one_hot(c_fake, num_classes)\n",
    "\n",
    "        x_fake = G(z, c_fake) if conditional_G else G(z)\n",
    "        # https://github.com/pytorch/examples/issues/116\n",
    "        # If we do not detach, then, although x_fake is not needed for gradient update of D,\n",
    "        #   as a consequence of backward pass which clears all the variables in the graph\n",
    "        #   graph for G will not be available for gradient update of G\n",
    "        # Also for performance considerations, detaching x_fake will prevent computing \n",
    "        #   gradients for parameters in G\n",
    "        y = D(x_fake.detach(), c_fake) if conditional_D else D(x_fake.detach())\n",
    "        loss_D_fake = criterion(y, fake_labels)\n",
    "        loss_D_fake.backward()\n",
    "\n",
    "        loss_D = loss_D_real + loss_D_fake\n",
    "\n",
    "        optimizerD.step()\n",
    "\n",
    "        ##############################################################\n",
    "        # Update Generator: Minimize E[log(1 - D(G(z)))] => Maximize E[log(D(G(z))))]\n",
    "        ##############################################################\n",
    "\n",
    "        G.zero_grad()\n",
    "\n",
    "        y = D(x_fake, c_fake) if conditional_D else D(x_fake)\n",
    "        loss_G = criterion(y, real_labels)\n",
    "        loss_G.backward()\n",
    "\n",
    "        optimizerG.step()\n",
    "\n",
    "        ##############################################################\n",
    "        # write/print\n",
    "        ##############################################################\n",
    "        \n",
    "        loss_D = loss_D.item()\n",
    "        loss_G = loss_G.item()\n",
    "        loss_total = loss_D + loss_G\n",
    "        \n",
    "        global_step = epoch*len(train_loader)+it\n",
    "        \n",
    "        if it % log_interval == log_interval-1:\n",
    "            print(f'[{epoch+1}/{n_epochs}][{it+1}/{len(train_loader)}]'\n",
    "                f'loss: {loss_total:.4}\\t'\n",
    "                f'loss_D: {loss_D:.4}\\t'\n",
    "                f'loss_G: {loss_G:.4}')\n",
    "            x_fake = G(fixed_z, fixed_c) if conditional_G else G(fixed_z)\n",
    "            tv_utils.save_image(x_fake.detach(),\n",
    "                os.path.join(figure_root,\n",
    "                    f'{model_name}_epoch={epoch}_it={it}.png'), nrow=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:misc_impl] *",
   "language": "python",
   "name": "conda-env-misc_impl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
