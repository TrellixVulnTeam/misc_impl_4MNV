
@inproceedings{rabinAdaptiveColorTransfer2014,
	title = {Adaptive color transfer with relaxed optimal transport},
	doi = {10.1109/ICIP.2014.7025983},
	abstract = {This paper studies the problem of color transfer between images using optimal transport techniques. While being a generic framework to handle statistics properly, it is also known to be sensitive to noise and outliers, and is not suitable for direct application to images without additional postprocessing regularization to remove artifacts. To tackle these issues, we propose to directly deal with the regularity of the transport map and the spatial consistency of the reconstruction. Our approach is based on the relaxed and regularized discrete optimal transport method of [1]. We extend this work by (i) modeling the spatial distribution of colors within the image domain and (ii) tuning automatically the relaxation parameters. Experiments on real images demonstrate the capacity of our model to adapt itself to the considered data.},
	eventtitle = {2014 {IEEE} International Conference on Image Processing ({ICIP})},
	pages = {4852--4856},
	booktitle = {2014 {IEEE} International Conference on Image Processing ({ICIP})},
	author = {Rabin, J. and Ferradans, S. and Papadakis, N.},
	date = {2014-10},
	keywords = {tbr},
	file = {Rabin et al_2014_Adaptive color transfer with relaxed optimal transport.pdf:/Users/wpq/Dropbox (MIT)/zotero/Rabin et al_2014_Adaptive color transfer with relaxed optimal transport.pdf:application/pdf}
}

@article{ozairWassersteinDependencyMeasure2019,
	title = {Wasserstein Dependency Measure for Representation Learning},
	url = {http://arxiv.org/abs/1903.11780},
	abstract = {Mutual information maximization has emerged as a powerful learning objective for unsupervised representation learning obtaining state-of-the-art performance in applications such as object recognition, speech recognition, and reinforcement learning. However, such approaches are fundamentally limited since a tight lower bound of mutual information requires sample size exponential in the mutual information. This limits the applicability of these approaches for prediction tasks with high mutual information, such as in video understanding or reinforcement learning. In these settings, such techniques are prone to overfit, both in theory and in practice, and capture only a few of the relevant factors of variation. This leads to incomplete representations that are not optimal for downstream tasks. In this work, we empirically demonstrate that mutual information-based representation learning approaches do fail to learn complete representations on a number of designed and real-world tasks. To mitigate these problems we introduce the Wasserstein dependency measure, which learns more complete representations by using the Wasserstein distance instead of the {KL} divergence in the mutual information estimator. We show that a practical approximation to this theoretically motivated solution, constructed using Lipschitz constraint techniques from the {GAN} literature, achieves substantially improved results on tasks where incomplete representations are a major challenge.},
	journaltitle = {{arXiv}:1903.11780 [cs, stat]},
	author = {Ozair, Sherjil and Lynch, Corey and Bengio, Yoshua and Oord, Aaron van den and Levine, Sergey and Sermanet, Pierre},
	urldate = {2020-06-23},
	date = {2019-03-27},
	eprinttype = {arxiv},
	eprint = {1903.11780},
	keywords = {read},
	file = {Ozair et al_2019_Wasserstein Dependency Measure for Representation Learning.pdf:/Users/wpq/Dropbox (MIT)/zotero/Ozair et al_2019_Wasserstein Dependency Measure for Representation Learning.pdf:application/pdf}
}

@article{feydyInterpolatingOptimalTransport2018,
	title = {Interpolating between Optimal Transport and {MMD} using Sinkhorn Divergences},
	url = {http://arxiv.org/abs/1810.08278},
	abstract = {Comparing probability distributions is a fundamental problem in data sciences. Simple norms and divergences such as the total variation and the relative entropy only compare densities in a point-wise manner and fail to capture the geometric nature of the problem. In sharp contrast, Maximum Mean Discrepancies ({MMD}) and Optimal Transport distances ({OT}) are two classes of distances between measures that take into account the geometry of the underlying space and metrize the convergence in law. This paper studies the Sinkhorn divergences, a family of geometric divergences that interpolates between {MMD} and {OT}. Relying on a new notion of geometric entropy, we provide theoretical guarantees for these divergences: positivity, convexity and metrization of the convergence in law. On the practical side, we detail a numerical scheme that enables the large scale application of these divergences for machine learning: on the {GPU}, gradients of the Sinkhorn loss can be computed for batches of a million samples.},
	journaltitle = {{arXiv}:1810.08278 [math, stat]},
	author = {Feydy, Jean and Séjourné, Thibault and Vialard, François-Xavier and Amari, Shun-ichi and Trouvé, Alain and Peyré, Gabriel},
	urldate = {2020-10-14},
	date = {2018-10-18},
	eprinttype = {arxiv},
	eprint = {1810.08278},
	keywords = {tbr},
	file = {Feydy et al_2018_Interpolating between Optimal Transport and MMD using Sinkhorn Divergences.pdf:/Users/wpq/Dropbox (MIT)/zotero/Feydy et al_2018_Interpolating between Optimal Transport and MMD using Sinkhorn Divergences.pdf:application/pdf}
}

@article{janatiSpatioTemporalAlignmentsOptimal2020,
	title = {Spatio-Temporal Alignments: Optimal transport through space and time},
	abstract = {Comparing data deﬁned over space and time is notoriously hard. It involves quantifying both spatial and temporal variability while taking into account the chronological structure of the data. Dynamic Time Warping ({DTW}) computes a minimal cost alignment between time series that preserves the chronological order but is inherently blind to spatiotemporal shifts. In this paper, we propose Spatio-Temporal Alignments ({STA}), a new diﬀerentiable formulation of {DTW} that captures spatial and temporal variability. Spatial diﬀerences between time samples are captured using regularized Optimal transport. While temporal alignment cost exploits a smooth variant of {DTW} called soft-{DTW}. We show how smoothing {DTW} leads to alignment costs that increase quadratically with time shifts. The costs are expressed using an unbalanced Wasserstein distance to cope with observations that are not probabilities. Experiments on handwritten letters and brain imaging data conﬁrm our theoretical ﬁndings and illustrate the eﬀectiveness of {STA} as a dissimilarity for spatio-temporal data.},
	pages = {9},
	author = {Janati, Hicham and Cuturi, Marco and Gramfort, Alexandre},
	date = {2020},
	langid = {english},
	keywords = {tbr},
	file = {Janati et al_2020_Spatio-Temporal Alignments - Optimal transport through space and time.pdf:/Users/wpq/Dropbox (MIT)/zotero/Janati et al_2020_Spatio-Temporal Alignments - Optimal transport through space and time.pdf:application/pdf}
}

@article{cuturiSinkhornDistancesLightspeed2013,
	title = {Sinkhorn Distances: Lightspeed Computation of Optimal Transportation Distances},
	url = {http://arxiv.org/abs/1306.0895},
	shorttitle = {Sinkhorn Distances},
	abstract = {Optimal transportation distances are a fundamental family of parameterized distances for histograms. Despite their appealing theoretical properties, excellent performance in retrieval tasks and intuitive formulation, their computation involves the resolution of a linear program whose cost is prohibitive whenever the histograms' dimension exceeds a few hundreds. We propose in this work a new family of optimal transportation distances that look at transportation problems from a maximum-entropy perspective. We smooth the classical optimal transportation problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn-Knopp's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transportation solvers. We also report improved performance over classical optimal transportation distances on the {MNIST} benchmark problem.},
	journaltitle = {{arXiv}:1306.0895 [stat]},
	author = {Cuturi, Marco},
	urldate = {2020-10-14},
	date = {2013-06-04},
	eprinttype = {arxiv},
	eprint = {1306.0895},
	keywords = {tbr},
	file = {Cuturi_2013_Sinkhorn Distances - Lightspeed Computation of Optimal Transportation Distances.pdf:/Users/wpq/Dropbox (MIT)/zotero/Cuturi_2013_Sinkhorn Distances - Lightspeed Computation of Optimal Transportation Distances.pdf:application/pdf}
}

@article{frognerLearningWassersteinLoss2015a,
	title = {Learning with a Wasserstein Loss},
	url = {http://arxiv.org/abs/1506.05439},
	abstract = {Learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions. In this paper we develop a loss function for multi-label learning, based on the Wasserstein distance. The Wasserstein distance provides a natural notion of dissimilarity for probability measures. Although optimizing with respect to the exact Wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed. We describe an efficient learning algorithm based on this regularization, as well as a novel extension of the Wasserstein distance from probability measures to unnormalized measures. We also describe a statistical learning bound for the loss. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the Yahoo Flickr Creative Commons dataset, outperforming a baseline that doesn't use the metric.},
	journaltitle = {{arXiv}:1506.05439 [cs, stat]},
	author = {Frogner, Charlie and Zhang, Chiyuan and Mobahi, Hossein and Araya-Polo, Mauricio and Poggio, Tomaso},
	urldate = {2020-10-16},
	date = {2015-12-29},
	eprinttype = {arxiv},
	eprint = {1506.05439},
	keywords = {good, tbr},
	file = {Frogner et al_2015_Learning with a Wasserstein Loss.pdf:/Users/wpq/Dropbox (MIT)/zotero/Frogner et al_2015_Learning with a Wasserstein Loss2.pdf:application/pdf}
}

@article{chizatFasterWassersteinDistance2020,
	title = {Faster Wasserstein Distance Estimation with the Sinkhorn Divergence},
	url = {http://arxiv.org/abs/2006.08172},
	abstract = {The squared Wasserstein distance is a natural quantity to compare probability distributions in a non-parametric setting. This quantity is usually estimated with the plug-in estimator, defined via a discrete optimal transport problem. It can be solved to \${\textbackslash}epsilon\$-accuracy by adding an entropic regularization of order \${\textbackslash}epsilon\$ and using for instance Sinkhorn's algorithm. In this work, we propose instead to estimate it with the Sinkhorn divergence, which is also built on entropic regularization but includes debiasing terms. We show that, for smooth densities, this estimator has a comparable sample complexity but allows higher regularization levels, of order \${\textbackslash}epsilon{\textasciicircum}\{1/2\}\$ , which leads to improved computational complexity bounds and a strong speedup in practice. Our theoretical analysis covers the case of both randomly sampled densities and deterministic discretizations on uniform grids. We also propose and analyze an estimator based on Richardson extrapolation of the Sinkhorn divergence which enjoys improved statistical and computational efficiency guarantees, under a condition on the regularity of the approximation error, which is in particular satisfied for Gaussian densities. We finally demonstrate the efficiency of the proposed estimators with numerical experiments.},
	journaltitle = {{arXiv}:2006.08172 [math, stat]},
	author = {Chizat, Lenaic and Roussillon, Pierre and Léger, Flavien and Vialard, François-Xavier and Peyré, Gabriel},
	urldate = {2020-10-18},
	date = {2020-06-15},
	eprinttype = {arxiv},
	eprint = {2006.08172},
	keywords = {tbr},
	file = {Chizat et al_2020_Faster Wasserstein Distance Estimation with the Sinkhorn Divergence.pdf:/Users/wpq/Dropbox (MIT)/zotero/Chizat et al_2020_Faster Wasserstein Distance Estimation with the Sinkhorn Divergence.pdf:application/pdf}
}

@article{yangPredictingCellLineages2020,
	title = {Predicting cell lineages using autoencoders and optimal transport},
	volume = {16},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1007828},
	doi = {10.1371/journal.pcbi.1007828},
	pages = {e1007828},
	number = {4},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLoS} Comput Biol},
	author = {Yang, Karren Dai and Damodaran, Karthik and Venkatachalapathy, Saradha and Soylemezoglu, Ali C. and Shivashankar, G. V. and Uhler, Caroline},
	editor = {Ma, Jian},
	urldate = {2020-10-22},
	date = {2020-04-28},
	langid = {english},
	keywords = {read},
	file = {Yang et al_2020_Predicting cell lineages using autoencoders and optimal transport.pdf:/Users/wpq/Dropbox (MIT)/zotero/Yang et al_2020_Predicting cell lineages using autoencoders and optimal transport.pdf:application/pdf}
}

@article{janatiMultisubjectMEGEEG2019,
	title = {Multi-subject {MEG}/{EEG} source imaging with sparse multi-task regression},
	url = {http://arxiv.org/abs/1910.01914},
	abstract = {Magnetoencephalography and electroencephalography (M/{EEG}) are non-invasive modalities that measure the weak electromagnetic fields generated by neural activity. Estimating the location and magnitude of the current sources that generated these electromagnetic fields is a challenging ill-posed regression problem known as {\textbackslash}emph\{source imaging\}. When considering a group study, a common approach consists in carrying out the regression tasks independently for each subject. An alternative is to jointly localize sources for all subjects taken together, while enforcing some similarity between them. By pooling all measurements in a single multi-task regression, one makes the problem better posed, offering the ability to identify more sources and with greater precision. The Minimum Wasserstein Estimates ({MWE}) promotes focal activations that do not perfectly overlap for all subjects, thanks to a regularizer based on Optimal Transport ({OT}) metrics. {MWE} promotes spatial proximity on the cortical mantel while coping with the varying noise levels across subjects. On realistic simulations, {MWE} decreases the localization error by up to 4 mm per source compared to individual solutions. Experiments on the Cam-{CAN} dataset show a considerable improvement in spatial specificity in population imaging. Our analysis of a multimodal dataset shows how multi-subject source localization closes the gap between {MEG} and {fMRI} for brain mapping.},
	journaltitle = {{arXiv}:1910.01914 [cs, q-bio, stat]},
	author = {Janati, Hicham and Bazeille, Thomas and Thirion, Bertrand and Cuturi, Marco and Gramfort, Alexandre},
	urldate = {2020-10-22},
	date = {2019-10-14},
	eprinttype = {arxiv},
	eprint = {1910.01914},
	keywords = {tbr},
	file = {Janati et al_2019_Multi-subject MEG-EEG source imaging with sparse multi-task regression.pdf:/Users/wpq/Dropbox (MIT)/zotero/Janati et al_2019_Multi-subject MEG-EEG source imaging with sparse multi-task regression.pdf:application/pdf}
}

@article{yangScalableUnbalancedOptimal2019,
	title = {Scalable Unbalanced Optimal Transport Using Generative Adversarial Networks},
	abstract = {Generative adversarial networks ({GANs}) are an expressive class of neural generative models with tremendous success in modeling high-dimensional continuous measures. In this paper, we present a scalable method for unbalanced optimal transport ({OT}) based on the generativeadversarial framework. We formulate unbalanced {OT} as a problem of simultaneously learning a transport map and a scaling factor that push a source measure to a target measure in a cost-optimal manner. We provide theoretical justiﬁcation for this formulation, showing that it is closely related to an existing static formulation by Liero et al. (2018). We then propose an algorithm for solving this problem based on stochastic alternating gradient updates, similar in practice to {GANs}, and perform numerical experiments demonstrating how this methodology can be applied to population modeling.},
	pages = {20},
	author = {Yang, Karren D and Uhler, Caroline},
	date = {2019},
	langid = {english},
	keywords = {tbr},
	file = {Yang_Uhler_2019_Scalable Unbalanced Optimal Transport Using Generative Adversarial Networks.pdf:/Users/wpq/Dropbox (MIT)/zotero/Yang_Uhler_2019_Scalable Unbalanced Optimal Transport Using Generative Adversarial Networks2.pdf:application/pdf}
}

@article{genevayLearningGenerativeModels2018,
	title = {Learning Generative Models with Sinkhorn Divergences},
	pages = {10},
	author = {Genevay, Aude and Peyré, Gabriel and Cuturi, Marco},
	date = {2018},
	langid = {english},
	keywords = {tbr},
	file = {Genevay et al_2018_Learning Generative Models with Sinkhorn Divergences.pdf:/Users/wpq/Dropbox (MIT)/zotero/Genevay et al_2018_Learning Generative Models with Sinkhorn Divergences.pdf:application/pdf}
}

@article{audeStochasticOptimizationLargescale2016,
	title = {Stochastic Optimization for Large-scale Optimal Transport},
	url = {http://arxiv.org/abs/1605.08527},
	abstract = {Optimal transport ({OT}) defines a powerful framework to compare probability distributions in a geometrically faithful way. However, the practical impact of {OT} is still limited because of its computational burden. We propose a new class of stochastic optimization algorithms to cope with large-scale problems routinely encountered in machine learning applications. These methods are able to manipulate arbitrary distributions (either discrete or continuous) by simply requiring to be able to draw samples from them, which is the typical setup in high-dimensional learning problems. This alleviates the need to discretize these densities, while giving access to provably convergent methods that output the correct distance without discretization error. These algorithms rely on two main ideas: (a) the dual {OT} problem can be re-cast as the maximization of an expectation ; (b) entropic regularization of the primal {OT} problem results in a smooth dual optimization optimization which can be addressed with algorithms that have a provably faster convergence. We instantiate these ideas in three different setups: (i) when comparing a discrete distribution to another, we show that incremental stochastic optimization schemes can beat Sinkhorn's algorithm, the current state-of-the-art finite dimensional {OT} solver; (ii) when comparing a discrete distribution to a continuous density, a semi-discrete reformulation of the dual program is amenable to averaged stochastic gradient descent, leading to better performance than approximately solving the problem by discretization ; (iii) when dealing with two continuous densities, we propose a stochastic gradient descent over a reproducing kernel Hilbert space ({RKHS}). This is currently the only known method to solve this problem, apart from computing {OT} on finite samples. We backup these claims on a set of discrete, semi-discrete and continuous benchmark problems.},
	journaltitle = {{arXiv}:1605.08527 [cs, math]},
	author = {Aude, Genevay and Cuturi, Marco and Peyré, Gabriel and Bach, Francis},
	urldate = {2020-11-10},
	date = {2016-05-27},
	eprinttype = {arxiv},
	eprint = {1605.08527},
	keywords = {good, tbr},
	file = {Aude et al_2016_Stochastic Optimization for Large-scale Optimal Transport.pdf:/Users/wpq/Dropbox (MIT)/zotero/Aude et al_2016_Stochastic Optimization for Large-scale Optimal Transport.pdf:application/pdf}
}

@article{mrouehWassersteinStyleTransfer2019,
	title = {Wasserstein Style Transfer},
	url = {http://arxiv.org/abs/1905.12828},
	abstract = {We propose Gaussian optimal transport for Image style transfer in an Encoder/Decoder framework. Optimal transport for Gaussian measures has closed forms Monge mappings from source to target distributions. Moreover interpolates between a content and a style image can be seen as geodesics in the Wasserstein Geometry. Using this insight, we show how to mix different target styles , using Wasserstein barycenter of Gaussian measures. Since Gaussians are closed under Wasserstein barycenter, this allows us a simple style transfer and style mixing and interpolation. Moreover we show how mixing different styles can be achieved using other geodesic metrics between gaussians such as the Fisher Rao metric, while the transport of the content to the new interpolate style is still performed with Gaussian {OT} maps. Our simple methodology allows to generate new stylized content interpolating between many artistic styles. The metric used in the interpolation results in different stylizations.},
	journaltitle = {{arXiv}:1905.12828 [cs, stat]},
	author = {Mroueh, Youssef},
	urldate = {2020-11-10},
	date = {2019-05-29},
	eprinttype = {arxiv},
	eprint = {1905.12828},
	keywords = {tbr},
	file = {Mroueh_2019_Wasserstein Style Transfer.pdf:/Users/wpq/Dropbox (MIT)/zotero/Mroueh_2019_Wasserstein Style Transfer.pdf:application/pdf}
}

@article{solomonOptimalTransportbasedPolar2019,
	title = {Optimal transport-based polar interpolation of directional fields},
	volume = {38},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3306346.3323005},
	doi = {10.1145/3306346.3323005},
	abstract = {We propose an algorithm that interpolates between vector and frame fields on triangulated surfaces, designed to complement field design methods in geometry processing and simulation. Our algorithm is based on a polar construction, leveraging a conservation law from the Hopf-Poincaré theorem to match singular points using ideas from optimal transport; the remaining detail of the field is interpolated using straightforward machinery. Our model is designed with topology in mind, sliding singular points along the surface rather than having them appear and disappear, and it caters to all surface topologies, including boundary and generator loops. {CCS} Concepts: • Computing methodologies → Shape analysis; • Mathematics of computing → Interpolation.},
	pages = {1--13},
	number = {4},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Trans. Graph.},
	author = {Solomon, Justin and Vaxman, Amir},
	urldate = {2020-11-10},
	date = {2019-07-12},
	langid = {english},
	keywords = {tbr},
	file = {Solomon_Vaxman_2019_Optimal transport-based polar interpolation of directional fields.pdf:/Users/wpq/Dropbox (MIT)/zotero/Solomon_Vaxman_2019_Optimal transport-based polar interpolation of directional fields.pdf:application/pdf}
}

@article{papadakisOptimalTransportProximal2014,
	title = {Optimal Transport with Proximal Splitting},
	volume = {7},
	issn = {1936-4954},
	url = {http://arxiv.org/abs/1304.5784},
	doi = {10.1137/130920058},
	abstract = {This article reviews the use of first order convex optimization schemes to solve the discretized dynamic optimal transport problem, initially proposed by Benamou and Brenier. We develop a staggered grid discretization that is well adapted to the computation of the \$L{\textasciicircum}2\$ optimal transport geodesic between distributions defined on a uniform spatial grid. We show how proximal splitting schemes can be used to solve the resulting large scale convex optimization problem. A specific instantiation of this method on a centered grid corresponds to the initial algorithm developed by Benamou and Brenier. We also show how more general cost functions can be taken into account and how to extend the method to perform optimal transport on a Riemannian manifold.},
	pages = {212--238},
	number = {1},
	journaltitle = {{SIAM} Journal on Imaging Sciences},
	shortjournal = {{SIAM} J. Imaging Sci.},
	author = {Papadakis, Nicolas and Peyré, Gabriel and Oudet, Edouard},
	urldate = {2020-12-21},
	date = {2014-01},
	eprinttype = {arxiv},
	eprint = {1304.5784},
	keywords = {tbr},
	file = {Papadakis et al_2014_Optimal Transport with Proximal Splitting.pdf:/Users/wpq/Dropbox (MIT)/zotero/Papadakis et al_2014_Optimal Transport with Proximal Splitting.pdf:application/pdf}
}

@article{lavenantDynamicalOptimalTransport2019,
	title = {Dynamical Optimal Transport on Discrete Surfaces},
	volume = {37},
	issn = {0730-0301, 1557-7368},
	url = {http://arxiv.org/abs/1809.07083},
	doi = {10.1145/3272127.3275064},
	abstract = {We propose a technique for interpolating between probability distributions on discrete surfaces, based on the theory of optimal transport. Unlike previous attempts that use linear programming, our method is based on a dynamical formulation of quadratic optimal transport proposed for flat domains by Benamou and Brenier [2000], adapted to discrete surfaces. Our structure-preserving construction yields a Riemannian metric on the (finite-dimensional) space of probability distributions on a discrete surface, which translates the so-called Otto calculus to discrete language. From a practical perspective, our technique provides a smooth interpolation between distributions on discrete surfaces with less diffusion than state-of-the-art algorithms involving entropic regularization. Beyond interpolation, we show how our discrete notion of optimal transport extends to other tasks, such as distribution-valued Dirichlet problems and time integration of gradient flows.},
	pages = {1--16},
	number = {6},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Trans. Graph.},
	author = {Lavenant, Hugo and Claici, Sebastian and Chien, Edward and Solomon, Justin},
	urldate = {2020-12-21},
	date = {2019-01-10},
	eprinttype = {arxiv},
	eprint = {1809.07083},
	keywords = {tbr},
	file = {Lavenant et al_2019_Dynamical Optimal Transport on Discrete Surfaces.pdf:/Users/wpq/Dropbox (MIT)/zotero/Lavenant et al_2019_Dynamical Optimal Transport on Discrete Surfaces.pdf:application/pdf}
}

@article{alvarez-melisOptimalTransportGlobal2019,
	title = {Towards Optimal Transport with Global Invariances},
	url = {http://arxiv.org/abs/1806.09277},
	abstract = {Many problems in machine learning involve calculating correspondences between sets of objects, such as point clouds or images. Discrete optimal transport provides a natural and successful approach to such tasks whenever the two sets of objects can be represented in the same space, or at least distances between them can be directly evaluated. Unfortunately neither requirement is likely to hold when object representations are learned from data. Indeed, automatically derived representations such as word embeddings are typically fixed only up to some global transformations, for example, reflection or rotation. As a result, pairwise distances across two such instances are ill-defined without specifying their relative transformation. In this work, we propose a general framework for optimal transport in the presence of latent global transformations. We cast the problem as a joint optimization over transport couplings and transformations chosen from a flexible class of invariances, propose algorithms to solve it, and show promising results in various tasks, including a popular unsupervised word translation benchmark.},
	journaltitle = {{arXiv}:1806.09277 [cs, stat]},
	author = {Alvarez-Melis, David and Jegelka, Stefanie and Jaakkola, Tommi S.},
	urldate = {2021-02-10},
	date = {2019-02-26},
	eprinttype = {arxiv},
	eprint = {1806.09277},
	keywords = {read},
	file = {Alvarez-Melis et al_2019_Towards Optimal Transport with Global Invariances.pdf:/Users/wpq/Dropbox (MIT)/zotero/Alvarez-Melis et al_2019_Towards Optimal Transport with Global Invariances.pdf:application/pdf}
}

@inproceedings{alvarez-melisGromovWassersteinAlignmentWord2018,
	location = {Brussels, Belgium},
	title = {Gromov-Wasserstein Alignment of Word Embedding Spaces},
	url = {https://www.aclweb.org/anthology/D18-1214},
	doi = {10.18653/v1/D18-1214},
	abstract = {Cross-lingual or cross-domain correspondences play key roles in tasks ranging from machine translation to transfer learning. Recently, purely unsupervised methods operating on monolingual embeddings have become effective alignment tools. Current state-of-the-art methods, however, involve multiple steps, including heuristic post-hoc refinement strategies. In this paper, we cast the correspondence problem directly as an optimal transport ({OT}) problem, building on the idea that word embeddings arise from metric recovery algorithms. Indeed, we exploit the Gromov-Wasserstein distance that measures how similarities between pairs of words relate across languages. We show that our {OT} objective can be estimated efficiently, requires little or no tuning, and results in performance comparable with the state-of-the-art in various unsupervised word translation tasks.},
	eventtitle = {{EMNLP} 2018},
	pages = {1881--1890},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Alvarez-Melis, David and Jaakkola, Tommi},
	urldate = {2021-02-10},
	date = {2018-10},
	keywords = {tbr},
	file = {Alvarez-Melis_Jaakkola_2018_Gromov-Wasserstein Alignment of Word Embedding Spaces.pdf:/Users/wpq/Dropbox (MIT)/zotero/Alvarez-Melis_Jaakkola_2018_Gromov-Wasserstein Alignment of Word Embedding Spaces.pdf:application/pdf}
}

@article{feydyGeometricDataAnalysis2020,
	title = {Geometric data analysis, beyond convolutions},
	pages = {259},
	author = {Feydy, Jean},
	date = {2020},
	langid = {english},
	keywords = {tbr},
	file = {Feydy_2020_Geometric data analysis, beyond convolutions.pdf:/Users/wpq/Dropbox (MIT)/zotero/Feydy_2020_Geometric data analysis, beyond convolutions.pdf:application/pdf}
}

@article{peyreComputationalOptimalTransport2020,
	title = {Computational Optimal Transport},
	url = {http://arxiv.org/abs/1803.00567},
	abstract = {Optimal transport ({OT}) theory can be informally described using the words of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total effort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in {OT} cast that problem as that of comparing two probability distributions, two different piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a "global" cost to every such transport, using the "local" consideration of how much it costs to move a grain of sand from one place to another. Recent years have witnessed the spread of {OT} in several fields, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, {OT} is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This short book reviews {OT} with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of {OT} that make it particularly useful for some of these applications.},
	journaltitle = {{arXiv}:1803.00567 [stat]},
	author = {Peyré, Gabriel and Cuturi, Marco},
	urldate = {2021-03-05},
	date = {2020-03-18},
	eprinttype = {arxiv},
	eprint = {1803.00567},
	keywords = {good, tbr},
	file = {Peyre_Cuturi_2020_Computational Optimal Transport.pdf:/Users/wpq/Dropbox (MIT)/zotero/Peyre_Cuturi_2020_Computational Optimal Transport.pdf:application/pdf}
}

@article{mallastoBayesianInferenceOptimal2020,
	title = {Bayesian Inference for Optimal Transport with Stochastic Cost},
	url = {http://arxiv.org/abs/2010.09327},
	abstract = {In machine learning and computer vision, optimal transport has had significant success in learning generative models and defining metric distances between structured and stochastic data objects, that can be cast as probability measures. The key element of optimal transport is the so called lifting of an {\textbackslash}emph\{exact\} cost (distance) function, defined on the sample space, to a cost (distance) between probability measures over the sample space. However, in many real life applications the cost is {\textbackslash}emph\{stochastic\}: e.g., the unpredictable traffic flow affects the cost of transportation between a factory and an outlet. To take this stochasticity into account, we introduce a Bayesian framework for inferring the optimal transport plan distribution induced by the stochastic cost, allowing for a principled way to include prior information and to model the induced stochasticity on the transport plans. Additionally, we tailor an {HMC} method to sample from the resulting transport plan posterior distribution.},
	journaltitle = {{arXiv}:2010.09327 [cs, stat]},
	author = {Mallasto, Anton and Heinonen, Markus and Kaski, Samuel},
	urldate = {2021-03-12},
	date = {2020-10-19},
	eprinttype = {arxiv},
	eprint = {2010.09327},
	keywords = {tbr},
	file = {Mallasto et al_2020_Bayesian Inference for Optimal Transport with Stochastic Cost.pdf:/Users/wpq/Dropbox (MIT)/zotero/Mallasto et al_2020_Bayesian Inference for Optimal Transport with Stochastic Cost.pdf:application/pdf}
}

@article{solomonOptimalTransportDiscrete2018,
	title = {Optimal Transport on Discrete Domains},
	url = {http://arxiv.org/abs/1801.07745},
	abstract = {Inspired by the matching of supply to demand in logistical problems, the optimal transport (or Monge--Kantorovich) problem involves the matching of probability distributions defined over a geometric domain such as a surface or manifold. In its most obvious discretization, optimal transport becomes a large-scale linear program, which typically is infeasible to solve efficiently on triangle meshes, graphs, point clouds, and other domains encountered in graphics and machine learning. Recent breakthroughs in numerical optimal transport, however, enable scalability to orders-of-magnitude larger problems, solvable in a fraction of a second. Here, we discuss advances in numerical optimal transport that leverage understanding of both discrete and smooth aspects of the problem. State-of-the-art techniques in discrete optimal transport combine insight from partial differential equations ({PDE}) with convex analysis to reformulate, discretize, and optimize transportation problems. The end result is a set of theoretically-justified models suitable for domains with thousands or millions of vertices. Since numerical optimal transport is a relatively new discipline, special emphasis is placed on identifying and explaining open problems in need of mathematical insight and additional research.},
	journaltitle = {{arXiv}:1801.07745 [cs, math]},
	author = {Solomon, Justin},
	urldate = {2021-03-13},
	date = {2018-05-01},
	eprinttype = {arxiv},
	eprint = {1801.07745},
	keywords = {good, read},
	file = {Solomon_2018_Optimal Transport on Discrete Domains.pdf:/Users/wpq/Dropbox (MIT)/zotero/Solomon_2018_Optimal Transport on Discrete Domains.pdf:application/pdf}
}

@article{benamouComputationalFluidMechanics2000,
	title = {A computational fluid mechanics solution to the Monge-Kantorovich mass transfer problem},
	volume = {84},
	issn = {0945-3245},
	url = {https://doi.org/10.1007/s002110050002},
	doi = {10.1007/s002110050002},
	pages = {375--393},
	number = {3},
	journaltitle = {Numerische Mathematik},
	shortjournal = {Numer. Math.},
	author = {Benamou, Jean-David and Brenier, Yann},
	urldate = {2021-03-13},
	date = {2000-01-01},
	langid = {english},
	keywords = {good, tbr},
	file = {Benamou_Brenier_2000_A computational fluid mechanics solution to the Monge-Kantorovich mass transfer problem.pdf:/Users/wpq/Dropbox (MIT)/zotero/Benamou_Brenier_2000_A computational fluid mechanics solution to the Monge-Kantorovich mass transfer problem.pdf:application/pdf}
}

@article{levyNotionsOptimalTransport2017,
	title = {Notions of optimal transport theory and how to implement them on a computer},
	url = {http://arxiv.org/abs/1710.02634},
	abstract = {This article gives an introduction to optimal transport, a mathematical theory that makes it possible to measure distances between functions (or distances between more general objects), to interpolate between objects or to enforce mass/volume conservation in certain computational physics simulations. Optimal transport is a rich scientific domain, with active research communities, both on its theoretical aspects and on more applicative considerations, such as geometry processing and machine learning. This article aims at explaining the main principles behind the theory of optimal transport, introduce the different involved notions, and more importantly, how they relate, to let the reader grasp an intuition of the elegant theory that structures them. Then we will consider a specific setting, called semi-discrete, where a continuous function is transported to a discrete sum of Dirac masses. Studying this specific setting naturally leads to an efficient computational algorithm, that uses classical notions of computational geometry, such as a generalization of Voronoi diagrams called Laguerre diagrams.},
	journaltitle = {{arXiv}:1710.02634 [math]},
	author = {Levy, Bruno and Schwindt, Erica},
	urldate = {2021-03-13},
	date = {2017-10-07},
	eprinttype = {arxiv},
	eprint = {1710.02634},
	keywords = {good, tbr},
	file = {Levy_Schwindt_2017_Notions of optimal transport theory and how to implement them on a computer.pdf:/Users/wpq/Dropbox (MIT)/zotero/Levy_Schwindt_2017_Notions of optimal transport theory and how to implement them on a computer.pdf:application/pdf}
}

@inproceedings{ningMatrixvaluedMongeKantorovichOptimal2013,
	title = {Matrix-valued Monge-Kantorovich optimal mass transport},
	doi = {10.1109/CDC.2013.6760486},
	abstract = {We formulate an optimal transport problem for matrix-valued density functions. This is pertinent in the spectral analysis of multivariable time-series. The “mass” represents energy at various frequencies whereas, in addition to a usual transportation cost across frequencies, a cost of rotation is also taken into account. We show that it is natural to seek the transportation plan in the tensor product of the spaces for the two matrix-valued marginals. In contrast to the classical Monge-Kantorovich setting, the transportation plan is no longer supported on a thin zero-measure set.},
	eventtitle = {52nd {IEEE} Conference on Decision and Control},
	pages = {3906--3911},
	booktitle = {52nd {IEEE} Conference on Decision and Control},
	author = {Ning, L. and Georgiou, T. T. and Tannenbaum, A.},
	date = {2013-12},
	note = {{ISSN}: 0191-2216},
	keywords = {tbr},
	file = {Ning et al_2013_Matrix-valued Monge-Kantorovich optimal mass transport.pdf:/Users/wpq/Dropbox (MIT)/zotero/Ning et al_2013_Matrix-valued Monge-Kantorovich optimal mass transport.pdf:application/pdf}
}

@article{weberRobustLargeMarginLearning2020,
	title = {Robust Large-Margin Learning in Hyperbolic Space},
	url = {http://arxiv.org/abs/2004.05465},
	abstract = {Recently, there has been a surge of interest in representation learning in hyperbolic spaces, driven by their ability to represent hierarchical data with significantly fewer dimensions than standard Euclidean spaces. However, the viability and benefits of hyperbolic spaces for downstream machine learning tasks have received less attention. In this paper, we present, to our knowledge, the first theoretical guarantees for learning a classifier in hyperbolic rather than Euclidean space. Specifically, we consider the problem of learning a large-margin classifier for data possessing a hierarchical structure. Our first contribution is a hyperbolic perceptron algorithm, which provably converges to a separating hyperplane. We then provide an algorithm to efficiently learn a large-margin hyperplane, relying on the careful injection of adversarial examples. Finally, we prove that for hierarchical data that embeds well into hyperbolic space, the low embedding dimension ensures superior guarantees when learning the classifier directly in hyperbolic space.},
	journaltitle = {{arXiv}:2004.05465 [cs, stat]},
	author = {Weber, Melanie and Zaheer, Manzil and Rawat, Ankit Singh and Menon, Aditya and Kumar, Sanjiv},
	urldate = {2021-03-14},
	date = {2020-11-03},
	eprinttype = {arxiv},
	eprint = {2004.05465},
	keywords = {tbr},
	file = {Weber et al_2020_Robust Large-Margin Learning in Hyperbolic Space.pdf:/Users/wpq/Dropbox (MIT)/zotero/Weber et al_2020_Robust Large-Margin Learning in Hyperbolic Space.pdf:application/pdf}
}

@article{genevayLearningGenerativeModels2017,
	title = {Learning Generative Models with Sinkhorn Divergences},
	url = {http://arxiv.org/abs/1706.00292},
	abstract = {The ability to compare two degenerate probability distributions (i.e. two probability distributions supported on two distinct low-dimensional manifolds living in a much higher-dimensional space) is a crucial problem arising in the estimation of generative models for high-dimensional observations such as those arising in computer vision or natural language. It is known that optimal transport metrics can represent a cure for this problem, since they were specifically designed as an alternative to information divergences to handle such problematic scenarios. Unfortunately, training generative machines using {OT} raises formidable computational and statistical challenges, because of (i) the computational burden of evaluating {OT} losses, (ii) the instability and lack of smoothness of these losses, (iii) the difficulty to estimate robustly these losses and their gradients in high dimension. This paper presents the first tractable computational method to train large scale generative models using an optimal transport loss, and tackles these three issues by relying on two key ideas: (a) entropic smoothing, which turns the original {OT} loss into one that can be computed using Sinkhorn fixed point iterations; (b) algorithmic (automatic) differentiation of these iterations. These two approximations result in a robust and differentiable approximation of the {OT} loss with streamlined {GPU} execution. Entropic smoothing generates a family of losses interpolating between Wasserstein ({OT}) and Maximum Mean Discrepancy ({MMD}), thus allowing to find a sweet spot leveraging the geometry of {OT} and the favorable high-dimensional sample complexity of {MMD} which comes with unbiased gradient estimates. The resulting computational architecture complements nicely standard deep network generative models by a stack of extra layers implementing the loss function.},
	journaltitle = {{arXiv}:1706.00292 [stat]},
	author = {Genevay, Aude and Peyré, Gabriel and Cuturi, Marco},
	urldate = {2021-03-14},
	date = {2017-10-20},
	eprinttype = {arxiv},
	eprint = {1706.00292},
	keywords = {tbr},
	file = {Genevay et al_2017_Learning Generative Models with Sinkhorn Divergences.pdf:/Users/wpq/Dropbox (MIT)/zotero/Genevay et al_2017_Learning Generative Models with Sinkhorn Divergences.pdf:application/pdf}
}

@article{lombardiEulerianModelsAlgorithms2015,
	title = {Eulerian models and algorithms for unbalanced optimal transport},
	volume = {49},
	issn = {0764-583X, 1290-3841},
	url = {http://www.esaim-m2an.org/10.1051/m2an/2015025},
	doi = {10.1051/m2an/2015025},
	abstract = {Benamou and Brenier formulation of Monge transportation problem [4] has proven to be of great interest in image processing to compute warpings and distances between pair of images [2]. One requirement for the algorithm to work is to interpolate densities of same mass. In most applications to image interpolation, this is a serious limitation. Existing approaches [3, 15, 16] to overcome this caveat are reviewed, and discussed. Due to the mix between transport and L2 interpolation, these models can produce instantaneous motion at ﬁnite range. In this paper we propose new methods, parameter-free, for interpolating unbalanced densities. One of our motivations is the application to interpolation of growing tumor images.},
	pages = {1717--1744},
	number = {6},
	journaltitle = {{ESAIM}: Mathematical Modelling and Numerical Analysis},
	shortjournal = {{ESAIM}: M2AN},
	author = {Lombardi, Damiano and Maitre, Emmanuel},
	urldate = {2021-03-14},
	date = {2015-11},
	langid = {english},
	keywords = {tbr},
	file = {Lombardi and Maitre - 2015 - Eulerian models and algorithms for unbalanced opti.pdf:/Users/wpq/Zotero/storage/MIPYIFXJ/Lombardi and Maitre - 2015 - Eulerian models and algorithms for unbalanced opti.pdf:application/pdf}
}

@thesis{genevayEntropyRegularizedOptimalTransport2019,
	title = {Entropy-Regularized Optimal Transport for Machine Learning},
	url = {https://tel.archives-ouvertes.fr/tel-02319318},
	abstract = {This thesis proposes theoretical and numerical contributions to use Entropy-regularized Optimal Transport ({EOT}) for machine learning. We introduce Sinkhorn Divergences ({SD}), a class of discrepancies between probability measures based on {EOT} which interpolates between two other well-known discrepancies: Optimal Transport ({OT}) and Maximum Mean Discrepancies ({MMD}). We develop an efficient numerical method to use {SD} for density fitting tasks, showing that a suitable choice of regularization can improve performance over existing methods. We derive a sample complexity theorem for {SD} which proves that choosing a large enough regularization parameter allows to break the curse of dimensionality from {OT}, and recover asymptotic rates similar to {MMD}. We propose and analyze stochastic optimization solvers for {EOT}, which yield online methods that can cope with arbitrary measures and are well suited to large scale problems, contrarily to existing discrete batch solvers.},
	institution = {{PSL} University},
	type = {phdthesis},
	author = {Genevay, Aude},
	urldate = {2021-03-16},
	date = {2019-03-13},
	langid = {english},
	keywords = {tbr},
	file = {Genevay_2019_Entropy-Regularized Optimal Transport for Machine Learning.pdf:/Users/wpq/Dropbox (MIT)/zotero/Genevay_2019_Entropy-Regularized Optimal Transport for Machine Learning.pdf:application/pdf}
}

@article{degoesOptimalTransportApproach2011,
	title = {An Optimal Transport Approach to Robust Reconstruction and Simplification of 2D Shapes},
	volume = {30},
	issn = {01677055},
	url = {http://doi.wiley.com/10.1111/j.1467-8659.2011.02033.x},
	doi = {10.1111/j.1467-8659.2011.02033.x},
	abstract = {We propose a robust 2D shape reconstruction and simpliﬁcation algorithm which takes as input a defect-laden point set with noise and outliers. We introduce an optimal-transport driven approach where the input point set, considered as a sum of Dirac measures, is approximated by a simplicial complex considered as a sum of uniform measures on 0- and 1-simplices. A ﬁne-to-coarse scheme is devised to construct the resulting simplicial complex through greedy decimation of a Delaunay triangulation of the input point set. Our method performs well on a variety of examples ranging from line drawings to grayscale images, with or without noise, features, and boundaries.},
	pages = {1593--1602},
	number = {5},
	journaltitle = {Computer Graphics Forum},
	author = {de Goes, Fernando and Cohen-Steiner, David and Alliez, Pierre and Desbrun, Mathieu},
	urldate = {2021-03-17},
	date = {2011-08},
	langid = {english},
	keywords = {tbr},
	file = {de Goes et al. - 2011 - An Optimal Transport Approach to Robust Reconstruc.pdf:/Users/wpq/Zotero/storage/RE4KDE7A/de Goes et al. - 2011 - An Optimal Transport Approach to Robust Reconstruc.pdf:application/pdf}
}

@thesis{claiciStructureSimplificationTransportation2020,
	title = {Structure as simplification : transportation tools for understanding data},
	rights = {{MIT} theses may be protected by copyright. Please reuse {MIT} thesis content according to the {MIT} Libraries Permissions Policy, which is available through the {URL} provided.},
	url = {https://dspace.mit.edu/handle/1721.1/127014},
	shorttitle = {Structure as simplification},
	abstract = {The typical machine learning algorithms looks for a pattern in data, and makes an assumption that the signal to noise ratio of the pattern is high. This approach depends strongly on the quality of the datasets these algorithms operate on, and many complex algorithms fail in spectacular fashion on simple tasks by overfitting noise or outlier examples. These algorithms have training procedures that scale poorly in the size of the dataset, and their out-puts are difficult to intepret. This thesis proposes solutions to both problems by leveraging the theory of optimal transport and proposing efficient algorithms to solve problems in: (1) quantization, with extensions to the Wasserstein barycenter problem, and a link to the classical coreset problem; (2) natural language processing where the hierarchical structure of text allows us to compare documents efficiently;(3) Bayesian inference where we can impose a hierarchy on the label switching problem to resolve ambiguities.},
	institution = {Massachusetts Institute of Technology},
	type = {Thesis},
	author = {Claici, Sebastian},
	urldate = {2021-03-17},
	date = {2020},
	note = {Accepted: 2020-09-03T17:41:55Z
{ISBN}: 9781191624381
Journal Abbreviation: Transportation tools for understanding data},
	keywords = {tbr},
	file = {Claici_2020_Structure as simplification - transportation tools for understanding data.pdf:/Users/wpq/Dropbox (MIT)/zotero/Claici_2020_Structure as simplification - transportation tools for understanding data.pdf:application/pdf}
}

@article{digneFeaturePreservingSurfaceReconstruction2014,
	title = {Feature-Preserving Surface Reconstruction and Simplification from Defect-Laden Point Sets},
	volume = {48},
	issn = {1573-7683},
	url = {https://doi.org/10.1007/s10851-013-0414-y},
	doi = {10.1007/s10851-013-0414-y},
	abstract = {We introduce a robust and feature-capturing surface reconstruction and simplification method that turns an input point set into a low triangle-count simplicial complex. Our approach starts with a (possibly non-manifold) simplicial complex filtered from a 3D Delaunay triangulation of the input points. This initial approximation is iteratively simplified based on an error metric that measures, through optimal transport, the distance between the input points and the current simplicial complex—both seen as mass distributions. Our approach is shown to exhibit both robustness to noise and outliers, as well as preservation of sharp features and boundaries. Our new feature-sensitive metric between point sets and triangle meshes can also be used as a post-processing tool that, from the smooth output of a reconstruction method, recovers sharp features and boundaries present in the initial point set.},
	pages = {369--382},
	number = {2},
	journaltitle = {Journal of Mathematical Imaging and Vision},
	shortjournal = {J Math Imaging Vis},
	author = {Digne, Julie and Cohen-Steiner, David and Alliez, Pierre and de Goes, Fernando and Desbrun, Mathieu},
	urldate = {2021-03-17},
	date = {2014-02-01},
	langid = {english},
	keywords = {tbr},
	file = {Digne et al_2014_Feature-Preserving Surface Reconstruction and Simplification from Defect-Laden Point Sets.pdf:/Users/wpq/Dropbox (MIT)/zotero/Digne et al_2014_Feature-Preserving Surface Reconstruction and Simplification from Defect-Laden Point Sets.pdf:application/pdf}
}