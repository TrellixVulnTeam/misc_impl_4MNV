{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision \n",
    "import torch.distributions as dist\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = 100\n",
    "nc_gaussian = 2\n",
    "\n",
    "seed = 1\n",
    "nfg = 64\n",
    "nfd = 64\n",
    "nc = 1\n",
    "model_name = f'infogan_seed={seed}'\n",
    "model_name = f'{model_name}_{datetime.now().strftime(\"%Y.%m.%d-%H:%M:%S\")}'\n",
    "model_name = 'jpt_test'\n",
    "data_root = './data'\n",
    "figure_root = os.path.join('./figures', model_name)\n",
    "model_root = os.path.join('./models', model_name)\n",
    "log_root = os.path.join('./logs', model_name)\n",
    "load_weights_generator = ''\n",
    "load_weights_discriminator = ''\n",
    "image_size = 32\n",
    "batch_size = 64\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "n_epochs = 10\n",
    "n_batches_print = 100\n",
    "n_workers = 8\n",
    "gpu_id = '1'\n",
    "\n",
    "weight_param = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, nz, nf, nc, nc_gaussian):\n",
    "        \"\"\"\n",
    "            nz      dimension of noise and latent codes\n",
    "            nf      dimension of features in last conv layer\n",
    "            nc      number of channels in the image\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_channels, out_channels, stride=2, padding=1, batch_norm=True, nonlinearity=nn.ReLU(True)):\n",
    "            return [\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=stride, padding=padding, bias=False),\n",
    "                *( [nn.BatchNorm2d(out_channels)] if batch_norm else [] ),\n",
    "                nonlinearity,\n",
    "            ]\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # (nz+1+nc_gaussian)   x 1 x 1\n",
    "            *block(nz+10+nc_gaussian,   4*nf, stride=1, padding=0),\n",
    "            # (4*nf) x 4 x 4\n",
    "            *block(4*nf, 2*nf),\n",
    "            # (2*nf) x 8 x 8\n",
    "            *block(2*nf,   nf),\n",
    "            # (nf) x 16 x 16\n",
    "            *block(nf,     nc, batch_norm=False, nonlinearity=nn.Tanh()),\n",
    "            # (nc) x 32 x 32\n",
    "        )\n",
    "\n",
    "    def forward(self, z, c):\n",
    "        \"\"\"\n",
    "            z       (N, nz, 1, 1) or (N, nz)\n",
    "                incompressible noise vector\n",
    "            c       (N, 10+nc_gaussian, 1)\n",
    "                latent code vector\n",
    "            Returns (N, nc, 32, 32)\n",
    "                image generated from model distribution\n",
    "        \"\"\"\n",
    "        if len(z.shape) != 4:\n",
    "            z = z.view(z.shape[0], z.shape[1], 1, 1)\n",
    "        if len(c.shape) != 4:\n",
    "            c = c.view(c.shape[0], c.shape[1], 1, 1)\n",
    "    \n",
    "        return self.model(torch.cat((z, c), 1))\n",
    "    \n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, nc, nf, nc_gaussian):\n",
    "        \"\"\"\n",
    "            nc      number of channels in the image\n",
    "            nf      dimension of features of first conv layer\n",
    "            nc_gaussian\n",
    "                    number of latent codes that has Gaussian distrib\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        def block(in_channels, out_channels,\n",
    "                  stride=2, padding=1,\n",
    "                  batch_norm=True,\n",
    "                  nonlinearity=nn.LeakyReLU(0.2, inplace=True)):\n",
    "            \"\"\" stride=1, padding=0: H_out = H_in - 3              # 4 -> 1\n",
    "                stride=2, padding=1: H_out = floor((H_in-1)/2 +1)  # roughly halves\n",
    "            \"\"\"\n",
    "            return [\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=stride, padding=padding, bias=False),\n",
    "                *( [nn.BatchNorm2d(out_channels)] if batch_norm else [] ),\n",
    "                nonlinearity,\n",
    "            ]\n",
    "        \n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            # (nc) x 32 x 32\n",
    "            *block(nc,     nf, batch_norm=False),\n",
    "            # (nf) x 16 x 16\n",
    "            *block(nf,   2*nf),\n",
    "            # (2*nf) x 8 x 8\n",
    "            *block(2*nf, 4*nf),\n",
    "            # (4*nf) x 4 x 4\n",
    "        )\n",
    "        \n",
    "        n_in_units = 4*nf*16\n",
    "        \n",
    "        # D: discriminator \n",
    "        self.fc_y = nn.Sequential(\n",
    "            nn.Linear(n_in_units, 1),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "        # Q: variational distribution \n",
    "        \n",
    "        # (1d Categorical)\n",
    "        self.fc_cat = nn.Sequential(\n",
    "            nn.Linear(n_in_units, 10),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "        \n",
    "        # (2d isotropic Gaussian)\n",
    "        self.fc_mu = nn.Sequential(\n",
    "            nn.Linear(n_in_units, nc_gaussian))\n",
    "        self.fc_logvariance = nn.Sequential(\n",
    "            nn.Linear(n_in_units, nc_gaussian))\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            x        (N, nc, h, w)\n",
    "            Returns\n",
    "                y    (N, 1)\n",
    "                    classification probability that x comes from data distribution\n",
    "                logp (N, 10)\n",
    "                    log probability for 1D categorical latent code\n",
    "                mu, variance  (N, 2)\n",
    "                    parameters for 2D Gaussian latent codes\n",
    "        \"\"\"\n",
    "        h = self.conv_blocks(x)\n",
    "        h = h.view(h.shape[0], -1)\n",
    "        \n",
    "        y = self.fc_y(h)\n",
    "        \n",
    "        logp = self.fc_cat(h)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvariance = self.fc_logvariance(h)\n",
    "\n",
    "        return  y, logp, mu, logvariance\n",
    "    \n",
    "def weights_initialization(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "def plot_one(x, color_bar=False):\n",
    "    x = x.detach().cpu().numpy().transpose((1,2,0)).squeeze()\n",
    "    plt.imshow(x)\n",
    "    plt.axis('off')\n",
    "    if color_bar:\n",
    "        plt.colorbar(extend='both')\n",
    "    return plt    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_root, exist_ok=True)\n",
    "os.makedirs(model_root, exist_ok=True)\n",
    "os.makedirs(figure_root, exist_ok=True)\n",
    "os.makedirs(log_root, exist_ok=True)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "trainset = datasets.MNIST(root=data_root, download=True,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.Resize(image_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,), (0.5,)),\n",
    "            ]))\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(trainloader))\n",
    "plot_one(x[0][0])\n",
    "torch.mean(x[0]), torch.std(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_id\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "G = Generator(nz, nfg, nc, nc_gaussian).to(device)\n",
    "G.apply(weights_initialization)\n",
    "if load_weights_generator != '':\n",
    "    G.load_state_dict(torch.load(load_weights_generator))\n",
    "\n",
    "D = Discriminator(nc, nfd, nc_gaussian).to(device)\n",
    "D.apply(weights_initialization)\n",
    "if load_weights_discriminator != '':\n",
    "    D.load_state_dict(torch.load(load_weights_discriminator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def logpdf_gaussian(x, mu, logvariance, reduction='mean'):\n",
    "    \"\"\" Evaluate log Normal(x;μ,exp.(σ^2))\n",
    "            i.e. log-pdf of x under spherical Gaussian N(x|μ,σ^2 I)\n",
    "\n",
    "        x           (batch_size, n_x)\n",
    "        mu          (batch_size, n_x)\n",
    "        log_sigma2  (batch_size, n_x)\n",
    "\n",
    "    batch dot product: https://github.com/pytorch/pytorch/issues/18027\n",
    "    \n",
    "    overflow problem fix: put 1/sigma^2 \\circ (x-mu) first, ....\n",
    "    \"\"\"\n",
    "    log_probs = (-mu.shape[-1]/2)*math.log(2*math.pi) - \\\n",
    "        (1/2)*torch.sum(logvariance,dim=1) - \\\n",
    "        (1/2)*torch.sum((1/torch.exp(logvariance))*(x-mu)*(x-mu),-1)\n",
    "    if reduction == 'mean':\n",
    "        return torch.mean(log_probs,dim=-1)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_probs,dim=-1)\n",
    "    else:\n",
    "        raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_loss = nn.BCELoss()\n",
    "Q_cat_loss = nn.NLLLoss()\n",
    "Q_gaussian_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# label flipping MIGHT helps with training G!\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "optimizerD = torch.optim.Adam(chain(\n",
    "    D.conv_blocks.parameters(),\n",
    "    D.fc_y.parameters()),lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(G.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "# might want to remove conv_blocks parameter here, just the fcs\n",
    "optimizerQ = torch.optim.Adam(chain(\n",
    "    D.conv_blocks.parameters(),\n",
    "    D.fc_cat.parameters(),\n",
    "    D.fc_mu.parameters(),\n",
    "    D.fc_logvariance.parameters()), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "writer = SummaryWriter(log_root)\n",
    "writer.flush()\n",
    "\n",
    "pdf_c_cat = dist.Categorical(torch.ones(10)/10)\n",
    "pdf_c_gaussian = dist.MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
    "\n",
    "\n",
    "def sample_latents(batch_size):\n",
    "\n",
    "    # sample noise\n",
    "    z = torch.randn(batch_size, nz, device=device)\n",
    "\n",
    "    # sample latent codes\n",
    "    c_cat = pdf_c_cat.sample(sample_shape=(batch_size,)).to(device)\n",
    "    c_cat_onehot = nn.functional.one_hot(c_cat, 10)\n",
    "    c_gaussian = pdf_c_gaussian.sample(sample_shape=(batch_size,)).to(device)\n",
    "    c = torch.cat([c_cat_onehot.float(), c_gaussian], dim=-1)\n",
    "\n",
    "    return z, c, c_cat, c_gaussian\n",
    "\n",
    "fixed_z, fixed_c, _, _ = sample_latents(batch_size)\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    for it, (x_real, _) in enumerate(trainloader):\n",
    "\n",
    "        # batch_size for last batch might be different ...\n",
    "        batch_size = x_real.size(0)\n",
    "        real_labels = torch.full((batch_size,), real_label, device=device)\n",
    "        fake_labels = torch.full((batch_size,), fake_label, device=device)\n",
    "\n",
    "        ##############################################################\n",
    "        # Update D: Minimize E[-log(D(x))] + E[-log(1 - D(G(c,z)))] (saturating)\n",
    "        #                                    # D(x) -> p(1|x), probability of x being real\n",
    "        #           Minimize E[-log(D(x))] + E[-log(D(G(c,z)))]     (non-saturating)\n",
    "        #                                    # D(x) -> p(0|x), probability of x being fake\n",
    "        ##############################################################\n",
    "\n",
    "        optimizerD.zero_grad()\n",
    "\n",
    "        # a minibatch of samples from data distribution\n",
    "        x_real = x_real.to(device)\n",
    "\n",
    "        y,_,_,_ = D(x_real)\n",
    "        loss_D_real = D_loss(y, real_labels)\n",
    "        loss_D_real.backward()\n",
    "\n",
    "        D_x = y.mean().item()\n",
    "\n",
    "        z, c, c_cat, c_gaussian = sample_latents(batch_size)\n",
    "        x_fake = G(z, c)\n",
    "        \n",
    "        # https://github.com/pytorch/examples/issues/116\n",
    "        # If we do not detach, then, although x_fake is not needed for gradient update of D,\n",
    "        #   as a consequence of backward pass which clears all the variables in the graph\n",
    "        #   Generator's graph will not be available for gradient update of G\n",
    "        # Also for performance considerations, detaching x_fake will prevent computing \n",
    "        #   gradients for parameters in G\n",
    "        y,_,_,_ = D(x_fake.detach())\n",
    "        loss_D_fake = D_loss(y, fake_labels)\n",
    "        loss_D_fake.backward()\n",
    "\n",
    "        D_G_z1 = y.mean().item()\n",
    "        loss_D = loss_D_real + loss_D_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ##############################################################\n",
    "        # Update G: Minimize E[-log(D(G(c,z))) - \\lambda log Q(G(c,z))]\n",
    "        #                    # D(x) -> p(0|x), probability of x being fake\n",
    "        # Update Q: Minimize E[-\\lambda log Q(x')]\n",
    "        ##############################################################\n",
    "\n",
    "        optimizerG.zero_grad()\n",
    "\n",
    "        y, logp, mu, logvariance = D(x_fake)\n",
    "        loss_bce = D_loss(y, real_labels)\n",
    "        \n",
    "        optimizerQ.zero_grad()\n",
    "        \n",
    "        loss_c_cat_nll = Q_cat_loss(logp, c_cat)\n",
    "        loss_c_gaussian_nll = -logpdf_gaussian(c_gaussian, mu, logvariance)\n",
    "        loss_Q = loss_c_cat_nll + loss_c_gaussian_nll\n",
    "\n",
    "        loss_G = loss_bce + weight_param*loss_Q\n",
    "        loss_G.backward()\n",
    "        \n",
    "        optimizerG.step()\n",
    "        optimizerQ.step()\n",
    "\n",
    "\n",
    "        ##############################################################\n",
    "        # write/print\n",
    "        ##############################################################\n",
    "        \n",
    "\n",
    "        loss_D = loss_D.item()\n",
    "        loss_G = loss_G.item()\n",
    "        loss_Q = loss_Q.item()\n",
    "\n",
    "        loss_total = loss_D + weight_param*loss_Q\n",
    "\n",
    "        if it % n_batches_print == n_batches_print-1:\n",
    "            print(f'[{epoch+1}/{n_epochs}][{it+1}/{len(trainloader)}]'\n",
    "                  f'loss: {loss_total:.4}\\t'\n",
    "                  f'loss_D: {loss_D:.4}\\t'\n",
    "                  f'loss_G: {loss_G:.4}\\t'\n",
    "                  f'loss_Q: {loss_Q:.4}')\n",
    "            vutils.save_image(G(fixed_z, fixed_c).detach(),\n",
    "                os.path.join(figure_root,\n",
    "                     f'{model_name}_fake_samples_epoch={epoch}_it={it}.png'))\n",
    "\n",
    "        if it == 0:\n",
    "            global_step = epoch*len(trainloader)+it\n",
    "#             writer.add_scalar('discriminator/D(x)', D_x, global_step)\n",
    "#             writer.add_scalar('discriminator/D(G(z1))', D_G_z1, global_step)\n",
    "#             writer.add_scalar('discriminator/D(G(z2))', D_G_z2, global_step)\n",
    "            writer.add_scalar('loss/total', loss_total, global_step)\n",
    "            writer.add_scalar('loss/D', loss_D, global_step)\n",
    "            writer.add_scalar('loss/G', loss_G, global_step)\n",
    "            writer.add_scalar('loss/Q', loss_Q, global_step)\n",
    "#             writer.add_scalar('gradient/G_conv_W_first', G.model[0].weight.grad.mean().detach().cpu().item(), global_step)\n",
    "#             writer.add_scalar('gradient/G_conv_W_last', G.model[-2].weight.grad.mean().detach().cpu().item(), global_step)\n",
    "#             writer.add_scalar('gradient/D_conv_W_first', D.model[0].weight.grad.mean().detach().cpu().item(), global_step)\n",
    "#             writer.add_scalar('gradient/D_conv_W_last', D.model[-2].weight.grad.mean().detach().cpu().item(), global_step)\n",
    "            writer.add_image('mnist', torchvision.utils.make_grid(x_fake), global_step)\n",
    "        \n",
    "        \n",
    "    torch.save(G.state_dict(), os.path.join(model_root, f'G_epoch_{epoch}.pt'))\n",
    "    torch.save(D.state_dict(), os.path.join(model_root, f'D_epoch_{epoch}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:misc_impl] *",
   "language": "python",
   "name": "conda-env-misc_impl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
