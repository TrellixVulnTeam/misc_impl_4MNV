
@article{pooleVariationalBoundsMutual2019,
	title = {On Variational Bounds of Mutual Information},
	abstract = {Estimating and optimizing Mutual Information ({MI}) is core to many problems in machine learning; however, bounding {MI} in high dimensions is challenging. To establish tractable and scalable objectives, recent work has turned to variational bounds parameterized by neural networks, but the relationships and tradeoffs between these bounds remains unclear. In this work, we unify these recent developments in a single framework. We ﬁnd that the existing variational lower bounds degrade when the {MI} is large, exhibiting either high bias or high variance. To address this problem, we introduce a continuum of lower bounds that encompasses previous bounds and ﬂexibly trades off bias and variance. On high-dimensional, controlled problems, we empirically characterize the bias and variance of the bounds and their gradients and demonstrate the effectiveness of our new bounds for estimation and representation learning.},
	pages = {10},
	author = {Poole, Ben and Ozair, Sherjil},
	date = {2019},
	langid = {english},
	file = {Poole_Ozair_2019_On Variational Bounds of Mutual Information.pdf:/Users/markwang/DropBox (MIT)/zotero/Poole_Ozair_2019_On Variational Bounds of Mutual Information.pdf:application/pdf}
}

@inproceedings{barberIMAlgorithmVariational2003,
	title = {The {IM} Algorithm: A Variational Approach to Information Maximization.},
	shorttitle = {The {IM} Algorithm},
	abstract = {The maximisation of information transmission over noisy channels is a common, albeit generally computationally difficult problem. We approach the difficulty of computing the mutual information for noisy channels by using a variational approximation. The re- sulting {IM} algorithm is analagous to the {EM} algorithm, yet max- imises mutual information, as opposed to likelihood. We apply the method to several practical examples, including linear compression, population encoding and {CDMA}.},
	author = {Barber, David and Agakov, Felix},
	date = {2003-01-01},
	file = {Barber_Agakov_2003_The IM Algorithm - A Variational Approach to Information Maximization.pdf:/Users/markwang/DropBox (MIT)/zotero/Barber_Agakov_2003_The IM Algorithm - A Variational Approach to Information Maximization.pdf:application/pdf}
}

@article{alemiDeepVariationalInformation2019,
	title = {Deep Variational Information Bottleneck},
	url = {http://arxiv.org/abs/1612.00410},
	abstract = {We present a variational approximation to the information bottleneck of Tishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method "Deep Variational Information Bottleneck", or Deep {VIB}. We show that models trained with the {VIB} objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack.},
	journaltitle = {{arXiv}:1612.00410 [cs, math]},
	author = {Alemi, Alexander A. and Fischer, Ian and Dillon, Joshua V. and Murphy, Kevin},
	urldate = {2020-01-10},
	date = {2019-10-23},
	eprinttype = {arxiv},
	eprint = {1612.00410},
	file = {Alemi et al_2019_Deep Variational Information Bottleneck.pdf:/Users/markwang/DropBox (MIT)/zotero/Alemi et al_2019_Deep Variational Information Bottleneck.pdf:application/pdf}
}

@article{hanchuanpengFeatureSelectionBased2005,
	title = {Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy},
	volume = {27},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2005.159},
	abstract = {Feature selection is an important problem for pattern classification systems. We study how to select good features according to the maximal statistical dependency criterion based on mutual information. Because of the difficulty in directly implementing the maximal dependency condition, we first derive an equivalent form, called minimal-redundancy-maximal-relevance criterion ({mRMR}), for first-order incremental feature selection. Then, we present a two-stage feature selection algorithm by combining {mRMR} and other more sophisticated feature selectors (e.g., wrappers). This allows us to select a compact set of superior features at very low cost. We perform extensive experimental comparison of our algorithm and other methods using three different classifiers (naive Bayes, support vector machine, and linear discriminate analysis) and four different data sets (handwritten digits, arrhythmia, {NCI} cancer cell lines, and lymphoma tissues). The results confirm that {mRMR} leads to promising improvement on feature selection and classification accuracy.},
	pages = {1226--1238},
	number = {8},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Hanchuan Peng and Fuhui Long and Ding, C.},
	date = {2005-08},
	file = {Hanchuan Peng et al_2005_Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy.pdf:/Users/markwang/DropBox (MIT)/zotero/Hanchuan Peng et al_2005_Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy.pdf:application/pdf}
}

@article{gaoVariationalInformationMaximization2016,
	title = {Variational Information Maximization for Feature Selection},
	url = {http://arxiv.org/abs/1606.02827},
	abstract = {Feature selection is one of the most fundamental problems in machine learning. An extensive body of work on information-theoretic feature selection exists which is based on maximizing mutual information between subsets of features and class labels. Practical methods are forced to rely on approximations due to the difficulty of estimating mutual information. We demonstrate that approximations made by existing methods are based on unrealistic assumptions. We formulate a more flexible and general class of assumptions based on variational distributions and use them to tractably generate lower bounds for mutual information. These bounds define a novel information-theoretic framework for feature selection, which we prove to be optimal under tree graphical models with proper choice of variational distributions. Our experiments demonstrate that the proposed method strongly outperforms existing information-theoretic feature selection approaches.},
	journaltitle = {{arXiv}:1606.02827 [cs, stat]},
	author = {Gao, Shuyang and Steeg, Greg Ver and Galstyan, Aram},
	urldate = {2020-01-29},
	date = {2016-06-09},
	eprinttype = {arxiv},
	eprint = {1606.02827},
	file = {Gao et al_2016_Variational Information Maximization for Feature Selection.pdf:/Users/markwang/DropBox (MIT)/zotero/Gao et al_2016_Variational Information Maximization for Feature Selection.pdf:application/pdf}
}