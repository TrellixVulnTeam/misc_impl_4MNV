
@article{jahanianSteerabilityGenerativeAdversarial2019,
	title = {On the ''steerability" of generative adversarial networks},
	url = {http://arxiv.org/abs/1907.07171},
	abstract = {An open secret in contemporary machine learning is that many models work beautifully on standard benchmarks but fail to generalize outside the lab. This has been attributed to training on biased data, which provide poor coverage over real world events. Generative models are no exception, but recent advances in generative adversarial networks ({GANs}) suggest otherwise -- these models can now synthesize strikingly realistic and diverse images. Is generative modeling of photos a solved problem? We show that although current {GANs} can fit standard datasets very well, they still fall short of being comprehensive models of the visual manifold. In particular, we study their ability to fit simple transformations such as camera movements and color changes. We find that the models reflect the biases of the datasets on which they are trained (e.g., centered objects), but that they also exhibit some capacity for generalization: by "steering" in latent space, we can shift the distribution while still creating realistic images. We hypothesize that the degree of distributional shift is related to the breadth of the training data distribution, and conduct experiments that demonstrate this. Code is released on our project page: https://ali-design.github.io/gan\_steerability/},
	journaltitle = {{arXiv}:1907.07171 [cs]},
	author = {Jahanian, Ali and Chai, Lucy and Isola, Phillip},
	urldate = {2019-09-14},
	date = {2019-07-16},
	eprinttype = {arxiv},
	eprint = {1907.07171},
	file = {Jahanian et al_2019_On the ''steerability of generative adversarial networks.pdf:/Users/markwang/DropBox (MIT)/zotero/Jahanian et al_2019_On the ''steerability of generative adversarial networks.pdf:application/pdf}
}

@article{chenInfoGANInterpretableRepresentation2016,
	title = {{InfoGAN}: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets},
	url = {http://arxiv.org/abs/1606.03657},
	shorttitle = {{InfoGAN}},
	abstract = {This paper describes {InfoGAN}, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. {InfoGAN} is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, {InfoGAN} successfully disentangles writing styles from digit shapes on the {MNIST} dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the {SVHN} dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the {CelebA} face dataset. Experiments show that {InfoGAN} learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
	journaltitle = {{arXiv}:1606.03657 [cs, stat]},
	author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
	urldate = {2019-12-04},
	date = {2016-06-11},
	eprinttype = {arxiv},
	eprint = {1606.03657},
	file = {Chen et al_2016_InfoGAN - Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.pdf:/Users/markwang/DropBox (MIT)/zotero/Chen et al_2016_InfoGAN - Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.pdf:application/pdf}
}

@article{radfordUnsupervisedRepresentationLearning2016,
	title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks ({CNNs}) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with {CNNs} has received less attention. In this work we hope to help bridge the gap between the success of {CNNs} for supervised learning and unsupervised learning. We introduce a class of {CNNs} called deep convolutional generative adversarial networks ({DCGANs}), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	journaltitle = {{arXiv}:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	urldate = {2019-12-12},
	date = {2016-01-07},
	eprinttype = {arxiv},
	eprint = {1511.06434},
	file = {Radford et al_2016_Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.pdf:/Users/markwang/DropBox (MIT)/zotero/Radford et al_2016_Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.pdf:application/pdf}
}

@article{goodfellowDistinguishabilityCriteriaEstimating2015,
	title = {On distinguishability criteria for estimating generative models},
	url = {http://arxiv.org/abs/1412.6515},
	abstract = {Two recently introduced criteria for estimation of generative models are both based on a reduction to binary classification. Noise-contrastive estimation ({NCE}) is an estimation procedure in which a generative model is trained to be able to distinguish data samples from noise samples. Generative adversarial networks ({GANs}) are pairs of generator and discriminator networks, with the generator network learning to generate samples by attempting to fool the discriminator network into believing its samples are real data. Both estimation procedures use the same function to drive learning, which naturally raises questions about how they are related to each other, as well as whether this function is related to maximum likelihood estimation ({MLE}). {NCE} corresponds to training an internal data model belonging to the \{{\textbackslash}em discriminator\} network but using a fixed generator network. We show that a variant of {NCE}, with a dynamic generator network, is equivalent to maximum likelihood estimation. Since pairing a learned discriminator with an appropriate dynamically selected generator recovers {MLE}, one might expect the reverse to hold for pairing a learned generator with a certain discriminator. However, we show that recovering {MLE} for a learned generator requires departing from the distinguishability game. Specifically: (i) The expected gradient of the {NCE} discriminator can be made to match the expected gradient of {MLE}, if one is allowed to use a non-stationary noise distribution for {NCE}, (ii) No choice of discriminator network can make the expected gradient for the {GAN} generator match that of {MLE}, and (iii) The existing theory does not guarantee that {GANs} will converge in the non-convex case. This suggests that the key next step in {GAN} research is to determine whether {GANs} converge, and if not, to modify their training algorithm to force convergence.},
	journaltitle = {{arXiv}:1412.6515 [stat]},
	author = {Goodfellow, Ian J.},
	urldate = {2019-12-12},
	date = {2015-05-21},
	eprinttype = {arxiv},
	eprint = {1412.6515},
	file = {Goodfellow_2015_On distinguishability criteria for estimating generative models.pdf:/Users/markwang/DropBox (MIT)/zotero/Goodfellow_2015_On distinguishability criteria for estimating generative models.pdf:application/pdf}
}

@incollection{dentonDeepGenerativeImage2015,
	title = {Deep Generative Image Models using a ￼Laplacian Pyramid of Adversarial Networks},
	url = {http://papers.nips.cc/paper/5773-deep-generative-image-models-using-a-laplacian-pyramid-of-adversarial-networks.pdf},
	pages = {1486--1494},
	booktitle = {Advances in Neural Information Processing Systems 28},
	publisher = {Curran Associates, Inc.},
	author = {Denton, Emily L and Chintala, Soumith and szlam, arthur and Fergus, Rob},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	urldate = {2019-12-12},
	date = {2015},
	file = {Denton et al_2015_Deep Generative Image Models using a ￼Laplacian Pyramid of Adversarial Networks.pdf:/Users/markwang/DropBox (MIT)/zotero/Denton et al_2015_Deep Generative Image Models using a ￼Laplacian Pyramid of Adversarial Networks.pdf:application/pdf}
}

@article{salimansImprovedTechniquesTraining2016a,
	title = {Improved Techniques for Training {GANs}},
	url = {http://arxiv.org/abs/1606.03498},
	abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks ({GANs}) framework. We focus on two applications of {GANs}: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on {MNIST}, {CIFAR}-10 and {SVHN}. The generated images are of high quality as confirmed by a visual Turing test: our model generates {MNIST} samples that humans cannot distinguish from real data, and {CIFAR}-10 samples that yield a human error rate of 21.3\%. We also present {ImageNet} samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of {ImageNet} classes.},
	journaltitle = {{arXiv}:1606.03498 [cs]},
	author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
	urldate = {2019-12-12},
	date = {2016-06-10},
	eprinttype = {arxiv},
	eprint = {1606.03498},
	file = {Salimans et al_2016_Improved Techniques for Training GANs.pdf:/Users/markwang/DropBox (MIT)/zotero/Salimans et al_2016_Improved Techniques for Training GANs2.pdf:application/pdf}
}

@article{goodfellowGenerativeAdversarialNetworks2014,
	title = {Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	journaltitle = {{arXiv}:1406.2661 [cs, stat]},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	urldate = {2019-12-12},
	date = {2014-06-10},
	eprinttype = {arxiv},
	eprint = {1406.2661},
	file = {Goodfellow et al_2014_Generative Adversarial Networks.pdf:/Users/markwang/DropBox (MIT)/zotero/Goodfellow et al_2014_Generative Adversarial Networks.pdf:application/pdf}
}

@article{metzUnrolledGenerativeAdversarial2017,
	title = {Unrolled Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1611.02163},
	abstract = {We introduce a method to stabilize Generative Adversarial Networks ({GANs}) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of {GANs} with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator.},
	journaltitle = {{arXiv}:1611.02163 [cs, stat]},
	author = {Metz, Luke and Poole, Ben and Pfau, David and Sohl-Dickstein, Jascha},
	urldate = {2019-12-12},
	date = {2017-05-12},
	eprinttype = {arxiv},
	eprint = {1611.02163},
	file = {Metz et al_2017_Unrolled Generative Adversarial Networks.pdf:/Users/markwang/DropBox (MIT)/zotero/Metz et al_2017_Unrolled Generative Adversarial Networks.pdf:application/pdf}
}

@article{reedGenerativeAdversarialText2016,
	title = {Generative Adversarial Text to Image Synthesis},
	url = {http://arxiv.org/abs/1605.05396},
	abstract = {Automatic synthesis of realistic images from text would be interesting and useful, but current {AI} systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks ({GANs}) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and {GAN} formulation to effectively bridge these advances in text and image model- ing, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.},
	journaltitle = {{arXiv}:1605.05396 [cs]},
	author = {Reed, Scott and Akata, Zeynep and Yan, Xinchen and Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak},
	urldate = {2019-12-12},
	date = {2016-06-05},
	eprinttype = {arxiv},
	eprint = {1605.05396},
	file = {Reed et al_2016_Generative Adversarial Text to Image Synthesis.pdf:/Users/markwang/DropBox (MIT)/zotero/Reed et al_2016_Generative Adversarial Text to Image Synthesis.pdf:application/pdf}
}

@article{theisNoteEvaluationGenerative2016,
	title = {A note on the evaluation of generative models},
	url = {http://arxiv.org/abs/1511.01844},
	abstract = {Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria---average log-likelihood, Parzen window estimates, and visual fidelity of samples---are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.},
	journaltitle = {{arXiv}:1511.01844 [cs, stat]},
	author = {Theis, Lucas and Oord, Aäron van den and Bethge, Matthias},
	urldate = {2019-12-12},
	date = {2016-04-24},
	eprinttype = {arxiv},
	eprint = {1511.01844},
	file = {Theis et al_2016_A note on the evaluation of generative models.pdf:/Users/markwang/DropBox (MIT)/zotero/Theis et al_2016_A note on the evaluation of generative models.pdf:application/pdf}
}

@article{donahueAdversarialFeatureLearning2017,
	title = {Adversarial Feature Learning},
	url = {http://arxiv.org/abs/1605.09782},
	abstract = {The ability of the Generative Adversarial Networks ({GANs}) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, {GANs} have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks ({BiGANs}) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.},
	journaltitle = {{arXiv}:1605.09782 [cs, stat]},
	author = {Donahue, Jeff and Krähenbühl, Philipp and Darrell, Trevor},
	urldate = {2019-12-12},
	date = {2017-04-03},
	eprinttype = {arxiv},
	eprint = {1605.09782},
	file = {Donahue et al_2017_Adversarial Feature Learning.pdf:/Users/markwang/DropBox (MIT)/zotero/Donahue et al_2017_Adversarial Feature Learning.pdf:application/pdf}
}

@article{zhuUnpairedImagetoImageTranslation2018,
	title = {Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
	url = {http://arxiv.org/abs/1703.10593},
	abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X {\textbackslash}rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y {\textbackslash}rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) {\textbackslash}approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
	journaltitle = {{arXiv}:1703.10593 [cs]},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	urldate = {2019-12-12},
	date = {2018-11-15},
	eprinttype = {arxiv},
	eprint = {1703.10593},
	file = {Zhu et al_2018_Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.pdf:/Users/markwang/DropBox (MIT)/zotero/Zhu et al_2018_Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.pdf:application/pdf}
}

@article{goodfellowNIPS2016Tutorial2017,
	title = {{NIPS} 2016 Tutorial: Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1701.00160},
	shorttitle = {{NIPS} 2016 Tutorial},
	abstract = {This report summarizes the tutorial presented by the author at {NIPS} 2016 on generative adversarial networks ({GANs}). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how {GANs} compare to other generative models, (3) the details of how {GANs} work, (4) research frontiers in {GANs}, and (5) state-of-the-art image models that combine {GANs} with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
	journaltitle = {{arXiv}:1701.00160 [cs]},
	author = {Goodfellow, Ian},
	urldate = {2020-01-06},
	date = {2017-04-03},
	eprinttype = {arxiv},
	eprint = {1701.00160},
	file = {Goodfellow_2017_NIPS 2016 Tutorial - Generative Adversarial Networks.pdf:/Users/markwang/DropBox (MIT)/zotero/Goodfellow_2017_NIPS 2016 Tutorial - Generative Adversarial Networks.pdf:application/pdf}
}

@article{sonderbyAmortisedMAPInference2017,
	title = {Amortised {MAP} Inference for Image Super-resolution},
	url = {http://arxiv.org/abs/1610.04490},
	abstract = {Image super-resolution ({SR}) is an underdetermined inverse problem, where a large number of plausible high-resolution images can explain the same downsampled image. Most current single image {SR} methods use empirical risk minimisation, often with a pixel-wise mean squared error ({MSE}) loss. However, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori ({MAP}) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct {MAP} estimation for {SR} is non-trivial, as it requires us to build a model for the image prior from samples. Furthermore, {MAP} inference is often performed via optimisation-based iterative algorithms which don't compare well with the efficiency of neural-network-based alternatives. Here we introduce new methods for amortised {MAP} inference whereby we calculate the {MAP} estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid {SR} solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised {MAP} inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks ({GAN}) (2) denoiser-guided {SR} which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the {GAN} based approach performs best on real image data. Lastly, we establish a connection between {GANs} and amortised variational inference as in e.g. variational autoencoders.},
	journaltitle = {{arXiv}:1610.04490 [cs, stat]},
	author = {Sønderby, Casper Kaae and Caballero, Jose and Theis, Lucas and Shi, Wenzhe and Huszár, Ferenc},
	urldate = {2020-01-10},
	date = {2017-02-21},
	eprinttype = {arxiv},
	eprint = {1610.04490},
	file = {Sonderby et al_2017_Amortised MAP Inference for Image Super-resolution.pdf:/Users/markwang/DropBox (MIT)/zotero/Sonderby et al_2017_Amortised MAP Inference for Image Super-resolution.pdf:application/pdf}
}

@article{ledigPhotoRealisticSingleImage2017,
	title = {Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network},
	url = {http://arxiv.org/abs/1609.04802},
	abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present {SRGAN}, a generative adversarial network ({GAN}) for image super-resolution ({SR}). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score ({MOS}) test shows hugely significant gains in perceptual quality using {SRGAN}. The {MOS} scores obtained with {SRGAN} are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
	journaltitle = {{arXiv}:1609.04802 [cs, stat]},
	author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
	urldate = {2020-01-11},
	date = {2017-05-25},
	eprinttype = {arxiv},
	eprint = {1609.04802},
	file = {Ledig et al_2017_Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.pdf:/Users/markwang/DropBox (MIT)/zotero/Ledig et al_2017_Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.pdf:application/pdf}
}

@article{mirzaConditionalGenerativeAdversarial2014,
	title = {Conditional Generative Adversarial Nets},
	url = {http://arxiv.org/abs/1411.1784},
	abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate {MNIST} digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
	journaltitle = {{arXiv}:1411.1784 [cs, stat]},
	author = {Mirza, Mehdi and Osindero, Simon},
	urldate = {2020-01-11},
	date = {2014-11-06},
	eprinttype = {arxiv},
	eprint = {1411.1784},
	file = {Mirza_Osindero_2014_Conditional Generative Adversarial Nets.pdf:/Users/markwang/DropBox (MIT)/zotero/Mirza_Osindero_2014_Conditional Generative Adversarial Nets.pdf:application/pdf}
}

@inproceedings{nguyenPlugPlayGenerative2017,
	location = {Honolulu, {HI}},
	title = {Plug \& Play Generative Networks: Conditional Iterative Generation of Images in Latent Space},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099857/},
	doi = {10.1109/CVPR.2017.374},
	shorttitle = {Plug \& Play Generative Networks},
	abstract = {Generating high-resolution, photo-realistic images has been a long-standing goal in machine learning. Recently, Nguyen et al. [37] showed one interesting way to synthesize novel images by performing gradient ascent in the latent space of a generator network to maximize the activations of one or multiple neurons in a separate classiﬁer network. In this paper we extend this method by introducing an additional prior on the latent code, improving both sample quality and sample diversity, leading to a state-of-the-art generative model that produces high quality images at higher resolutions (227 × 227) than previous generative models, and does so for all 1000 {ImageNet} categories. In addition, we provide a uniﬁed probabilistic interpretation of related activation maximization methods and call the general class of models “Plug and Play Generative Networks.” {PPGNs} are composed of 1) a generator network G that is capable of drawing a wide range of image types and 2) a replaceable “condition” network C that tells the generator what to draw. We demonstrate the generation of images conditioned on a class (when C is an {ImageNet} or {MIT} Places classiﬁcation network) and also conditioned on a caption (when C is an image captioning network). Our method also improves the state of the art of Multifaceted Feature Visualization [40], which generates the set of synthetic inputs that activate a neuron in order to better understand how deep neural networks operate. Finally, we show that our model performs reasonably well at the task of image inpainting. While image models are used in this paper, the approach is modality-agnostic and can be applied to many types of data.},
	eventtitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {3510--3520},
	booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Nguyen, Anh and Clune, Jeff and Bengio, Yoshua and Dosovitskiy, Alexey and Yosinski, Jason},
	urldate = {2020-01-16},
	date = {2017-07},
	langid = {english},
	file = {Nguyen et al_2017_Plug & Play Generative Networks - Conditional Iterative Generation of Images in Latent Space.pdf:/Users/markwang/DropBox (MIT)/zotero/Nguyen et al_2017_Plug & Play Generative Networks - Conditional Iterative Generation of Images in Latent Space.pdf:application/pdf}
}

@article{choiStarGANUnifiedGenerative2018,
	title = {{StarGAN}: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation},
	url = {http://arxiv.org/abs/1711.09020},
	shorttitle = {{StarGAN}},
	abstract = {Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose {StarGAN}, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of {StarGAN} allows simultaneous training of multiple datasets with different domains within a single network. This leads to {StarGAN}'s superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.},
	journaltitle = {{arXiv}:1711.09020 [cs]},
	author = {Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul},
	urldate = {2020-01-17},
	date = {2018-09-21},
	eprinttype = {arxiv},
	eprint = {1711.09020},
	file = {Choi et al_2018_StarGAN - Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation.pdf:/Users/markwang/DropBox (MIT)/zotero/Choi et al_2018_StarGAN - Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation.pdf:application/pdf}
}

@article{mohamedLearningImplicitGenerative2017,
	title = {Learning in Implicit Generative Models},
	url = {http://arxiv.org/abs/1610.03483},
	abstract = {Generative adversarial networks ({GANs}) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop our understanding of {GANs} with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame {GANs} within the wider landscape of algorithms for learning in implicit generative models--models that only specify a stochastic procedure with which to generate data--and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by {GANs}, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the {GAN} literature, and we synthesise these views to form an understanding in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.},
	journaltitle = {{arXiv}:1610.03483 [cs, stat]},
	author = {Mohamed, Shakir and Lakshminarayanan, Balaji},
	urldate = {2020-01-27},
	date = {2017-02-27},
	eprinttype = {arxiv},
	eprint = {1610.03483},
	file = {Mohamed_Lakshminarayanan_2017_Learning in Implicit Generative Models.pdf:/Users/markwang/DropBox (MIT)/zotero/Mohamed_Lakshminarayanan_2017_Learning in Implicit Generative Models.pdf:application/pdf}
}

@inproceedings{odenaConditionalImageSynthesis2017,
	title = {Conditional Image Synthesis with Auxiliary Classifier {GANs}},
	url = {http://proceedings.mlr.press/v70/odena17a.html},
	abstract = {In this paper we introduce new methods for the improved training of generative adversarial networks ({GANs}) for image synthesis. We construct a variant of {GANs} employing label conditioning that resu...},
	eventtitle = {International Conference on Machine Learning},
	pages = {2642--2651},
	booktitle = {International Conference on Machine Learning},
	author = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
	urldate = {2020-02-03},
	date = {2017-07-17},
	langid = {english},
	file = {Odena et al_2017_Conditional Image Synthesis with Auxiliary Classifier GANs.pdf:/Users/markwang/DropBox (MIT)/zotero/Odena et al_2017_Conditional Image Synthesis with Auxiliary Classifier GANs2.pdf:application/pdf}
}

@article{miyatoCGANsProjectionDiscriminator2018,
	title = {{cGANs} with Projection Discriminator},
	url = {http://arxiv.org/abs/1802.05637},
	abstract = {We propose a novel, projection based way to incorporate the conditional information into the discriminator of {GANs} that respects the role of the conditional information in the underlining probabilistic model. This approach is in contrast with most frameworks of conditional {GANs} used in application today, which use the conditional information by concatenating the (embedded) conditional vector to the feature vectors. With this modification, we were able to significantly improve the quality of the class conditional image generation on {ILSVRC}2012 ({ImageNet}) 1000-class image dataset from the current state-of-the-art result, and we achieved this with a single pair of a discriminator and a generator. We were also able to extend the application to super-resolution and succeeded in producing highly discriminative super-resolution images. This new structure also enabled high quality category transformation based on parametric functional transformation of conditional batch normalization layers in the generator.},
	journaltitle = {{arXiv}:1802.05637 [cs, stat]},
	author = {Miyato, Takeru and Koyama, Masanori},
	urldate = {2020-02-03},
	date = {2018-08-14},
	eprinttype = {arxiv},
	eprint = {1802.05637},
	file = {Miyato_Koyama_2018_cGANs with Projection Discriminator.pdf:/Users/markwang/DropBox (MIT)/zotero/Miyato_Koyama_2018_cGANs with Projection Discriminator.pdf:application/pdf}
}