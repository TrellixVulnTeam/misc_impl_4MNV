
@article{haufeInterpretationWeightVectors2014,
	title = {On the interpretation of weight vectors of linear models in multivariate neuroimaging},
	volume = {87},
	issn = {10538119},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811913010914},
	doi = {10.1016/j.neuroimage.2013.10.067},
	abstract = {The increase in spatiotemporal resolution of neuroimaging devices is accompanied by a trend towards more powerful multivariate analysis methods. Often it is desired to interpret the outcome of these methods with respect to the cognitive processes under study. Here we discuss which methods allow for such interpretations, and provide guidelines for choosing an appropriate analysis for a given experimental goal: For a surgeon who needs to decide where to remove brain tissue it is most important to determine the origin of cognitive functions and associated neural processes. In contrast, when communicating with paralyzed or comatose patients via brain–computer interfaces, it is most important to accurately extract the neural processes speciﬁc to a certain mental state. These equally important but complementary objectives require different analysis methods. Determining the origin of neural processes in time or space from the parameters of a data-driven model requires what we call a forward model of the data; such a model explains how the measured data was generated from the neural sources. Examples are general linear models ({GLMs}). Methods for the extraction of neural information from data can be considered as backward models, as they attempt to reverse the data generating process. Examples are multivariate classiﬁers. Here we demonstrate that the parameters of forward models are neurophysiologically interpretable in the sense that signiﬁcant nonzero weights are only observed at channels the activity of which is related to the brain process under study. In contrast, the interpretation of backward model parameters can lead to wrong conclusions regarding the spatial or temporal origin of the neural signals of interest, since signiﬁcant nonzero weights may also be observed at channels the activity of which is statistically independent of the brain process under study. As a remedy for the linear case, we propose a procedure for transforming backward models into forward models. This procedure enables the neurophysiological interpretation of the parameters of linear backward models. We hope that this work raises awareness for an often encountered problem and provides a theoretical basis for conducting better interpretable multivariate neuroimaging analyses.},
	pages = {96--110},
	journaltitle = {{NeuroImage}},
	shortjournal = {{NeuroImage}},
	author = {Haufe, Stefan and Meinecke, Frank and Görgen, Kai and Dähne, Sven and Haynes, John-Dylan and Blankertz, Benjamin and Bießmann, Felix},
	urldate = {2020-01-22},
	date = {2014-02},
	langid = {english},
	keywords = {tbr},
	file = {Haufe et al_2014_On the interpretation of weight vectors of linear models in multivariate neuroimaging.pdf:/Users/wpq/Dropbox (MIT)/zotero/Haufe et al_2014_On the interpretation of weight vectors of linear models in multivariate neuroimaging.pdf:application/pdf}
}

@article{dabkowskiRealTimeImage2017,
	title = {Real Time Image Saliency for Black Box Classifiers},
	url = {http://arxiv.org/abs/1705.07857},
	abstract = {In this work we develop a fast saliency detection method that can be applied to any differentiable image classifier. We train a masking model to manipulate the scores of the classifier by masking salient parts of the input image. Our model generalises well to unseen images and requires a single forward pass to perform saliency detection, therefore suitable for use in real-time systems. We test our approach on {CIFAR}-10 and {ImageNet} datasets and show that the produced saliency maps are easily interpretable, sharp, and free of artifacts. We suggest a new metric for saliency and test our method on the {ImageNet} object localisation task. We achieve results outperforming other weakly supervised methods.},
	journaltitle = {{arXiv}:1705.07857 [stat]},
	author = {Dabkowski, Piotr and Gal, Yarin},
	urldate = {2020-01-21},
	date = {2017-05-22},
	eprinttype = {arxiv},
	eprint = {1705.07857},
	keywords = {read},
	file = {Dabkowski_Gal_2017_Real Time Image Saliency for Black Box Classifiers.pdf:/Users/wpq/Dropbox (MIT)/zotero/Dabkowski_Gal_2017_Real Time Image Saliency for Black Box Classifiers.pdf:application/pdf}
}

@article{fongInterpretableExplanationsBlack2017,
	title = {Interpretable Explanations of Black Boxes by Meaningful Perturbation},
	url = {http://arxiv.org/abs/1704.03296},
	doi = {10.1109/ICCV.2017.371},
	abstract = {As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks "look" in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we specialise the framework to find the part of an image most responsible for a classifier decision. Unlike previous works, our method is model-agnostic and testable because it is grounded in explicit and interpretable image perturbations.},
	pages = {3449--3457},
	journaltitle = {2017 {IEEE} International Conference on Computer Vision ({ICCV})},
	author = {Fong, Ruth and Vedaldi, Andrea},
	urldate = {2020-01-21},
	date = {2017-10},
	eprinttype = {arxiv},
	eprint = {1704.03296},
	keywords = {read},
	file = {Fong_Vedaldi_2017_Interpretable Explanations of Black Boxes by Meaningful Perturbation.pdf:/Users/wpq/Dropbox (MIT)/zotero/Fong_Vedaldi_2017_Interpretable Explanations of Black Boxes by Meaningful Perturbation.pdf:application/pdf}
}

@article{chenLearningExplainInformationTheoretic2018,
	title = {Learning to Explain: An Information-Theoretic Perspective on Model Interpretation},
	url = {http://arxiv.org/abs/1802.07814},
	shorttitle = {Learning to Explain},
	abstract = {We introduce instancewise feature selection as a methodology for model interpretation. Our method is based on learning a function to extract a subset of features that are most informative for each given example. This feature selector is trained to maximize the mutual information between selected features and the response variable, where the conditional distribution of the response variable given the input is the model to be explained. We develop an efficient variational approximation to the mutual information, and show the effectiveness of our method on a variety of synthetic and real data sets using both quantitative metrics and human evaluation.},
	journaltitle = {{arXiv}:1802.07814 [cs, stat]},
	author = {Chen, Jianbo and Song, Le and Wainwright, Martin J. and Jordan, Michael I.},
	urldate = {2020-01-20},
	date = {2018-06-13},
	eprinttype = {arxiv},
	eprint = {1802.07814},
	keywords = {good, read},
	file = {Chen et al_2018_Learning to Explain - An Information-Theoretic Perspective on Model Interpretation.pdf:/Users/wpq/Dropbox (MIT)/zotero/Chen et al_2018_Learning to Explain - An Information-Theoretic Perspective on Model Interpretation.pdf:application/pdf}
}

@article{rossRightRightReasons2017,
	title = {Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations},
	url = {http://arxiv.org/abs/1703.03717},
	shorttitle = {Right for the Right Reasons},
	abstract = {Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. {LIME}) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.},
	journaltitle = {{arXiv}:1703.03717 [cs, stat]},
	author = {Ross, Andrew Slavin and Hughes, Michael C. and Doshi-Velez, Finale},
	urldate = {2020-01-20},
	date = {2017-05-25},
	eprinttype = {arxiv},
	eprint = {1703.03717},
	keywords = {read},
	file = {Ross et al_2017_Right for the Right Reasons - Training Differentiable Models by Constraining their Explanations.pdf:/Users/wpq/Dropbox (MIT)/zotero/Ross et al_2017_Right for the Right Reasons - Training Differentiable Models by Constraining their Explanations.pdf:application/pdf}
}

@article{janizekAdversarialApproachRobust2020,
	title = {An Adversarial Approach for the Robust Classification of Pneumonia from Chest Radiographs},
	url = {http://arxiv.org/abs/2001.04051},
	abstract = {While deep learning has shown promise in the domain of disease classification from medical images, models based on state-of-the-art convolutional neural network architectures often exhibit performance loss due to dataset shift. Models trained using data from one hospital system achieve high predictive performance when tested on data from the same hospital, but perform significantly worse when they are tested in different hospital systems. Furthermore, even within a given hospital system, deep learning models have been shown to depend on hospital- and patient-level confounders rather than meaningful pathology to make classifications. In order for these models to be safely deployed, we would like to ensure that they do not use confounding variables to make their classification, and that they will work well even when tested on images from hospitals that were not included in the training data. We attempt to address this problem in the context of pneumonia classification from chest radiographs. We propose an approach based on adversarial optimization, which allows us to learn more robust models that do not depend on confounders. Specifically, we demonstrate improved out-of-hospital generalization performance of a pneumonia classifier by training a model that is invariant to the view position of chest radiographs (anterior-posterior vs. posterior-anterior). Our approach leads to better predictive performance on external hospital data than both a standard baseline and previously proposed methods to handle confounding, and also suggests a method for identifying models that may rely on confounders. Code available at https://github.com/suinleelab/cxr\_adv.},
	journaltitle = {{arXiv}:2001.04051 [cs, eess, stat]},
	author = {Janizek, Joseph D. and Erion, Gabriel and {DeGrave}, Alex J. and Lee, Su-In},
	urldate = {2020-01-20},
	date = {2020-01-12},
	eprinttype = {arxiv},
	eprint = {2001.04051},
	keywords = {tbr},
	file = {Janizek et al_2020_An Adversarial Approach for the Robust Classification of Pneumonia from Chest Radiographs.pdf:/Users/wpq/Dropbox (MIT)/zotero/Janizek et al_2020_An Adversarial Approach for the Robust Classification of Pneumonia from Chest Radiographs.pdf:application/pdf}
}

@article{adebayoSanityChecksSaliency2018,
	title = {Sanity Checks for Saliency Maps},
	url = {http://arxiv.org/abs/1810.03292},
	abstract = {Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.},
	journaltitle = {{arXiv}:1810.03292 [cs, stat]},
	author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
	urldate = {2020-01-20},
	date = {2018-10-27},
	eprinttype = {arxiv},
	eprint = {1810.03292},
	keywords = {read},
	file = {Adebayo et al_2018_Sanity Checks for Saliency Maps.pdf:/Users/wpq/Dropbox (MIT)/zotero/Adebayo et al_2018_Sanity Checks for Saliency Maps.pdf:application/pdf}
}

@article{riegerInterpretationsAreUseful2019,
	title = {Interpretations are useful: penalizing explanations to align neural networks with prior knowledge},
	url = {http://arxiv.org/abs/1909.13584},
	shorttitle = {Interpretations are useful},
	abstract = {For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective. Too often, the litany of proposed explainable deep learning methods stop at the first step, providing practitioners with insight into a model, but no way to act on it. In this paper, we propose contextual decomposition explanation penalization ({CDEP}), a method which enables practitioners to leverage existing explanation methods in order to increase the predictive accuracy of deep learning models. In particular, when shown that a model has incorrectly assigned importance to some features, {CDEP} enables practitioners to correct these errors by directly regularizing the provided explanations. Using explanations provided by contextual decomposition ({CD}) (Murdoch et al., 2018), we demonstrate the ability of our method to increase performance on an array of toy and real datasets.},
	journaltitle = {{arXiv}:1909.13584 [cs, stat]},
	author = {Rieger, Laura and Singh, Chandan and Murdoch, W. James and Yu, Bin},
	urldate = {2020-01-20},
	date = {2019-10-01},
	eprinttype = {arxiv},
	eprint = {1909.13584},
	keywords = {tbr},
	file = {Rieger et al_2019_Interpretations are useful - penalizing explanations to align neural networks with prior knowledge.pdf:/Users/wpq/Dropbox (MIT)/zotero/Rieger et al_2019_Interpretations are useful - penalizing explanations to align neural networks with prior knowledge.pdf:application/pdf}
}

@article{lipovetskyAnalysisRegressionGame2001,
	title = {Analysis of Regression in Game Theory Approach},
	volume = {17},
	doi = {10.1002/asmb.446},
	abstract = {Working with multiple regression analysis a researcher usually wants to know a comparative importance of predictors in the model. However, the analysis can be made difficult because of multicollinearity among regressors, which produces biased coefficients and negative inputs to multiple determination from presum ably useful regressors. To solve this problem we apply a tool from the co-operative games theory, the Shapley Value imputation. We demonstrate the theoretical and practical advantages of the Shapley Value and show that it provides consistent results in the presence of multicollinearity. Copyright © 2001 John Wiley \& Sons, Ltd.},
	pages = {319--330},
	journaltitle = {Applied Stochastic Models in Business and Industry},
	shortjournal = {Applied Stochastic Models in Business and Industry},
	author = {Lipovetsky, Stan and Conklin, Michael},
	date = {2001-10-01},
	keywords = {tbr},
	file = {Lipovetsky_Conklin_2001_Analysis of Regression in Game Theory Approach.pdf:/Users/wpq/Dropbox (MIT)/zotero/Lipovetsky_Conklin_2001_Analysis of Regression in Game Theory Approach.pdf:application/pdf}
}

@incollection{yehFidelitySensitivityExplanations2019,
	title = {On the (In)fidelity and Sensitivity of Explanations},
	url = {http://papers.nips.cc/paper/9278-on-the-infidelity-and-sensitivity-of-explanations.pdf},
	pages = {10965--10976},
	booktitle = {Advances in Neural Information Processing Systems 32},
	publisher = {Curran Associates, Inc.},
	author = {Yeh, Chih-Kuan and Hsieh, Cheng-Yu and Suggala, Arun and Inouye, David I and Ravikumar, Pradeep K},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d\{{\textbackslash}textbackslash\}textquotesingle and Fox, E. and Garnett, R.},
	urldate = {2020-01-20},
	date = {2019},
	keywords = {tbr},
	file = {Yeh et al_2019_On the (In)fidelity and Sensitivity of Explanations.pdf:/Users/wpq/Dropbox (MIT)/zotero/Yeh et al_2019_On the (In)fidelity and Sensitivity of Explanations.pdf:application/pdf}
}

@article{leeRobustLocallyLinear2019,
	title = {Towards Robust, Locally Linear Deep Networks},
	url = {http://arxiv.org/abs/1907.03207},
	abstract = {Deep networks realize complex mappings that are often understood by their locally linear behavior at or around points of interest. For example, we use the derivative of the mapping with respect to its inputs for sensitivity analysis, or to explain (obtain coordinate relevance for) a prediction. One key challenge is that such derivatives are themselves inherently unstable. In this paper, we propose a new learning problem to encourage deep networks to have stable derivatives over larger regions. While the problem is challenging in general, we focus on networks with piecewise linear activation functions. Our algorithm consists of an inference step that identifies a region around a point where linear approximation is provably stable, and an optimization step to expand such regions. We propose a novel relaxation to scale the algorithm to realistic models. We illustrate our method with residual and recurrent networks on image and sequence datasets.},
	journaltitle = {{arXiv}:1907.03207 [cs, stat]},
	author = {Lee, Guang-He and Alvarez-Melis, David and Jaakkola, Tommi S.},
	urldate = {2020-01-20},
	date = {2019-07-06},
	eprinttype = {arxiv},
	eprint = {1907.03207},
	keywords = {tbr},
	file = {Lee et al_2019_Towards Robust, Locally Linear Deep Networks.pdf:/Users/wpq/Dropbox (MIT)/zotero/Lee et al_2019_Towards Robust, Locally Linear Deep Networks.pdf:application/pdf}
}

@article{alvarez-melisRobustInterpretabilitySelfExplaining2018,
	title = {Towards Robust Interpretability with Self-Explaining Neural Networks},
	url = {http://arxiv.org/abs/1806.07538},
	abstract = {Most recent work on interpretability of complex machine learning models has focused on estimating \${\textbackslash}textit\{a posteriori\}\$ explanations for previously trained models around specific predictions. \${\textbackslash}textit\{Self-explaining\}\$ models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general -- explicitness, faithfulness, and stability -- and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.},
	journaltitle = {{arXiv}:1806.07538 [cs, stat]},
	author = {Alvarez-Melis, David and Jaakkola, Tommi S.},
	urldate = {2020-01-20},
	date = {2018-12-03},
	eprinttype = {arxiv},
	eprint = {1806.07538},
	keywords = {good, read},
	file = {Alvarez-Melis_Jaakkola_2018_Towards Robust Interpretability with Self-Explaining Neural Networks.pdf:/Users/wpq/Dropbox (MIT)/zotero/Alvarez-Melis_Jaakkola_2018_Towards Robust Interpretability with Self-Explaining Neural Networks.pdf:application/pdf}
}

@inproceedings{gilpinExplainingExplanationsOverview2018,
	title = {Explaining Explanations: An Overview of Interpretability of Machine Learning},
	doi = {10.1109/DSAA.2018.00018},
	shorttitle = {Explaining Explanations},
	abstract = {There has recently been a surge of work in explanatory artificial intelligence ({XAI}). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. {XAI} allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.},
	eventtitle = {2018 {IEEE} 5th International Conference on Data Science and Advanced Analytics ({DSAA})},
	pages = {80--89},
	booktitle = {2018 {IEEE} 5th International Conference on Data Science and Advanced Analytics ({DSAA})},
	author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
	date = {2018-10},
	note = {{ISSN}: null},
	keywords = {tbr},
	file = {Gilpin et al_2018_Explaining Explanations - An Overview of Interpretability of Machine Learning.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gilpin et al_2018_Explaining Explanations - An Overview of Interpretability of Machine Learning.pdf:application/pdf}
}

@article{robnik-sikonjaExplainingClassificationsIndividual2008,
	title = {Explaining Classifications For Individual Instances},
	volume = {20},
	issn = {2326-3865},
	doi = {10.1109/TKDE.2007.190734},
	abstract = {We present a method for explaining predictions for individual instances. The presented approach is general and can be used with all classification models that output probabilities. It is based on the decomposition of a model's predictions on individual contributions of each attribute. Our method works for the so-called black box models such as support vector machines, neural networks, and nearest neighbor algorithms, as well as for ensemble methods such as boosting and random forests. We demonstrate that the generated explanations closely follow the learned models and present a visualization technique that shows the utility of our approach and enables the comparison of different prediction methods.},
	pages = {589--600},
	number = {5},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Robnik-Šikonja, Marko and Kononenko, Igor},
	date = {2008-05},
	keywords = {tbr},
	file = {Robnik-Sikonja_Kononenko_2008_Explaining Classifications For Individual Instances.pdf:/Users/wpq/Dropbox (MIT)/zotero/Robnik-Sikonja_Kononenko_2008_Explaining Classifications For Individual Instances.pdf:application/pdf}
}

@article{montavonExplainingNonLinearClassification2017,
	title = {Explaining {NonLinear} Classification Decisions with Deep Taylor Decomposition},
	volume = {65},
	issn = {00313203},
	url = {http://arxiv.org/abs/1512.02479},
	doi = {10.1016/j.patcog.2016.11.008},
	abstract = {Nonlinear methods such as Deep Neural Networks ({DNNs}) are the gold standard for various challenging machine learning problems, e.g., image classification, natural language processing or human action recognition. Although these methods perform impressively well, they have a significant disadvantage, the lack of transparency, limiting the interpretability of the solution and thus the scope of application in practice. Especially {DNNs} act as black boxes due to their multilayer nonlinear structure. In this paper we introduce a novel methodology for interpreting generic multilayer neural networks by decomposing the network classification decision into contributions of its input elements. Although our focus is on image classification, the method is applicable to a broad set of input data, learning tasks and network architectures. Our method is based on deep Taylor decomposition and efficiently utilizes the structure of the network by backpropagating the explanations from the output to the input layer. We evaluate the proposed method empirically on the {MNIST} and {ILSVRC} data sets.},
	pages = {211--222},
	journaltitle = {Pattern Recognition},
	shortjournal = {Pattern Recognition},
	author = {Montavon, Grégoire and Bach, Sebastian and Binder, Alexander and Samek, Wojciech and Müller, Klaus-Robert},
	urldate = {2020-01-18},
	date = {2017-05},
	eprinttype = {arxiv},
	eprint = {1512.02479},
	keywords = {tbr},
	file = {Montavon et al_2017_Explaining NonLinear Classification Decisions with Deep Taylor Decomposition.pdf:/Users/wpq/Dropbox (MIT)/zotero/Montavon et al_2017_Explaining NonLinear Classification Decisions with Deep Taylor Decomposition.pdf:application/pdf}
}

@article{bachPixelWiseExplanationsNonLinear2015,
	title = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
	volume = {10},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140},
	doi = {10.1371/journal.pone.0130140},
	abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on {PASCAL} {VOC} 2009 images, synthetic image data containing geometric shapes, the {MNIST} handwritten digits data set and for the pre-trained {ImageNet} model available as part of the Caffe open source package.},
	pages = {e0130140},
	number = {7},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Bach, Sebastian and Binder, Alexander and Montavon, Grégoire and Klauschen, Frederick and Müller, Klaus-Robert and Samek, Wojciech},
	urldate = {2020-01-18},
	date = {2015-07-10},
	langid = {english},
	keywords = {tbr},
	file = {Bach et al_2015_On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation.pdf:/Users/wpq/Dropbox (MIT)/zotero/Bach et al_2015_On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation.pdf:application/pdf}
}

@article{baehrensHowExplainIndividual2009,
	title = {How to Explain Individual Classification Decisions},
	url = {http://arxiv.org/abs/0912.1128},
	abstract = {After building a classifier with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted the particular label for a single instance and what features were most influential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classification method.},
	journaltitle = {{arXiv}:0912.1128 [cs, stat]},
	author = {Baehrens, David and Schroeter, Timon and Harmeling, Stefan and Kawanabe, Motoaki and Hansen, Katja and Mueller, Klaus-Robert},
	urldate = {2020-01-18},
	date = {2009-12-06},
	eprinttype = {arxiv},
	eprint = {0912.1128},
	keywords = {good, read},
	file = {Baehrens et al_2009_How to Explain Individual Classification Decisions.pdf:/Users/wpq/Dropbox (MIT)/zotero/Baehrens et al_2009_How to Explain Individual Classification Decisions.pdf:application/pdf}
}

@article{zhouLearningDeepFeatures2015,
	title = {Learning Deep Features for Discriminative Localization},
	url = {http://arxiv.org/abs/1512.04150},
	abstract = {In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1\% top-5 error for object localization on {ILSVRC} 2014, which is remarkably close to the 34.2\% top-5 error achieved by a fully supervised {CNN} approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them},
	journaltitle = {{arXiv}:1512.04150 [cs]},
	author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
	urldate = {2020-01-17},
	date = {2015-12-13},
	eprinttype = {arxiv},
	eprint = {1512.04150},
	keywords = {read},
	file = {Zhou et al_2015_Learning Deep Features for Discriminative Localization.pdf:/Users/wpq/Dropbox (MIT)/zotero/Zhou et al_2015_Learning Deep Features for Discriminative Localization.pdf:application/pdf}
}

@article{zintgrafVisualizingDeepNeural2017,
	title = {Visualizing Deep Neural Network Decisions: Prediction Difference Analysis},
	url = {http://arxiv.org/abs/1702.04595},
	shorttitle = {Visualizing Deep Neural Network Decisions},
	abstract = {This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images ({ImageNet} data), as well as medical images ({MRI} brain scans).},
	journaltitle = {{arXiv}:1702.04595 [cs]},
	author = {Zintgraf, Luisa M. and Cohen, Taco S. and Adel, Tameem and Welling, Max},
	urldate = {2020-01-17},
	date = {2017-02-15},
	eprinttype = {arxiv},
	eprint = {1702.04595},
	keywords = {good, read},
	file = {Zintgraf et al_2017_Visualizing Deep Neural Network Decisions - Prediction Difference Analysis.pdf:/Users/wpq/Dropbox (MIT)/zotero/Zintgraf et al_2017_Visualizing Deep Neural Network Decisions - Prediction Difference Analysis.pdf:application/pdf}
}

@article{leiRationalizingNeuralPredictions2016,
	title = {Rationalizing Neural Predictions},
	url = {http://arxiv.org/abs/1606.04155},
	abstract = {Prediction without justification has limited applicability. As a remedy, we learn to extract pieces of input text as justifications -- rationales -- that are tailored to be short and coherent, yet sufficient for making the same prediction. Our approach combines two modular components, generator and encoder, which are trained to operate well together. The generator specifies a distribution over text fragments as candidate rationales and these are passed through the encoder for prediction. Rationales are never given during training. Instead, the model is regularized by desiderata for rationales. We evaluate the approach on multi-aspect sentiment analysis against manually annotated test cases. Our approach outperforms attention-based baseline by a significant margin. We also successfully illustrate the method on the question retrieval task.},
	journaltitle = {{arXiv}:1606.04155 [cs]},
	author = {Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
	urldate = {2020-01-17},
	date = {2016-11-02},
	eprinttype = {arxiv},
	eprint = {1606.04155},
	keywords = {read},
	file = {Lei et al_2016_Rationalizing Neural Predictions.pdf:/Users/wpq/Dropbox (MIT)/zotero/Lei et al_2016_Rationalizing Neural Predictions.pdf:application/pdf}
}

@article{carterWhatMadeYou2019,
	title = {What made you do this? Understanding black-box decisions with sufficient input subsets},
	url = {http://arxiv.org/abs/1810.03805},
	shorttitle = {What made you do this?},
	abstract = {Local explanation frameworks aim to rationalize particular decisions made by a black-box prediction model. Existing techniques are often restricted to a specific type of predictor or based on input saliency, which may be undesirably sensitive to factors unrelated to the model's decision making process. We instead propose sufficient input subsets that identify minimal subsets of features whose observed values alone suffice for the same decision to be reached, even if all other input feature values are missing. General principles that globally govern a model's decision-making can also be revealed by searching for clusters of such input patterns across many data points. Our approach is conceptually straightforward, entirely model-agnostic, simply implemented using instance-wise backward selection, and able to produce more concise rationales than existing techniques. We demonstrate the utility of our interpretation method on various neural network models trained on text, image, and genomic data.},
	journaltitle = {{arXiv}:1810.03805 [cs, stat]},
	author = {Carter, Brandon and Mueller, Jonas and Jain, Siddhartha and Gifford, David},
	urldate = {2020-01-16},
	date = {2019-02-08},
	eprinttype = {arxiv},
	eprint = {1810.03805},
	keywords = {good, read},
	file = {Carter et al_2019_What made you do this - Understanding black-box decisions with sufficient input subsets.pdf:/Users/wpq/Dropbox (MIT)/zotero/Carter et al_2019_What made you do this - Understanding black-box decisions with sufficient input subsets.pdf:application/pdf;Carter et al_2019_What made you do this - Understanding black-box decisions with sufficient input subsets.pdf:/Users/wpq/Dropbox (MIT)/zotero/Carter et al_2019_What made you do this - Understanding black-box decisions with sufficient input subsets2.pdf:application/pdf}
}

@article{zeilerVisualizingUnderstandingConvolutional2013,
	title = {Visualizing and Understanding Convolutional Networks},
	url = {http://arxiv.org/abs/1311.2901},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the {ImageNet} benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky {\textbackslash}etal on the {ImageNet} classification benchmark. We show our {ImageNet} model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	journaltitle = {{arXiv}:1311.2901 [cs]},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	urldate = {2020-01-16},
	date = {2013-11-28},
	eprinttype = {arxiv},
	eprint = {1311.2901},
	keywords = {skim},
	file = {Zeiler_Fergus_2013_Visualizing and Understanding Convolutional Networks.pdf:/Users/wpq/Dropbox (MIT)/zotero/Zeiler_Fergus_2013_Visualizing and Understanding Convolutional Networks.pdf:application/pdf}
}

@article{kindermansReliabilitySaliencyMethods2017,
	title = {The (Un)reliability of saliency methods},
	url = {http://arxiv.org/abs/1711.00867},
	abstract = {Saliency methods aim to explain the predictions of deep neural networks. These methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction. We use a simple and common pre-processing step ---adding a constant shift to the input data--- to show that a transformation with no effect on the model can cause numerous methods to incorrectly attribute. In order to guarantee reliability, we posit that methods should fulfill input invariance, the requirement that a saliency method mirror the sensitivity of the model with respect to transformations of the input. We show, through several examples, that saliency methods that do not satisfy input invariance result in misleading attribution.},
	journaltitle = {{arXiv}:1711.00867 [cs, stat]},
	author = {Kindermans, Pieter-Jan and Hooker, Sara and Adebayo, Julius and Alber, Maximilian and Schütt, Kristof T. and Dähne, Sven and Erhan, Dumitru and Kim, Been},
	urldate = {2020-01-16},
	date = {2017-11-02},
	eprinttype = {arxiv},
	eprint = {1711.00867},
	keywords = {skim},
	file = {Kindermans et al_2017_The (Un)reliability of saliency methods.pdf:/Users/wpq/Dropbox (MIT)/zotero/Kindermans et al_2017_The (Un)reliability of saliency methods.pdf:application/pdf}
}

@article{kindermansLearningHowExplain2017,
	title = {Learning how to explain neural networks: {PatternNet} and {PatternAttribution}},
	url = {http://arxiv.org/abs/1705.05598},
	shorttitle = {Learning how to explain neural networks},
	abstract = {{DeConvNet}, Guided {BackProp}, {LRP}, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques ({PatternNet} and {PatternAttribution}) that are theoretically sound for linear models and produce improved explanations for deep networks.},
	journaltitle = {{arXiv}:1705.05598 [cs, stat]},
	author = {Kindermans, Pieter-Jan and Schütt, Kristof T. and Alber, Maximilian and Müller, Klaus-Robert and Erhan, Dumitru and Kim, Been and Dähne, Sven},
	urldate = {2020-01-16},
	date = {2017-10-24},
	eprinttype = {arxiv},
	eprint = {1705.05598},
	keywords = {tbr},
	file = {Kindermans et al_2017_Learning how to explain neural networks - PatternNet and PatternAttribution.pdf:/Users/wpq/Dropbox (MIT)/zotero/Kindermans et al_2017_Learning how to explain neural networks - PatternNet and PatternAttribution.pdf:application/pdf}
}

@article{nguyenSynthesizingPreferredInputs2016,
	title = {Synthesizing the preferred inputs for neurons in neural networks via deep generator networks},
	abstract = {Deep neural networks ({DNNs}) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right - similar to why we study the human brain - and will enable researchers to further improve {DNNs}. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization ({AM}), which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network ({DGN}). The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).},
	author = {Nguyen, Anh and Dosovitskiy, Alexey and Yosinski, Jason and Brox, Thomas and Clune, Jeff},
	date = {2016-05-30},
	keywords = {read},
	file = {Nguyen et al_2016_Synthesizing the preferred inputs for neurons in neural networks via deep generator networks.pdf:/Users/wpq/Dropbox (MIT)/zotero/Nguyen et al_2016_Synthesizing the preferred inputs for neurons in neural networks via deep generator networks.pdf:application/pdf}
}

@article{dhurandharExplanationsBasedMissing2018,
	title = {Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives},
	url = {http://arxiv.org/abs/1802.07623},
	shorttitle = {Explanations based on the Missing},
	abstract = {In this paper we propose a novel method that provides contrastive explanations justifying the classification of an input by a black box classifier such as a deep neural network. Given an input we find what should be \%necessarily and minimally and sufficiently present (viz. important object pixels in an image) to justify its classification and analogously what should be minimally and necessarily {\textbackslash}emph\{absent\} (viz. certain background pixels). We argue that such explanations are natural for humans and are used commonly in domains such as health care and criminology. What is minimally but critically {\textbackslash}emph\{absent\} is an important part of an explanation, which to the best of our knowledge, has not been explicitly identified by current explanation methods that explain predictions of neural networks. We validate our approach on three real datasets obtained from diverse domains; namely, a handwritten digits dataset {MNIST}, a large procurement fraud dataset and a brain activity strength dataset. In all three cases, we witness the power of our approach in generating precise explanations that are also easy for human experts to understand and evaluate.},
	journaltitle = {{arXiv}:1802.07623 [cs]},
	author = {Dhurandhar, Amit and Chen, Pin-Yu and Luss, Ronny and Tu, Chun-Chen and Ting, Paishun and Shanmugam, Karthikeyan and Das, Payel},
	urldate = {2020-01-16},
	date = {2018-10-29},
	eprinttype = {arxiv},
	eprint = {1802.07623},
	keywords = {read},
	file = {Dhurandhar et al_2018_Explanations based on the Missing - Towards Contrastive Explanations with Pertinent Negatives.pdf:/Users/wpq/Dropbox (MIT)/zotero/Dhurandhar et al_2018_Explanations based on the Missing - Towards Contrastive Explanations with Pertinent Negatives.pdf:application/pdf;Dhurandhar et al_2018_Explanations based on the Missing - Towards Contrastive Explanations with Pertinent Negatives.pdf:/Users/wpq/Dropbox (MIT)/zotero/Dhurandhar et al_2018_Explanations based on the Missing - Towards Contrastive Explanations with Pertinent Negatives2.pdf:application/pdf}
}

@article{binderLayerwiseRelevancePropagation2016,
	title = {Layer-wise Relevance Propagation for Neural Networks with Local Renormalization Layers},
	url = {http://arxiv.org/abs/1604.00825},
	abstract = {Layer-wise relevance propagation is a framework which allows to decompose the prediction of a deep neural network computed over a sample, e.g. an image, down to relevance scores for the single input dimensions of the sample such as subpixels of an image. While this approach can be applied directly to generalized linear mappings, product type non-linearities are not covered. This paper proposes an approach to extend layer-wise relevance propagation to neural networks with local renormalization layers, which is a very common product-type non-linearity in convolutional neural networks. We evaluate the proposed method for local renormalization layers on the {CIFAR}-10, Imagenet and {MIT} Places datasets.},
	journaltitle = {{arXiv}:1604.00825 [cs]},
	author = {Binder, Alexander and Montavon, Grégoire and Bach, Sebastian and Müller, Klaus-Robert and Samek, Wojciech},
	urldate = {2020-01-16},
	date = {2016-04-04},
	eprinttype = {arxiv},
	eprint = {1604.00825},
	keywords = {tbr},
	file = {Binder et al_2016_Layer-wise Relevance Propagation for Neural Networks with Local Renormalization Layers.pdf:/Users/wpq/Dropbox (MIT)/zotero/Binder et al_2016_Layer-wise Relevance Propagation for Neural Networks with Local Renormalization Layers.pdf:application/pdf}
}

@article{erhanVisualizingHigherLayerFeatures2009,
	title = {Visualizing Higher-Layer Features of a Deep Network},
	abstract = {Deep architectures have demonstrated state-of-the-art results in a variety of settings, especially with vision datasets. Beyond the model deﬁnitions and the quantitative analyses, there is a need for qualitative comparisons of the solutions learned by various deep architectures. The goal of this paper is to ﬁnd good qualitative interpretations of high level features represented by such models. To this end, we contrast and compare several techniques applied on Stacked Denoising Autoencoders and Deep Belief Networks, trained on several vision datasets. We show that, perhaps counter-intuitively, such interpretation is possible at the unit level, that it is simple to accomplish and that the results are consistent across various techniques. We hope that such techniques will allow researchers in deep architectures to understand more of how and why deep architectures work.},
	pages = {13},
	author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal and Box, P O},
	date = {2009-06-09},
	langid = {english},
	keywords = {read},
	file = {Erhan et al_2009_Visualizing Higher-Layer Features of a Deep Network.pdf:/Users/wpq/Dropbox (MIT)/zotero/Erhan et al_2009_Visualizing Higher-Layer Features of a Deep Network.pdf:application/pdf}
}

@article{anconaBetterUnderstandingGradientbased2018,
	title = {Towards better understanding of gradient-based attribution methods for Deep Neural Networks},
	url = {http://arxiv.org/abs/1711.06104},
	abstract = {Understanding the flow of information in Deep Neural Networks ({DNNs}) is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, there have been only a few attempts to compare them from a theoretical perspective. What is more, no exhaustive empirical comparison has been performed in the past. In this work, we analyze four gradient-based attribution methods and formally prove conditions of equivalence and approximation between them. By reformulating two of these methods, we construct a unified framework which enables a direct comparison, as well as an easier implementation. Finally, we propose a novel evaluation metric, called Sensitivity-n and test the gradient-based attribution methods alongside with a simple perturbation-based attribution method on several datasets in the domains of image and text classification, using various network architectures.},
	journaltitle = {{arXiv}:1711.06104 [cs, stat]},
	author = {Ancona, Marco and Ceolini, Enea and Öztireli, Cengiz and Gross, Markus},
	urldate = {2020-01-16},
	date = {2018-03-07},
	eprinttype = {arxiv},
	eprint = {1711.06104},
	keywords = {tbr},
	file = {Ancona et al_2018_Towards better understanding of gradient-based attribution methods for Deep Neural Networks.pdf:/Users/wpq/Dropbox (MIT)/zotero/Ancona et al_2018_Towards better understanding of gradient-based attribution methods for Deep Neural Networks.pdf:application/pdf}
}

@article{springenbergStrivingSimplicityAll2015,
	title = {Striving for Simplicity: The All Convolutional Net},
	url = {http://arxiv.org/abs/1412.6806},
	shorttitle = {Striving for Simplicity},
	abstract = {Most modern convolutional neural networks ({CNNs}) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets ({CIFAR}-10, {CIFAR}-100, {ImageNet}). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by {CNNs}, which can be applied to a broader range of network structures than existing approaches.},
	journaltitle = {{arXiv}:1412.6806 [cs]},
	author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
	urldate = {2020-01-16},
	date = {2015-04-13},
	eprinttype = {arxiv},
	eprint = {1412.6806},
	keywords = {read},
	file = {Springenberg et al_2015_Striving for Simplicity - The All Convolutional Net.pdf:/Users/wpq/Dropbox (MIT)/zotero/Springenberg et al_2015_Striving for Simplicity - The All Convolutional Net.pdf:application/pdf}
}

@article{smilkovSmoothGradRemovingNoise2017,
	title = {{SmoothGrad}: removing noise by adding noise},
	url = {http://arxiv.org/abs/1706.03825},
	shorttitle = {{SmoothGrad}},
	abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces {SmoothGrad}, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.},
	journaltitle = {{arXiv}:1706.03825 [cs, stat]},
	author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Viégas, Fernanda and Wattenberg, Martin},
	urldate = {2020-01-16},
	date = {2017-06-12},
	eprinttype = {arxiv},
	eprint = {1706.03825},
	keywords = {read},
	file = {Smilkov et al_2017_SmoothGrad - removing noise by adding noise.pdf:/Users/wpq/Dropbox (MIT)/zotero/Smilkov et al_2017_SmoothGrad - removing noise by adding noise.pdf:application/pdf}
}

@article{sundararajanAxiomaticAttributionDeep2017,
	title = {Axiomatic Attribution for Deep Networks},
	url = {http://arxiv.org/abs/1703.01365},
	abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
	journaltitle = {{arXiv}:1703.01365 [cs]},
	author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
	urldate = {2020-01-16},
	date = {2017-06-12},
	eprinttype = {arxiv},
	eprint = {1703.01365},
	keywords = {read},
	file = {Sundararajan et al_2017_Axiomatic Attribution for Deep Networks.pdf:/Users/wpq/Dropbox (MIT)/zotero/Sundararajan et al_2017_Axiomatic Attribution for Deep Networks.pdf:application/pdf}
}

@incollection{lundbergUnifiedApproachInterpreting2017,
	title = {A Unified Approach to Interpreting Model Predictions},
	url = {http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf},
	pages = {4765--4774},
	booktitle = {Advances in Neural Information Processing Systems 30},
	publisher = {Curran Associates, Inc.},
	author = {Lundberg, Scott M and Lee, Su-In},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	urldate = {2020-01-14},
	date = {2017-09-01},
	keywords = {read},
	file = {Lundberg_Lee_2017_A Unified Approach to Interpreting Model Predictions.pdf:/Users/wpq/Dropbox (MIT)/zotero/Lundberg_Lee_2017_A Unified Approach to Interpreting Model Predictions.pdf:application/pdf;Lundberg_Lee_2017_A Unified Approach to Interpreting Model Predictions.pdf:/Users/wpq/Dropbox (MIT)/zotero/Lundberg_Lee_2017_A Unified Approach to Interpreting Model Predictions2.pdf:application/pdf;Lundberg_Lee_2017_A Unified Approach to Interpreting Model Predictions.pdf:/Users/wpq/Dropbox (MIT)/zotero/Lundberg_Lee_2017_A Unified Approach to Interpreting Model Predictions4.pdf:application/pdf;Lundberg_Lee_2017_A Unified Approach to Interpreting Model Predictions.pdf:/Users/wpq/Dropbox (MIT)/zotero/Lundberg_Lee_2017_A Unified Approach to Interpreting Model Predictions3.pdf:application/pdf}
}

@article{simonyanDeepConvolutionalNetworks2014,
	title = {Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps},
	url = {http://arxiv.org/abs/1312.6034},
	shorttitle = {Deep Inside Convolutional Networks},
	abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks ({ConvNets}). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a {ConvNet}. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification {ConvNets}. Finally, we establish the connection between the gradient-based {ConvNet} visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
	journaltitle = {{arXiv}:1312.6034 [cs]},
	author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	urldate = {2020-01-15},
	date = {2014-04-19},
	eprinttype = {arxiv},
	eprint = {1312.6034},
	keywords = {read},
	file = {Simonyan et al_2014_Deep Inside Convolutional Networks - Visualising Image Classification Models and Saliency Maps.pdf:/Users/wpq/Dropbox (MIT)/zotero/Simonyan et al_2014_Deep Inside Convolutional Networks - Visualising Image Classification Models and Saliency Maps.pdf:application/pdf}
}

@article{shrikumarLearningImportantFeatures2017,
	title = {Learning Important Features Through Propagating Activation Differences},
	abstract = {The purported "black box"' nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present {DeepLIFT} (Deep Learning Important {FeaTures}), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. {DeepLIFT} compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, {DeepLIFT} can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply {DeepLIFT} to models trained on {MNIST} and simulated genomic data, and show significant advantages over gradient-based methods. A detailed video tutorial on the method is at http://goo.gl/{qKb}7pL and code is at http://goo.gl/{RM}8jvH.},
	author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
	date = {2017-04-09},
	keywords = {read},
	file = {Shrikumar et al_2017_Learning Important Features Through Propagating Activation Differences.pdf:/Users/wpq/Dropbox (MIT)/zotero/Shrikumar et al_2017_Learning Important Features Through Propagating Activation Differences.pdf:application/pdf}
}

@article{selvarajuGradCAMVisualExplanations2016,
	title = {Grad-{CAM}: Visual Explanations from Deep Networks via Gradient-based Localization},
	issn = {0920-5691, 1573-1405},
	url = {http://arxiv.org/abs/1610.02391},
	doi = {10.1007/s11263-019-01228-7},
	shorttitle = {Grad-{CAM}},
	abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of {CNN}-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-{CAM}), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-{CAM} is applicable to a wide variety of {CNN} model-families: (1) {CNNs} with fully-connected layers, (2) {CNNs} used for structured outputs, (3) {CNNs} used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-{CAM} with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering ({VQA}) models, including {ResNet}-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and {VQA}, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-{CAM} and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-{CAM} helps users establish appropriate trust in predictions from models and show that Grad-{CAM} helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/{COjUB}9Izk6E.},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {Int J Comput Vis},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	urldate = {2020-01-15},
	date = {2016},
	eprinttype = {arxiv},
	eprint = {1610.02391},
	keywords = {tbr},
	file = {Selvaraju et al_2016_Grad-CAM - Visual Explanations from Deep Networks via Gradient-based Localization.pdf:/Users/wpq/Dropbox (MIT)/zotero/Selvaraju et al_2016_Grad-CAM - Visual Explanations from Deep Networks via Gradient-based Localization.pdf:application/pdf}
}

@article{bauNetworkDissectionQuantifying2017,
	title = {Network Dissection: Quantifying Interpretability of Deep Visual Representations},
	url = {http://arxiv.org/abs/1704.05796},
	shorttitle = {Network Dissection},
	abstract = {We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of {CNNs} by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any {CNN} model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of {CNN} models and training methods that go beyond measurements of their discriminative power.},
	journaltitle = {{arXiv}:1704.05796 [cs]},
	author = {Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
	urldate = {2020-01-15},
	date = {2017-04-19},
	eprinttype = {arxiv},
	eprint = {1704.05796},
	keywords = {tbr},
	file = {Bau et al_2017_Network Dissection - Quantifying Interpretability of Deep Visual Representations.pdf:/Users/wpq/Dropbox (MIT)/zotero/Bau et al_2017_Network Dissection - Quantifying Interpretability of Deep Visual Representations.pdf:application/pdf}
}

@article{kohUnderstandingBlackboxPredictions2017,
	title = {Understanding Black-box Predictions via Influence Functions},
	url = {http://arxiv.org/abs/1703.04730},
	abstract = {How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
	journaltitle = {{arXiv}:1703.04730 [cs, stat]},
	author = {Koh, Pang Wei and Liang, Percy},
	urldate = {2020-01-15},
	date = {2017-07-09},
	eprinttype = {arxiv},
	eprint = {1703.04730},
	keywords = {tbr, read},
	file = {Koh_Liang_2017_Understanding Black-box Predictions via Influence Functions.pdf:/Users/wpq/Dropbox (MIT)/zotero/Koh_Liang_2017_Understanding Black-box Predictions via Influence Functions.pdf:application/pdf}
}

@article{zhangInterpretableConvolutionalNeural2018,
	title = {Interpretable Convolutional Neural Networks},
	url = {http://arxiv.org/abs/1710.00935},
	abstract = {This paper proposes a method to modify traditional convolutional neural networks ({CNNs}) into interpretable {CNNs}, in order to clarify knowledge representations in high conv-layers of {CNNs}. In an interpretable {CNN}, each filter in a high conv-layer represents a certain object part. We do not need any annotations of object parts or textures to supervise the learning process. Instead, the interpretable {CNN} automatically assigns each filter in a high conv-layer with an object part during the learning process. Our method can be applied to different types of {CNNs} with different structures. The clear knowledge representation in an interpretable {CNN} can help people understand the logics inside a {CNN}, i.e., based on which patterns the {CNN} makes the decision. Experiments showed that filters in an interpretable {CNN} were more semantically meaningful than those in traditional {CNNs}.},
	journaltitle = {{arXiv}:1710.00935 [cs]},
	author = {Zhang, Quanshi and Wu, Ying Nian and Zhu, Song-Chun},
	urldate = {2020-01-15},
	date = {2018-02-14},
	eprinttype = {arxiv},
	eprint = {1710.00935},
	keywords = {read},
	file = {Zhang et al_2018_Interpretable Convolutional Neural Networks.pdf:/Users/wpq/Dropbox (MIT)/zotero/Zhang et al_2018_Interpretable Convolutional Neural Networks.pdf:application/pdf}
}

@article{zhangInterpretingCNNsDecision2019,
	title = {Interpreting {CNNs} via Decision Trees},
	url = {http://arxiv.org/abs/1802.00121},
	abstract = {This paper aims to quantitatively explain rationales of each prediction that is made by a pre-trained convolutional neural network ({CNN}). We propose to learn a decision tree, which clarifies the specific reason for each prediction made by the {CNN} at the semantic level. I.e., the decision tree decomposes feature representations in high conv-layers of the {CNN} into elementary concepts of object parts. In this way, the decision tree tells people which object parts activate which filters for the prediction and how much they contribute to the prediction score. Such semantic and quantitative explanations for {CNN} predictions have specific values beyond the traditional pixel-level analysis of {CNNs}. More specifically, our method mines all potential decision modes of the {CNN}, where each mode represents a common case of how the {CNN} uses object parts for prediction. The decision tree organizes all potential decision modes in a coarse-to-fine manner to explain {CNN} predictions at different fine-grained levels. Experiments have demonstrated the effectiveness of the proposed method.},
	journaltitle = {{arXiv}:1802.00121 [cs]},
	author = {Zhang, Quanshi and Yang, Yu and Ma, Haotian and Wu, Ying Nian},
	urldate = {2020-01-15},
	date = {2019-02-25},
	eprinttype = {arxiv},
	eprint = {1802.00121},
	keywords = {tbr},
	file = {Zhang et al_2019_Interpreting CNNs via Decision Trees.pdf:/Users/wpq/Dropbox (MIT)/zotero/Zhang et al_2019_Interpreting CNNs via Decision Trees.pdf:application/pdf}
}

@article{ribeiroWhyShouldTrust2016,
	title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
	url = {http://arxiv.org/abs/1602.04938},
	shorttitle = {"Why Should I Trust You?},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose {LIME}, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
	journaltitle = {{arXiv}:1602.04938 [cs, stat]},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	urldate = {2020-01-14},
	date = {2016-08-09},
	eprinttype = {arxiv},
	eprint = {1602.04938},
	keywords = {read},
	file = {Ribeiro et al_2016_Why Should I Trust You - - Explaining the Predictions of Any Classifier.pdf:/Users/wpq/Dropbox (MIT)/zotero/Ribeiro et al_2016_Why Should I Trust You - - Explaining the Predictions of Any Classifier.pdf:application/pdf}
}

@article{changExplainingImageClassifiers2019,
	title = {Explaining Image Classifiers by Counterfactual Generation},
	url = {http://arxiv.org/abs/1807.08024},
	abstract = {When an image classifier makes a prediction, which parts of the image are relevant and why? We can rephrase this question to ask: which parts of the image, if they were not seen by the classifier, would most change its decision? Producing an answer requires marginalizing over images that could have been seen but weren't. We can sample plausible image in-fills by conditioning a generative model on the rest of the image. We then optimize to find the image regions that most change the classifier's decision after in-fill. Our approach contrasts with ad-hoc in-filling approaches, such as blurring or injecting noise, which generate inputs far from the data distribution, and ignore informative relationships between different parts of the image. Our method produces more compact and relevant saliency maps, with fewer artifacts compared to previous methods.},
	journaltitle = {{arXiv}:1807.08024 [cs]},
	author = {Chang, Chun-Hao and Creager, Elliot and Goldenberg, Anna and Duvenaud, David},
	urldate = {2020-01-11},
	date = {2019-02-25},
	eprinttype = {arxiv},
	eprint = {1807.08024},
	keywords = {read},
	file = {Chang et al_2019_Explaining Image Classifiers by Counterfactual Generation.pdf:/Users/wpq/Dropbox (MIT)/zotero/Chang et al_2019_Explaining Image Classifiers by Counterfactual Generation.pdf:application/pdf}
}

@article{murdochInterpretableMachineLearning2019,
	title = {Interpretable machine learning: definitions, methods, and applications},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	url = {http://arxiv.org/abs/1901.04592},
	doi = {10.1073/pnas.1900654116},
	shorttitle = {Interpretable machine learning},
	abstract = {Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related, and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the Predictive, Descriptive, Relevant ({PDR}) framework for discussing interpretations. The {PDR} framework provides three overarching desiderata for evaluation: predictive accuracy, descriptive accuracy and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post-hoc categories, with sub-groups including sparsity, modularity and simulatability. To demonstrate how practitioners can use the {PDR} framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often under-appreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.},
	pages = {22071--22080},
	number = {44},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc Natl Acad Sci {USA}},
	author = {Murdoch, W. James and Singh, Chandan and Kumbier, Karl and Abbasi-Asl, Reza and Yu, Bin},
	urldate = {2019-12-11},
	date = {2019-10-29},
	eprinttype = {arxiv},
	eprint = {1901.04592},
	keywords = {tbr},
	file = {Murdoch et al_2019_Interpretable machine learning - definitions, methods, and applications.pdf:/Users/wpq/Dropbox (MIT)/zotero/Murdoch et al_2019_Interpretable machine learning - definitions, methods, and applications.pdf:application/pdf}
}

@article{doshi-velezRigorousScienceInterpretable2017,
	title = {Towards A Rigorous Science of Interpretable Machine Learning},
	url = {http://arxiv.org/abs/1702.08608},
	abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
	journaltitle = {{arXiv}:1702.08608 [cs, stat]},
	author = {Doshi-Velez, Finale and Kim, Been},
	urldate = {2019-12-11},
	date = {2017-03-02},
	eprinttype = {arxiv},
	eprint = {1702.08608},
	keywords = {tbr},
	file = {Doshi-Velez_Kim_2017_Towards A Rigorous Science of Interpretable Machine Learning.pdf:/Users/wpq/Dropbox (MIT)/zotero/Doshi-Velez_Kim_2017_Towards A Rigorous Science of Interpretable Machine Learning.pdf:application/pdf}
}

@article{guidottiSurveyMethodsExplaining2018,
	title = {A Survey of Methods for Explaining Black Box Models},
	volume = {51},
	issn = {0360-0300},
	url = {http://doi.acm.org/10.1145/3236009},
	doi = {10.1145/3236009},
	abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
	pages = {93:1--93:42},
	number = {5},
	journaltitle = {{ACM} Comput. Surv.},
	author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
	urldate = {2019-12-11},
	date = {2018-08},
	keywords = {tbr},
	file = {Guidotti et al_2018_A Survey of Methods for Explaining Black Box Models.pdf:/Users/wpq/Dropbox (MIT)/zotero/Guidotti et al_2018_A Survey of Methods for Explaining Black Box Models.pdf:application/pdf}
}

@article{joshiXGEMsGeneratingExamplars2018,
	title = {{xGEMs}: Generating Examplars to Explain Black-Box Models},
	url = {http://arxiv.org/abs/1806.08867},
	shorttitle = {{xGEMs}},
	abstract = {This work proposes {xGEMs} or manifold guided exemplars, a framework to understand black-box classifier behavior by exploring the landscape of the underlying data manifold as data points cross decision boundaries. To do so, we train an unsupervised implicit generative model -- treated as a proxy to the data manifold. We summarize black-box model behavior quantitatively by perturbing data samples along the manifold. We demonstrate {xGEMs}' ability to detect and quantify bias in model learning and also for understanding the changes in model behavior as training progresses.},
	journaltitle = {{arXiv}:1806.08867 [cs, stat]},
	author = {Joshi, Shalmali and Koyejo, Oluwasanmi and Kim, Been and Ghosh, Joydeep},
	urldate = {2019-12-08},
	date = {2018-06-22},
	eprinttype = {arxiv},
	eprint = {1806.08867},
	keywords = {read},
	file = {Joshi et al_2018_xGEMs - Generating Examplars to Explain Black-Box Models.pdf:/Users/wpq/Dropbox (MIT)/zotero/Joshi et al_2018_xGEMs - Generating Examplars to Explain Black-Box Models.pdf:application/pdf}
}

@article{zhangInterpretingAdversariallyTrained2019,
	title = {Interpreting Adversarially Trained Convolutional Neural Networks},
	url = {http://arxiv.org/abs/1905.09797},
	abstract = {We attempt to interpret how adversarially trained convolutional neural networks ({AT}-{CNNs}) recognize objects. We design systematic approaches to interpret {AT}-{CNNs} in both qualitative and quantitative ways and compare them with normally trained models. Surprisingly, we find that adversarial training alleviates the texture bias of standard {CNNs} when trained on object recognition tasks, and helps {CNNs} learn a more shape-biased representation. We validate our hypothesis from two aspects. First, we compare the salience maps of {AT}-{CNNs} and standard {CNNs} on clean images and images under different transformations. The comparison could visually show that the prediction of the two types of {CNNs} is sensitive to dramatically different types of features. Second, to achieve quantitative verification, we construct additional test datasets that destroy either textures or shapes, such as style-transferred version of clean data, saturated images and patch-shuffled ones, and then evaluate the classification accuracy of {AT}-{CNNs} and normal {CNNs} on these datasets. Our findings shed some light on why {AT}-{CNNs} are more robust than those normally trained ones and contribute to a better understanding of adversarial training over {CNNs} from an interpretation perspective.},
	journaltitle = {{arXiv}:1905.09797 [cs, stat]},
	author = {Zhang, Tianyuan and Zhu, Zhanxing},
	urldate = {2020-01-27},
	date = {2019-05-23},
	eprinttype = {arxiv},
	eprint = {1905.09797},
	keywords = {tbr},
	file = {Zhang_Zhu_2019_Interpreting Adversarially Trained Convolutional Neural Networks.pdf:/Users/wpq/Dropbox (MIT)/zotero/Zhang_Zhu_2019_Interpreting Adversarially Trained Convolutional Neural Networks.pdf:application/pdf}
}

@article{khosraviWhatExpectClassifiers2019,
	title = {What to Expect of Classifiers? Reasoning about Logistic Regression with Missing Features},
	url = {http://arxiv.org/abs/1903.01620},
	shorttitle = {What to Expect of Classifiers?},
	abstract = {While discriminative classifiers often yield strong predictive performance, missing feature values at prediction time can still be a challenge. Classifiers may not behave as expected under certain ways of substituting the missing values, since they inherently make assumptions about the data distribution they were trained on. In this paper, we propose a novel framework that classifies examples with missing features by computing the expected prediction with respect to a feature distribution. Moreover, we use geometric programming to learn a naive Bayes distribution that embeds a given logistic regression classifier and can efficiently take its expected predictions. Empirical evaluations show that our model achieves the same performance as the logistic regression with all features observed, and outperforms standard imputation techniques when features go missing during prediction time. Furthermore, we demonstrate that our method can be used to generate "sufficient explanations" of logistic regression classifications, by removing features that do not affect the classification.},
	journaltitle = {{arXiv}:1903.01620 [cs, stat]},
	author = {Khosravi, Pasha and Liang, Yitao and Choi, {YooJung} and Broeck, Guy Van den},
	urldate = {2020-01-29},
	date = {2019-06-01},
	eprinttype = {arxiv},
	eprint = {1903.01620},
	keywords = {tbr},
	file = {Khosravi et al_2019_What to Expect of Classifiers - Reasoning about Logistic Regression with Missing Features.pdf:/Users/wpq/Dropbox (MIT)/zotero/Khosravi et al_2019_What to Expect of Classifiers - Reasoning about Logistic Regression with Missing Features.pdf:application/pdf}
}

@article{burnsInterpretingBlackBox2019,
	title = {Interpreting Black Box Models via Hypothesis Testing},
	url = {http://arxiv.org/abs/1904.00045},
	abstract = {While many methods for interpreting machine learning models have been proposed, they are often ad hoc, difficult to interpret, and come with limited guarantees. This is especially problematic in science and medicine, where model interpretations may be reported as discoveries or guide patient treatments. As a step toward more principled and reliable interpretations, in this paper we reframe black box model interpretability as a multiple hypothesis testing problem. The task is to discover "important" features by testing whether the model prediction is significantly different from what would be expected if the features were replaced with uninformative counterfactuals. We propose two testing methods: one that provably controls the false discovery rate but which is not yet feasible for large-scale applications, and an approximate testing method which can be applied to real-world data sets. In simulation, both tests have high power relative to existing interpretability methods. When applied to state-of-the-art vision and language models, the framework selects features that intuitively explain model predictions. The resulting explanations have the additional advantage that they are themselves easy to interpret.},
	journaltitle = {{arXiv}:1904.00045 [cs, stat]},
	author = {Burns, Collin and Thomason, Jesse and Tansey, Wesley},
	urldate = {2020-01-29},
	date = {2019-06-09},
	eprinttype = {arxiv},
	eprint = {1904.00045},
	keywords = {tbr},
	file = {Burns et al_2019_Interpreting Black Box Models via Hypothesis Testing.pdf:/Users/wpq/Dropbox (MIT)/zotero/Burns et al_2019_Interpreting Black Box Models via Hypothesis Testing.pdf:application/pdf}
}

@inproceedings{goyalCounterfactualVisualExplanations2019,
	title = {Counterfactual Visual Explanations},
	url = {http://proceedings.mlr.press/v97/goyal19a.html},
	abstract = {In this work, we develop a technique to produce counterfactual visual explanations. Given a ‘query’ image \$I\$ for which a vision system predicts class \$c\$, a counterfactual visual explanation ident...},
	eventtitle = {International Conference on Machine Learning},
	pages = {2376--2384},
	booktitle = {International Conference on Machine Learning},
	author = {Goyal, Yash and Wu, Ziyan and Ernst, Jan and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
	urldate = {2020-01-29},
	date = {2019-05-24},
	langid = {english},
	keywords = {tbr},
	file = {Goyal et al_2019_Counterfactual Visual Explanations.pdf:/Users/wpq/Dropbox (MIT)/zotero/Goyal et al_2019_Counterfactual Visual Explanations2.pdf:application/pdf}
}

@article{bienPrototypeSelectionInterpretable2011,
	title = {Prototype selection for interpretable classification},
	volume = {5},
	issn = {1932-6157},
	url = {http://arxiv.org/abs/1202.5933},
	doi = {10.1214/11-AOAS495},
	abstract = {Prototype methods seek a minimal subset of samples that can serve as a distillation or condensed view of a data set. As the size of modern data sets grows, being able to present a domain specialist with a short list of "representative" samples chosen from the data set is of increasing interpretative value. While much recent statistical research has been focused on producing sparse-in-the-variables methods, this paper aims at achieving sparsity in the samples. We discuss a method for selecting prototypes in the classification setting (in which the samples fall into known discrete categories). Our method of focus is derived from three basic properties that we believe a good prototype set should satisfy. This intuition is translated into a set cover optimization problem, which we solve approximately using standard approaches. While prototype selection is usually viewed as purely a means toward building an efficient classifier, in this paper we emphasize the inherent value of having a set of prototypical elements. That said, by using the nearest-neighbor rule on the set of prototypes, we can of course discuss our method as a classifier as well.},
	pages = {2403--2424},
	number = {4},
	journaltitle = {The Annals of Applied Statistics},
	shortjournal = {Ann. Appl. Stat.},
	author = {Bien, Jacob and Tibshirani, Robert},
	urldate = {2020-01-30},
	date = {2011-12},
	eprinttype = {arxiv},
	eprint = {1202.5933},
	keywords = {tbr},
	file = {Bien_Tibshirani_2011_Prototype selection for interpretable classification.pdf:/Users/wpq/Dropbox (MIT)/zotero/Bien_Tibshirani_2011_Prototype selection for interpretable classification.pdf:application/pdf}
}

@incollection{kimExamplesAreNot2016,
	title = {Examples are not enough, learn to criticize! Criticism for Interpretability},
	url = {http://papers.nips.cc/paper/6300-examples-are-not-enough-learn-to-criticize-criticism-for-interpretability.pdf},
	pages = {2280--2288},
	booktitle = {Advances in Neural Information Processing Systems 29},
	publisher = {Curran Associates, Inc.},
	author = {Kim, Been and Khanna, Rajiv and Koyejo, Oluwasanmi O},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	urldate = {2020-01-30},
	date = {2016},
	keywords = {tbr},
	file = {Kim et al_2016_Examples are not enough, learn to criticize! Criticism for Interpretability.pdf:/Users/wpq/Dropbox (MIT)/zotero/Kim et al_2016_Examples are not enough, learn to criticize! Criticism for Interpretability2.pdf:application/pdf}
}

@article{singlaExplanationProgressiveExaggeration2019,
	title = {Explanation by Progressive Exaggeration},
	url = {https://arxiv.org/abs/1911.00483v2},
	abstract = {As machine learning methods see greater adoption and implementation in high
stakes applications such as medical image diagnosis, the need for model
interpretability and explanation has become more critical. Classical approaches
that assess feature importance (e.g. saliency maps) do not explain how and why
a particular region of an image is relevant to the prediction. We propose a
method that explains the outcome of a classification black-box by gradually
exaggerating the semantic effect of a given class. Given a query input to a
classifier, our method produces a progressive set of plausible variations of
that query, which gradually changes the posterior probability from its original
class to its negation. These counter-factually generated samples preserve
features unrelated to the classification decision, such that a user can employ
our method as a "tuning knob" to traverse a data manifold while crossing the
decision boundary. Our method is model agnostic and only requires the output
value and gradient of the predictor with respect to its input.},
	author = {Singla, Sumedha and Pollack, Brian and Chen, Junxiang and Batmanghelich, Kayhan},
	urldate = {2020-01-30},
	date = {2019-11-01},
	langid = {english},
	keywords = {read},
	file = {Singla et al_2019_Explanation by Progressive Exaggeration.pdf:/Users/wpq/Dropbox (MIT)/zotero/Singla et al_2019_Explanation by Progressive Exaggeration.pdf:application/pdf}
}

@article{chattopadhyayNeuralNetworkAttributions2019,
	title = {Neural Network Attributions: A Causal Perspective},
	url = {http://arxiv.org/abs/1902.02302},
	shorttitle = {Neural Network Attributions},
	abstract = {We propose a new attribution method for neural networks developed using first principles of causality (to the best of our knowledge, the first such). The neural network architecture is viewed as a Structural Causal Model, and a methodology to compute the causal effect of each feature on the output is presented. With reasonable assumptions on the causal structure of the input data, we propose algorithms to efficiently compute the causal effects, as well as scale the approach to data with large dimensionality. We also show how this method can be used for recurrent neural networks. We report experimental results on both simulated and real datasets showcasing the promise and usefulness of the proposed algorithm.},
	journaltitle = {{arXiv}:1902.02302 [cs, stat]},
	author = {Chattopadhyay, Aditya and Manupriya, Piyushi and Sarkar, Anirban and Balasubramanian, Vineeth N.},
	urldate = {2020-01-30},
	date = {2019-07-03},
	eprinttype = {arxiv},
	eprint = {1902.02302},
	keywords = {tbr},
	file = {Chattopadhyay et al_2019_Neural Network Attributions - A Causal Perspective.pdf:/Users/wpq/Dropbox (MIT)/zotero/Chattopadhyay et al_2019_Neural Network Attributions - A Causal Perspective.pdf:application/pdf}
}

@article{schwabCXPlainCausalExplanations2019,
	title = {{CXPlain}: Causal Explanations for Model Interpretation under Uncertainty},
	url = {http://arxiv.org/abs/1910.12336},
	shorttitle = {{CXPlain}},
	abstract = {Feature importance estimates that inform users about the degree to which given inputs influence the output of a predictive model are crucial for understanding, validating, and interpreting machine-learning models. However, providing fast and accurate estimates of feature importance for high-dimensional data, and quantifying the uncertainty of such estimates remain open challenges. Here, we frame the task of providing explanations for the decisions of machine-learning models as a causal learning task, and train causal explanation ({CXPlain}) models that learn to estimate to what degree certain inputs cause outputs in another machine-learning model. {CXPlain} can, once trained, be used to explain the target model in little time, and enables the quantification of the uncertainty associated with its feature importance estimates via bootstrap ensembling. We present experiments that demonstrate that {CXPlain} is significantly more accurate and faster than existing model-agnostic methods for estimating feature importance. In addition, we confirm that the uncertainty estimates provided by {CXPlain} ensembles are strongly correlated with their ability to accurately estimate feature importance on held-out data.},
	journaltitle = {{arXiv}:1910.12336 [cs, stat]},
	author = {Schwab, Patrick and Karlen, Walter},
	urldate = {2020-01-30},
	date = {2019-10-27},
	eprinttype = {arxiv},
	eprint = {1910.12336},
	keywords = {tbr},
	file = {Schwab_Karlen_2019_CXPlain - Causal Explanations for Model Interpretation under Uncertainty.pdf:/Users/wpq/Dropbox (MIT)/zotero/Schwab_Karlen_2019_CXPlain - Causal Explanations for Model Interpretation under Uncertainty.pdf:application/pdf}
}

@article{laugelDangersPosthocInterpretability2019,
	title = {The Dangers of Post-hoc Interpretability: Unjustified Counterfactual Explanations},
	url = {http://arxiv.org/abs/1907.09294},
	shorttitle = {The Dangers of Post-hoc Interpretability},
	abstract = {Post-hoc interpretability approaches have been proven to be powerful tools to generate explanations for the predictions made by a trained black-box model. However, they create the risk of having explanations that are a result of some artifacts learned by the model instead of actual knowledge from the data. This paper focuses on the case of counterfactual explanations and asks whether the generated instances can be justified, i.e. continuously connected to some ground-truth data. We evaluate the risk of generating unjustified counterfactual examples by investigating the local neighborhoods of instances whose predictions are to be explained and show that this risk is quite high for several datasets. Furthermore, we show that most state of the art approaches do not differentiate justified from unjustified counterfactual examples, leading to less useful explanations.},
	journaltitle = {{arXiv}:1907.09294 [cs, stat]},
	author = {Laugel, Thibault and Lesot, Marie-Jeanne and Marsala, Christophe and Renard, Xavier and Detyniecki, Marcin},
	urldate = {2020-01-30},
	date = {2019-07-22},
	eprinttype = {arxiv},
	eprint = {1907.09294},
	keywords = {tbr},
	file = {Laugel et al_2019_The Dangers of Post-hoc Interpretability - Unjustified Counterfactual Explanations.pdf:/Users/wpq/Dropbox (MIT)/zotero/Laugel et al_2019_The Dangers of Post-hoc Interpretability - Unjustified Counterfactual Explanations.pdf:application/pdf}
}

@article{dombrowskiExplanationsCanBe2019,
	title = {Explanations can be manipulated and geometry is to blame},
	url = {http://arxiv.org/abs/1906.07983},
	abstract = {Explanation methods aim to make neural networks more trustworthy and interpretable. In this paper, we demonstrate a property of explanation methods which is disconcerting for both of these purposes. Namely, we show that explanations can be manipulated arbitrarily by applying visually hardly perceptible perturbations to the input that keep the network's output approximately constant. We establish theoretically that this phenomenon can be related to certain geometrical properties of neural networks. This allows us to derive an upper bound on the susceptibility of explanations to manipulations. Based on this result, we propose effective mechanisms to enhance the robustness of explanations.},
	journaltitle = {{arXiv}:1906.07983 [cs, stat]},
	author = {Dombrowski, Ann-Kathrin and Alber, Maximilian and Anders, Christopher J. and Ackermann, Marcel and Müller, Klaus-Robert and Kessel, Pan},
	urldate = {2020-01-30},
	date = {2019-09-25},
	eprinttype = {arxiv},
	eprint = {1906.07983},
	keywords = {tbr},
	file = {Dombrowski et al_2019_Explanations can be manipulated and geometry is to blame.pdf:/Users/wpq/Dropbox (MIT)/zotero/Dombrowski et al_2019_Explanations can be manipulated and geometry is to blame.pdf:application/pdf}
}

@article{leeFunctionalTransparencyStructured2019,
	title = {Functional Transparency for Structured Data: a Game-Theoretic Approach},
	url = {http://arxiv.org/abs/1902.09737},
	shorttitle = {Functional Transparency for Structured Data},
	abstract = {We provide a new approach to training neural models to exhibit transparency in a well-defined, functional manner. Our approach naturally operates over structured data and tailors the predictor, functionally, towards a chosen family of (local) witnesses. The estimation problem is setup as a co-operative game between an unrestricted predictor such as a neural network, and a set of witnesses chosen from the desired transparent family. The goal of the witnesses is to highlight, locally, how well the predictor conforms to the chosen family of functions, while the predictor is trained to minimize the highlighted discrepancy. We emphasize that the predictor remains globally powerful as it is only encouraged to agree locally with locally adapted witnesses. We analyze the effect of the proposed approach, provide example formulations in the context of deep graph and sequence models, and empirically illustrate the idea in chemical property prediction, temporal modeling, and molecule representation learning.},
	journaltitle = {{arXiv}:1902.09737 [cs, stat]},
	author = {Lee, Guang-He and Jin, Wengong and Alvarez-Melis, David and Jaakkola, Tommi S.},
	urldate = {2020-01-30},
	date = {2019-02-26},
	eprinttype = {arxiv},
	eprint = {1902.09737},
	keywords = {tbr},
	file = {Lee et al_2019_Functional Transparency for Structured Data - a Game-Theoretic Approach.pdf:/Users/wpq/Dropbox (MIT)/zotero/Lee et al_2019_Functional Transparency for Structured Data - a Game-Theoretic Approach.pdf:application/pdf}
}

@article{etmannConnectionAdversarialRobustness2019,
	title = {On the Connection Between Adversarial Robustness and Saliency Map Interpretability},
	url = {http://arxiv.org/abs/1905.04172},
	abstract = {Recent studies on the adversarial vulnerability of neural networks have shown that models trained to be more robust to adversarial attacks exhibit more interpretable saliency maps than their non-robust counterparts. We aim to quantify this behavior by considering the alignment between input image and saliency map. We hypothesize that as the distance to the decision boundary grows,so does the alignment. This connection is strictly true in the case of linear models. We confirm these theoretical findings with experiments based on models trained with a local Lipschitz regularization and identify where the non-linear nature of neural networks weakens the relation.},
	journaltitle = {{arXiv}:1905.04172 [cs, stat]},
	author = {Etmann, Christian and Lunz, Sebastian and Maass, Peter and Schönlieb, Carola-Bibiane},
	urldate = {2020-01-30},
	date = {2019-05-10},
	eprinttype = {arxiv},
	eprint = {1905.04172},
	keywords = {tbr},
	file = {Etmann et al_2019_On the Connection Between Adversarial Robustness and Saliency Map Interpretability.pdf:/Users/wpq/Dropbox (MIT)/zotero/Etmann et al_2019_On the Connection Between Adversarial Robustness and Saliency Map Interpretability.pdf:application/pdf}
}

@article{singlaUnderstandingImpactsHighOrder2019,
	title = {Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation},
	url = {http://arxiv.org/abs/1902.00407},
	abstract = {Current methods to interpret deep learning models by generating saliency maps generally rely on two key assumptions. First, they use first-order approximations of the loss function neglecting higher-order terms such as the loss curvatures. Second, they evaluate each feature's importance in isolation, ignoring their inter-dependencies. In this work, we study the effect of relaxing these two assumptions. First, by characterizing a closed-form formula for the Hessian matrix of a deep {ReLU} network, we prove that, for a classification problem with a large number of classes, if an input has a high confidence classification score, the inclusion of the Hessian term has small impacts in the final solution. We prove this result by showing that in this case the Hessian matrix is approximately of rank one and its leading eigenvector is almost parallel to the gradient of the loss function. Our empirical experiments on {ImageNet} samples are consistent with our theory. This result can have implications in other related problems such as adversarial examples as well. Second, we compute the importance of group-features in deep learning interpretation by introducing a sparsity regularization term. We use the \$L\_0-L\_1\$ relaxation technique along with the proximal gradient descent to have an efficient computation of group feature importance scores. Our empirical results indicate that considering group features can improve deep learning interpretation significantly.},
	journaltitle = {{arXiv}:1902.00407 [cs, stat]},
	author = {Singla, Sahil and Wallace, Eric and Feng, Shi and Feizi, Soheil},
	urldate = {2020-01-30},
	date = {2019-05-30},
	eprinttype = {arxiv},
	eprint = {1902.00407},
	keywords = {tbr},
	file = {Singla et al_2019_Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation.pdf:/Users/wpq/Dropbox (MIT)/zotero/Singla et al_2019_Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation.pdf:application/pdf}
}

@inproceedings{guanDeepUnifiedUnderstanding2019,
	title = {Towards a Deep and Unified Understanding of Deep Neural Models in {NLP}},
	url = {http://proceedings.mlr.press/v97/guan19a.html},
	abstract = {We define a unified information-based measure to provide quantitative explanations on how intermediate layers of deep Natural Language Processing ({NLP}) models leverage information of input words. O...},
	eventtitle = {International Conference on Machine Learning},
	pages = {2454--2463},
	booktitle = {International Conference on Machine Learning},
	author = {Guan, Chaoyu and Wang, Xiting and Zhang, Quanshi and Chen, Runjin and He, Di and Xie, Xing},
	urldate = {2020-01-30},
	date = {2019-05-24},
	langid = {english},
	keywords = {tbr},
	file = {Guan et al_2019_Towards a Deep and Unified Understanding of Deep Neural Models in NLP.pdf:/Users/wpq/Dropbox (MIT)/zotero/Guan et al_2019_Towards a Deep and Unified Understanding of Deep Neural Models in NLP.pdf:application/pdf}
}

@inproceedings{odenaTensorFuzzDebuggingNeural2019,
	title = {{TensorFuzz}: Debugging Neural Networks with Coverage-Guided Fuzzing},
	url = {http://proceedings.mlr.press/v97/odena19a.html},
	shorttitle = {{TensorFuzz}},
	abstract = {Neural networks are difficult to interpret and debug. We introduce testing techniques for neural networks that can discover errors occurring only for rare inputs. Specifically, we develop coverage-...},
	eventtitle = {International Conference on Machine Learning},
	pages = {4901--4911},
	booktitle = {International Conference on Machine Learning},
	author = {Odena, Augustus and Olsson, Catherine and Andersen, David and Goodfellow, Ian},
	urldate = {2020-01-30},
	date = {2019-05-24},
	langid = {english},
	keywords = {tbr},
	file = {Odena et al_2019_TensorFuzz - Debugging Neural Networks with Coverage-Guided Fuzzing.pdf:/Users/wpq/Dropbox (MIT)/zotero/Odena et al_2019_TensorFuzz - Debugging Neural Networks with Coverage-Guided Fuzzing.pdf:application/pdf}
}

@inproceedings{liDeepLearningCaseBased2018,
	title = {Deep Learning for Case-Based Reasoning Through Prototypes: A Neural Network That Explains Its Predictions},
	rights = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence ({AAAI}), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify {AAAI}, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense {AAAI} may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to {AAAI} in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that {AAAI} copyright and the source are indicated, and that the copies are not used in a way that implies {AAAI} endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the {AAAI} electronic server, and shall not post other {AAAI} copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without {AAAI}’s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, {AAAI} grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by {AAAI}, or is withdrawn by the author(s) before acceptance by {AAAI}, this agreement becomes null and void.},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17082},
	shorttitle = {Deep Learning for Case-Based Reasoning Through Prototypes},
	abstract = {Deep neural networks are widely used for classification. These deep models often suffer from a lack of interpretability---they are particularly difficult to understand because of their non-linear nature. As a result, neural networks are often treated as "black box" models, and in the past, have been trained purely to optimize the accuracy of predictions. In this work, we create a novel network architecture for deep learning that naturally explains its own reasoning for each prediction. This architecture contains an autoencoder and a special prototype layer, where each unit of that layer stores a weight vector that resembles an encoded training input. The encoder of the autoencoder allows us to do comparisons within the latent space, while the decoder allows us to visualize the learned prototypes. The training objective has four terms: an accuracy term, a term that encourages every prototype to be similar to at least one encoded input, a term that encourages every encoded input to be close to at least one prototype, and a term that encourages faithful reconstruction by the autoencoder. The distances computed in the prototype layer are used as part of the classification process. Since the prototypes are learned during training, the learned network naturally comes with explanations for each prediction, and the explanations are loyal to what the network actually computes.},
	eventtitle = {Thirty-Second {AAAI} Conference on Artificial Intelligence},
	booktitle = {Thirty-Second {AAAI} Conference on Artificial Intelligence},
	author = {Li, Oscar and Liu, Hao and Chen, Chaofan and Rudin, Cynthia},
	urldate = {2020-01-30},
	date = {2018-04-29},
	langid = {english},
	keywords = {read},
	file = {Li et al_2018_Deep Learning for Case-Based Reasoning Through Prototypes - A Neural Network That Explains Its Predictions.pdf:/Users/wpq/Dropbox (MIT)/zotero/Li et al_2018_Deep Learning for Case-Based Reasoning Through Prototypes - A Neural Network That Explains Its Predictions.pdf:application/pdf}
}

@inproceedings{kimInterpretabilityFeatureAttribution2018,
	title = {Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors ({TCAV})},
	url = {http://proceedings.mlr.press/v80/kim18d.html},
	shorttitle = {Interpretability Beyond Feature Attribution},
	abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level ...},
	eventtitle = {International Conference on Machine Learning},
	pages = {2668--2677},
	booktitle = {International Conference on Machine Learning},
	author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
	urldate = {2020-01-31},
	date = {2018-07-03},
	langid = {english},
	keywords = {read},
	file = {Kim et al_2018_Interpretability Beyond Feature Attribution - Quantitative Testing with Concept Activation Vectors (TCAV).pdf:/Users/wpq/Dropbox (MIT)/zotero/Kim et al_2018_Interpretability Beyond Feature Attribution - Quantitative Testing with Concept Activation Vectors (TCAV)2.pdf:application/pdf}
}

@article{alainUnderstandingIntermediateLayers2018,
	title = {Understanding intermediate layers using linear classifier probes},
	url = {http://arxiv.org/abs/1610.01644},
	abstract = {Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as "probes", trained entirely independently of the model itself. This helps us better understand the roles and dynamics of the intermediate layers. We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems. We apply this technique to the popular models Inception v3 and Resnet-50. Among other things, we observe experimentally that the linear separability of features increase monotonically along the depth of the model.},
	journaltitle = {{arXiv}:1610.01644 [cs, stat]},
	author = {Alain, Guillaume and Bengio, Yoshua},
	urldate = {2020-01-31},
	date = {2018-11-22},
	eprinttype = {arxiv},
	eprint = {1610.01644},
	keywords = {tbr},
	file = {Alain_Bengio_2018_Understanding intermediate layers using linear classifier probes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Alain_Bengio_2018_Understanding intermediate layers using linear classifier probes.pdf:application/pdf}
}

@inproceedings{alvarez-melisCausalFrameworkExplaining2017,
	location = {Copenhagen, Denmark},
	title = {A causal framework for explaining the predictions of black-box sequence-to-sequence models},
	url = {https://www.aclweb.org/anthology/D17-1042},
	doi = {10.18653/v1/D17-1042},
	abstract = {We interpret the predictions of any black-box structured input-structured output model around a specific input-output pair. Our method returns an “explanation” consisting of groups of input-output tokens that are causally related. These dependencies are inferred by querying the model with perturbed inputs, generating a graph over tokens from the responses, and solving a partitioning problem to select the most relevant components. We focus the general approach on sequence-to-sequence problems, adopting a variational autoencoder to yield meaningful input perturbations. We test our method across several {NLP} sequence generation tasks.},
	eventtitle = {{EMNLP} 2017},
	pages = {412--421},
	booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Alvarez-Melis, David and Jaakkola, Tommi},
	urldate = {2020-02-01},
	date = {2017-09},
	keywords = {read},
	file = {Alvarez-Melis_Jaakkola_2017_A causal framework for explaining the predictions of black-box sequence-to-sequence models.pdf:/Users/wpq/Dropbox (MIT)/zotero/Alvarez-Melis_Jaakkola_2017_A causal framework for explaining the predictions of black-box sequence-to-sequence models.pdf:application/pdf}
}

@article{kimLearningInterpretableModels2019,
	title = {Learning Interpretable Models with Causal Guarantees},
	url = {http://arxiv.org/abs/1901.08576},
	abstract = {Machine learning has shown much promise in helping improve the quality of medical, legal, and economic decision-making. In these applications, machine learning models must satisfy two important criteria: (i) they must be causal, since the goal is typically to predict individual treatment effects, and (ii) they must be interpretable, so that human decision makers can validate and trust the model predictions. There has recently been much progress along each direction independently, yet the state-of-the-art approaches are fundamentally incompatible. We propose a framework for learning causal interpretable models---from observational data---that can be used to predict individual treatment effects. Our framework can be used with any algorithm for learning interpretable models. Furthermore, we prove an error bound on the treatment effects predicted by our model. Finally, in an experiment on real-world data, we show that the models trained using our framework significantly outperform a number of baselines.},
	journaltitle = {{arXiv}:1901.08576 [cs, stat]},
	author = {Kim, Carolyn and Bastani, Osbert},
	urldate = {2020-02-01},
	date = {2019-01-24},
	eprinttype = {arxiv},
	eprint = {1901.08576},
	keywords = {tbr},
	file = {Kim_Bastani_2019_Learning Interpretable Models with Causal Guarantees.pdf:/Users/wpq/Dropbox (MIT)/zotero/Kim_Bastani_2019_Learning Interpretable Models with Causal Guarantees.pdf:application/pdf}
}

@article{murdochWordImportanceContextual2018,
	title = {Beyond Word Importance: Contextual Decomposition to Extract Interactions from {LSTMs}},
	url = {http://arxiv.org/abs/1801.05453},
	shorttitle = {Beyond Word Importance},
	abstract = {The driving force behind the recent success of {LSTMs} has been their ability to learn complex and non-linear relationships. Consequently, our inability to describe these relationships has led to {LSTMs} being characterized as black boxes. To this end, we introduce contextual decomposition ({CD}), an interpretation algorithm for analysing individual predictions made by standard {LSTMs}, without any changes to the underlying model. By decomposing the output of a {LSTM}, {CD} captures the contributions of combinations of words or variables to the final prediction of an {LSTM}. On the task of sentiment analysis with the Yelp and {SST} data sets, we show that {CD} is able to reliably identify words and phrases of contrasting sentiment, and how they are combined to yield the {LSTM}'s final prediction. Using the phrase-level labels in {SST}, we also demonstrate that {CD} is able to successfully extract positive and negative negations from an {LSTM}, something which has not previously been done.},
	journaltitle = {{arXiv}:1801.05453 [cs, stat]},
	author = {Murdoch, W. James and Liu, Peter J. and Yu, Bin},
	urldate = {2020-02-02},
	date = {2018-04-27},
	eprinttype = {arxiv},
	eprint = {1801.05453},
	keywords = {tbr},
	file = {Murdoch et al_2018_Beyond Word Importance - Contextual Decomposition to Extract Interactions from LSTMs.pdf:/Users/wpq/Dropbox (MIT)/zotero/Murdoch et al_2018_Beyond Word Importance - Contextual Decomposition to Extract Interactions from LSTMs.pdf:application/pdf}
}

@article{guptaSimpleSaliencyMethod2019,
	title = {A Simple Saliency Method That Passes the Sanity Checks},
	url = {http://arxiv.org/abs/1905.12152},
	abstract = {There is great interest in "saliency methods" (also called "attribution methods"), which give "explanations" for a deep net's decision, by assigning a "score" to each feature/pixel in the input. Their design usually involves credit-assignment via the gradient of the output with respect to input. Recently Adebayo et al. [{arXiv}:1810.03292] questioned the validity of many of these methods since they do not pass simple *sanity checks* which test whether the scores shift/vanish when layers of the trained net are randomized, or when the net is retrained using random labels for inputs. We propose a simple fix to existing saliency methods that helps them pass sanity checks, which we call "competition for pixels". This involves computing saliency maps for all possible labels in the classification task, and using a simple competition among them to identify and remove less relevant pixels from the map. The simplest variant of this is "Competitive Gradient \${\textbackslash}odot\$ Input ({CGI})": it is efficient, requires no additional training, and uses only the input and gradient. Some theoretical justification is provided for it (especially for {ReLU} networks) and its performance is empirically demonstrated.},
	journaltitle = {{arXiv}:1905.12152 [cs, stat]},
	author = {Gupta, Arushi and Arora, Sanjeev},
	urldate = {2020-02-06},
	date = {2019-06-06},
	eprinttype = {arxiv},
	eprint = {1905.12152},
	keywords = {tbr},
	file = {Gupta_Arora_2019_A Simple Saliency Method That Passes the Sanity Checks.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gupta_Arora_2019_A Simple Saliency Method That Passes the Sanity Checks.pdf:application/pdf}
}

@article{upchurchDeepFeatureInterpolation2017,
	title = {Deep Feature Interpolation for Image Content Changes},
	url = {http://arxiv.org/abs/1611.05507},
	abstract = {We propose Deep Feature Interpolation ({DFI}), a new data-driven baseline for automatic high-resolution image transformation. As the name suggests, it relies only on simple linear interpolation of deep convolutional features from pre-trained convnets. We show that despite its simplicity, {DFI} can perform high-level semantic transformations like "make older/younger", "make bespectacled", "add smile", among others, surprisingly well - sometimes even matching or outperforming the state-of-the-art. This is particularly unexpected as {DFI} requires no specialized network architecture or even any deep network to be trained for these tasks. {DFI} therefore can be used as a new baseline to evaluate more complex algorithms and provides a practical answer to the question of which image transformation tasks are still challenging in the rise of deep learning.},
	journaltitle = {{arXiv}:1611.05507 [cs]},
	author = {Upchurch, Paul and Gardner, Jacob and Pleiss, Geoff and Pless, Robert and Snavely, Noah and Bala, Kavita and Weinberger, Kilian},
	urldate = {2020-02-21},
	date = {2017-06-19},
	eprinttype = {arxiv},
	eprint = {1611.05507},
	keywords = {tbr},
	file = {Upchurch et al_2017_Deep Feature Interpolation for Image Content Changes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Upchurch et al_2017_Deep Feature Interpolation for Image Content Changes.pdf:application/pdf}
}

@article{frosstDistillingNeuralNetwork2017,
	title = {Distilling a Neural Network Into a Soft Decision Tree},
	url = {http://arxiv.org/abs/1711.09784},
	abstract = {Deep neural networks have proved to be a very effective way to perform classification tasks. They excel when the input data is high dimensional, the relationship between the input and the output is complicated, and the number of labeled training examples is large. But it is hard to explain why a learned network makes a particular classification decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead, explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data.},
	journaltitle = {{arXiv}:1711.09784 [cs, stat]},
	author = {Frosst, Nicholas and Hinton, Geoffrey},
	urldate = {2020-03-19},
	date = {2017-11-27},
	eprinttype = {arxiv},
	eprint = {1711.09784},
	keywords = {tbr},
	file = {Frosst_Hinton_2017_Distilling a Neural Network Into a Soft Decision Tree.pdf:/Users/wpq/Dropbox (MIT)/zotero/Frosst_Hinton_2017_Distilling a Neural Network Into a Soft Decision Tree.pdf:application/pdf}
}

@inproceedings{pearlTheoreticalImpedimentsMachine2018,
	title = {Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution},
	isbn = {978-1-4503-5581-0},
	doi = {10.1145/3159652.3176182},
	abstract = {Current machine learning systems operate, almost exclusively, in a statistical, or model-blind mode, which entails severe theoretical limits on their power and performance. Such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong {AI}. To achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference. To demonstrate the essential role of such models, I will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal inference.},
	eventtitle = {{WSDM} '18: Proceedings of the Eleventh {ACM} International Conference on Web Search and Data Mining},
	pages = {3--3},
	author = {Pearl, Judea},
	date = {2018-02-02},
	keywords = {good, tbr},
	file = {Pearl_2018_Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution.pdf:/Users/wpq/Dropbox (MIT)/zotero/Pearl_2018_Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution.pdf:application/pdf}
}

@article{moraffahCausalInterpretabilityMachine2020,
	title = {Causal Interpretability for Machine Learning -- Problems, Methods and Evaluation},
	url = {http://arxiv.org/abs/2003.03934},
	abstract = {Machine learning models have had discernible achievements in a myriad of applications. However, most of these models are black-boxes, and it is obscure how the decisions are made by them. This makes the models unreliable and untrustworthy. To provide insights into the decision making processes of these models, a variety of traditional interpretable models have been proposed. Moreover, to generate more human-friendly explanations, recent work on interpretability tries to answer questions related to causality such as {\textbackslash}Why does this model makes such decisions?" or {\textbackslash}Was it a specific feature that caused the decision made by the model?". In this work, models that aim to answer causal questions are referred to as causal interpretable models. The existing surveys have covered concepts and methodologies of traditional interpretability. In this work, we present a comprehensive survey on causal interpretable models from the aspects of the problems and methods. In addition, this survey provides in-depth insights into the existing evaluation metrics for measuring interpretability, which can help practitioners understand for what scenarios each evaluation metric is suitable.},
	journaltitle = {{arXiv}:2003.03934 [cs, stat]},
	author = {Moraffah, Raha and Karami, Mansooreh and Guo, Ruocheng and Raglin, Adrienne and Liu, Huan},
	urldate = {2020-03-21},
	date = {2020-03-11},
	eprinttype = {arxiv},
	eprint = {2003.03934},
	keywords = {skim},
	file = {Moraffah et al_2020_Causal Interpretability for Machine Learning -- Problems, Methods and Evaluation.pdf:/Users/wpq/Dropbox (MIT)/zotero/Moraffah et al_2020_Causal Interpretability for Machine Learning -- Problems, Methods and Evaluation.pdf:application/pdf}
}

@article{parafitaExplainingVisualModels2019,
	title = {Explaining Visual Models by Causal Attribution},
	url = {http://arxiv.org/abs/1909.08891},
	abstract = {Model explanations based on pure observational data cannot compute the effects of features reliably, due to their inability to estimate how each factor alteration could affect the rest. We argue that explanations should be based on the causal model of the data and the derived intervened causal models, that represent the data distribution subject to interventions. With these models, we can compute counterfactuals, new samples that will inform us how the model reacts to feature changes on our input. We propose a novel explanation methodology based on Causal Counterfactuals and identify the limitations of current Image Generative Models in their application to counterfactual creation.},
	journaltitle = {{arXiv}:1909.08891 [cs, stat]},
	author = {Parafita, Álvaro and Vitrià, Jordi},
	urldate = {2020-03-21},
	date = {2019-09-19},
	eprinttype = {arxiv},
	eprint = {1909.08891},
	keywords = {read},
	file = {Parafita_Vitria_2019_Explaining Visual Models by Causal Attribution.pdf:/Users/wpq/Dropbox (MIT)/zotero/Parafita_Vitria_2019_Explaining Visual Models by Causal Attribution.pdf:application/pdf}
}

@article{goyalExplainingClassifiersCausal2020,
	title = {Explaining Classifiers with Causal Concept Effect ({CaCE})},
	url = {http://arxiv.org/abs/1907.07165},
	abstract = {How can we understand classification decisions made by deep neural networks? Many existing explainability methods rely solely on correlations and fail to account for confounding, which may result in potentially misleading explanations. To overcome this problem, we define the Causal Concept Effect ({CaCE}) as the causal effect of (the presence or absence of) a human-interpretable concept on a deep neural net's predictions. We show that the {CaCE} measure can avoid errors stemming from confounding. Estimating {CaCE} is difficult in situations where we cannot easily simulate the do-operator. To mitigate this problem, we use a generative model, specifically a Variational {AutoEncoder} ({VAE}), to measure {VAE}-{CaCE}. In an extensive experimental analysis, we show that the {VAE}-{CaCE} is able to estimate the true concept causal effect, compared to baselines for a number of datasets including high dimensional images.},
	journaltitle = {{arXiv}:1907.07165 [cs, stat]},
	author = {Goyal, Yash and Feder, Amir and Shalit, Uri and Kim, Been},
	urldate = {2020-03-21},
	date = {2020-02-28},
	eprinttype = {arxiv},
	eprint = {1907.07165},
	keywords = {tbr},
	file = {Goyal et al_2020_Explaining Classifiers with Causal Concept Effect (CaCE).pdf:/Users/wpq/Dropbox (MIT)/zotero/Goyal et al_2020_Explaining Classifiers with Causal Concept Effect (CaCE).pdf:application/pdf}
}

@article{liptonMythosModelInterpretability2017,
	title = {The Mythos of Model Interpretability},
	url = {http://arxiv.org/abs/1606.03490},
	abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
	journaltitle = {{arXiv}:1606.03490 [cs, stat]},
	author = {Lipton, Zachary C.},
	urldate = {2020-04-12},
	date = {2017-03-06},
	eprinttype = {arxiv},
	eprint = {1606.03490},
	keywords = {good, read},
	file = {Lipton_2017_The Mythos of Model Interpretability.pdf:/Users/wpq/Dropbox (MIT)/zotero/Lipton_2017_The Mythos of Model Interpretability.pdf:application/pdf}
}

@article{luRobustSaliencyMaps2020,
	title = {Robust saliency maps with decoy-enhanced saliency score},
	url = {http://arxiv.org/abs/2002.00526},
	abstract = {Saliency methods help to make deep neural network predictions more interpretable by identifying particular features, such as pixels in an image, that contribute most strongly to the network's prediction. Unfortunately, recent evidence suggests that many saliency methods perform poorly when gradients are saturated or in the presence of strong inter-feature dependence or noise injected by an adversarial attack. In this work, we propose to infer robust saliency scores by integrating the saliency scores of a set of decoys with a novel decoy-enhanced saliency score, in which the decoys are generated by either solving an optimization problem or blurring the original input. We theoretically analyze that our method compensates for gradient saturation and considers joint activation patterns of pixels. We also apply our method to three different {CNNs}---{VGGNet}, {AlexNet}, and {ResNet} trained on {ImageNet} data set. The empirical results show both qualitatively and quantitatively that our method outperforms raw scores produced by three existing saliency methods, even in the presence of adversarial attacks.},
	journaltitle = {{arXiv}:2002.00526 [cs, stat]},
	author = {Lu, Yang and Guo, Wenbo and Xing, Xinyu and Noble, William Stafford},
	urldate = {2020-04-16},
	date = {2020-02-02},
	eprinttype = {arxiv},
	eprint = {2002.00526},
	keywords = {read},
	file = {Lu et al_2020_Robust saliency maps with decoy-enhanced saliency score.pdf:/Users/wpq/Dropbox (MIT)/zotero/Lu et al_2020_Robust saliency maps with decoy-enhanced saliency score.pdf:application/pdf}
}

@article{schulzRestrictingFlowInformation2020,
	title = {Restricting the Flow: Information Bottlenecks for Attribution},
	url = {http://arxiv.org/abs/2001.00396},
	shorttitle = {Restricting the Flow},
	abstract = {Attribution methods provide insights into the decision-making of machine learning models like artificial neural networks. For a given input sample, they assign a relevance score to each individual input variable, such as the pixels of an image. In this work we adapt the information bottleneck concept for attribution. By adding noise to intermediate feature maps we restrict the flow of information and can quantify (in bits) how much information image regions provide. We compare our method against ten baselines using three different metrics on {VGG}-16 and {ResNet}-50, and find that our methods outperform all baselines in five out of six settings. The method's information-theoretic foundation provides an absolute frame of reference for attribution values (bits) and a guarantee that regions scored close to zero are not necessary for the network's decision. For reviews: https://openreview.net/forum?id=S1xWh1rYwB For code: https://github.com/{BioroboticsLab}/{IBA}},
	journaltitle = {{arXiv}:2001.00396 [cs, stat]},
	author = {Schulz, Karl and Sixt, Leon and Tombari, Federico and Landgraf, Tim},
	urldate = {2020-04-22},
	date = {2020-02-15},
	eprinttype = {arxiv},
	eprint = {2001.00396},
	keywords = {read},
	file = {Schulz et al_2020_Restricting the Flow - Information Bottlenecks for Attribution.pdf:/Users/wpq/Dropbox (MIT)/zotero/Schulz et al_2020_Restricting the Flow - Information Bottlenecks for Attribution.pdf:application/pdf}
}