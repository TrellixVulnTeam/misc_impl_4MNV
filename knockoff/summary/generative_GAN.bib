
@article{choiStarGANUnifiedGenerative2018,
	title = {{StarGAN}: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation},
	url = {http://arxiv.org/abs/1711.09020},
	shorttitle = {{StarGAN}},
	abstract = {Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose {StarGAN}, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of {StarGAN} allows simultaneous training of multiple datasets with different domains within a single network. This leads to {StarGAN}'s superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.},
	journaltitle = {{arXiv}:1711.09020 [cs]},
	author = {Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul},
	urldate = {2020-01-17},
	date = {2018-09-21},
	eprinttype = {arxiv},
	eprint = {1711.09020},
	file = {Choi et al_2018_StarGAN - Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation.pdf:/Users/wpq/Dropbox (MIT)/zotero/Choi et al_2018_StarGAN - Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation.pdf:application/pdf}
}

@inproceedings{nguyenPlugPlayGenerative2017,
	location = {Honolulu, {HI}},
	title = {Plug \& Play Generative Networks: Conditional Iterative Generation of Images in Latent Space},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099857/},
	doi = {10.1109/CVPR.2017.374},
	shorttitle = {Plug \& Play Generative Networks},
	abstract = {Generating high-resolution, photo-realistic images has been a long-standing goal in machine learning. Recently, Nguyen et al. [37] showed one interesting way to synthesize novel images by performing gradient ascent in the latent space of a generator network to maximize the activations of one or multiple neurons in a separate classiﬁer network. In this paper we extend this method by introducing an additional prior on the latent code, improving both sample quality and sample diversity, leading to a state-of-the-art generative model that produces high quality images at higher resolutions (227 × 227) than previous generative models, and does so for all 1000 {ImageNet} categories. In addition, we provide a uniﬁed probabilistic interpretation of related activation maximization methods and call the general class of models “Plug and Play Generative Networks.” {PPGNs} are composed of 1) a generator network G that is capable of drawing a wide range of image types and 2) a replaceable “condition” network C that tells the generator what to draw. We demonstrate the generation of images conditioned on a class (when C is an {ImageNet} or {MIT} Places classiﬁcation network) and also conditioned on a caption (when C is an image captioning network). Our method also improves the state of the art of Multifaceted Feature Visualization [40], which generates the set of synthetic inputs that activate a neuron in order to better understand how deep neural networks operate. Finally, we show that our model performs reasonably well at the task of image inpainting. While image models are used in this paper, the approach is modality-agnostic and can be applied to many types of data.},
	eventtitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {3510--3520},
	booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Nguyen, Anh and Clune, Jeff and Bengio, Yoshua and Dosovitskiy, Alexey and Yosinski, Jason},
	urldate = {2020-01-16},
	date = {2017-07},
	langid = {english},
	file = {Nguyen et al_2017_Plug & Play Generative Networks - Conditional Iterative Generation of Images in Latent Space.pdf:/Users/wpq/Dropbox (MIT)/zotero/Nguyen et al_2017_Plug & Play Generative Networks - Conditional Iterative Generation of Images in Latent Space.pdf:application/pdf}
}

@article{mirzaConditionalGenerativeAdversarial2014,
	title = {Conditional Generative Adversarial Nets},
	url = {http://arxiv.org/abs/1411.1784},
	abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate {MNIST} digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
	journaltitle = {{arXiv}:1411.1784 [cs, stat]},
	author = {Mirza, Mehdi and Osindero, Simon},
	urldate = {2020-01-11},
	date = {2014-11-06},
	eprinttype = {arxiv},
	eprint = {1411.1784},
	file = {Mirza_Osindero_2014_Conditional Generative Adversarial Nets.pdf:/Users/wpq/Dropbox (MIT)/zotero/Mirza_Osindero_2014_Conditional Generative Adversarial Nets.pdf:application/pdf}
}

@article{ledigPhotoRealisticSingleImage2017,
	title = {Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network},
	url = {http://arxiv.org/abs/1609.04802},
	abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present {SRGAN}, a generative adversarial network ({GAN}) for image super-resolution ({SR}). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score ({MOS}) test shows hugely significant gains in perceptual quality using {SRGAN}. The {MOS} scores obtained with {SRGAN} are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
	journaltitle = {{arXiv}:1609.04802 [cs, stat]},
	author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
	urldate = {2020-01-11},
	date = {2017-05-25},
	eprinttype = {arxiv},
	eprint = {1609.04802},
	file = {Ledig et al_2017_Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.pdf:/Users/wpq/Dropbox (MIT)/zotero/Ledig et al_2017_Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.pdf:application/pdf}
}

@article{sonderbyAmortisedMAPInference2017,
	title = {Amortised {MAP} Inference for Image Super-resolution},
	url = {http://arxiv.org/abs/1610.04490},
	abstract = {Image super-resolution ({SR}) is an underdetermined inverse problem, where a large number of plausible high-resolution images can explain the same downsampled image. Most current single image {SR} methods use empirical risk minimisation, often with a pixel-wise mean squared error ({MSE}) loss. However, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori ({MAP}) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct {MAP} estimation for {SR} is non-trivial, as it requires us to build a model for the image prior from samples. Furthermore, {MAP} inference is often performed via optimisation-based iterative algorithms which don't compare well with the efficiency of neural-network-based alternatives. Here we introduce new methods for amortised {MAP} inference whereby we calculate the {MAP} estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid {SR} solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised {MAP} inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks ({GAN}) (2) denoiser-guided {SR} which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the {GAN} based approach performs best on real image data. Lastly, we establish a connection between {GANs} and amortised variational inference as in e.g. variational autoencoders.},
	journaltitle = {{arXiv}:1610.04490 [cs, stat]},
	author = {Sønderby, Casper Kaae and Caballero, Jose and Theis, Lucas and Shi, Wenzhe and Huszár, Ferenc},
	urldate = {2020-01-10},
	date = {2017-02-21},
	eprinttype = {arxiv},
	eprint = {1610.04490},
	file = {Sonderby et al_2017_Amortised MAP Inference for Image Super-resolution.pdf:/Users/wpq/Dropbox (MIT)/zotero/Sonderby et al_2017_Amortised MAP Inference for Image Super-resolution.pdf:application/pdf}
}

@article{goodfellowNIPS2016Tutorial2017,
	title = {{NIPS} 2016 Tutorial: Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1701.00160},
	shorttitle = {{NIPS} 2016 Tutorial},
	abstract = {This report summarizes the tutorial presented by the author at {NIPS} 2016 on generative adversarial networks ({GANs}). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how {GANs} compare to other generative models, (3) the details of how {GANs} work, (4) research frontiers in {GANs}, and (5) state-of-the-art image models that combine {GANs} with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
	journaltitle = {{arXiv}:1701.00160 [cs]},
	author = {Goodfellow, Ian},
	urldate = {2020-01-06},
	date = {2017-04-03},
	eprinttype = {arxiv},
	eprint = {1701.00160},
	file = {Goodfellow_2017_NIPS 2016 Tutorial - Generative Adversarial Networks.pdf:/Users/wpq/Dropbox (MIT)/zotero/Goodfellow_2017_NIPS 2016 Tutorial - Generative Adversarial Networks.pdf:application/pdf}
}

@article{zhuUnpairedImagetoImageTranslation2018,
	title = {Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
	url = {http://arxiv.org/abs/1703.10593},
	abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X {\textbackslash}rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y {\textbackslash}rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) {\textbackslash}approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
	journaltitle = {{arXiv}:1703.10593 [cs]},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	urldate = {2019-12-12},
	date = {2018-11-15},
	eprinttype = {arxiv},
	eprint = {1703.10593},
	file = {Zhu et al_2018_Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.pdf:/Users/wpq/Dropbox (MIT)/zotero/Zhu et al_2018_Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.pdf:application/pdf}
}

@article{donahueAdversarialFeatureLearning2017,
	title = {Adversarial Feature Learning},
	url = {http://arxiv.org/abs/1605.09782},
	abstract = {The ability of the Generative Adversarial Networks ({GANs}) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, {GANs} have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks ({BiGANs}) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.},
	journaltitle = {{arXiv}:1605.09782 [cs, stat]},
	author = {Donahue, Jeff and Krähenbühl, Philipp and Darrell, Trevor},
	urldate = {2019-12-12},
	date = {2017-04-03},
	eprinttype = {arxiv},
	eprint = {1605.09782},
	file = {Donahue et al_2017_Adversarial Feature Learning.pdf:/Users/wpq/Dropbox (MIT)/zotero/Donahue et al_2017_Adversarial Feature Learning.pdf:application/pdf}
}

@article{theisNoteEvaluationGenerative2016,
	title = {A note on the evaluation of generative models},
	url = {http://arxiv.org/abs/1511.01844},
	abstract = {Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria---average log-likelihood, Parzen window estimates, and visual fidelity of samples---are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.},
	journaltitle = {{arXiv}:1511.01844 [cs, stat]},
	author = {Theis, Lucas and Oord, Aäron van den and Bethge, Matthias},
	urldate = {2019-12-12},
	date = {2016-04-24},
	eprinttype = {arxiv},
	eprint = {1511.01844},
	file = {Theis et al_2016_A note on the evaluation of generative models.pdf:/Users/wpq/Dropbox (MIT)/zotero/Theis et al_2016_A note on the evaluation of generative models.pdf:application/pdf}
}

@article{reedGenerativeAdversarialText2016,
	title = {Generative Adversarial Text to Image Synthesis},
	url = {http://arxiv.org/abs/1605.05396},
	abstract = {Automatic synthesis of realistic images from text would be interesting and useful, but current {AI} systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks ({GANs}) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and {GAN} formulation to effectively bridge these advances in text and image model- ing, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.},
	journaltitle = {{arXiv}:1605.05396 [cs]},
	author = {Reed, Scott and Akata, Zeynep and Yan, Xinchen and Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak},
	urldate = {2019-12-12},
	date = {2016-06-05},
	eprinttype = {arxiv},
	eprint = {1605.05396},
	file = {Reed et al_2016_Generative Adversarial Text to Image Synthesis.pdf:/Users/wpq/Dropbox (MIT)/zotero/Reed et al_2016_Generative Adversarial Text to Image Synthesis.pdf:application/pdf}
}

@article{metzUnrolledGenerativeAdversarial2017,
	title = {Unrolled Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1611.02163},
	abstract = {We introduce a method to stabilize Generative Adversarial Networks ({GANs}) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of {GANs} with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator.},
	journaltitle = {{arXiv}:1611.02163 [cs, stat]},
	author = {Metz, Luke and Poole, Ben and Pfau, David and Sohl-Dickstein, Jascha},
	urldate = {2019-12-12},
	date = {2017-05-12},
	eprinttype = {arxiv},
	eprint = {1611.02163},
	file = {Metz et al_2017_Unrolled Generative Adversarial Networks.pdf:/Users/wpq/Dropbox (MIT)/zotero/Metz et al_2017_Unrolled Generative Adversarial Networks.pdf:application/pdf}
}

@article{goodfellowGenerativeAdversarialNetworks2014,
	title = {Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	journaltitle = {{arXiv}:1406.2661 [cs, stat]},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	urldate = {2019-12-12},
	date = {2014-06-10},
	eprinttype = {arxiv},
	eprint = {1406.2661},
	file = {Goodfellow et al_2014_Generative Adversarial Networks.pdf:/Users/wpq/Dropbox (MIT)/zotero/Goodfellow et al_2014_Generative Adversarial Networks.pdf:application/pdf}
}

@article{salimansImprovedTechniquesTraining2016,
	title = {Improved Techniques for Training {GANs}},
	url = {http://arxiv.org/abs/1606.03498},
	abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks ({GANs}) framework. We focus on two applications of {GANs}: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on {MNIST}, {CIFAR}-10 and {SVHN}. The generated images are of high quality as confirmed by a visual Turing test: our model generates {MNIST} samples that humans cannot distinguish from real data, and {CIFAR}-10 samples that yield a human error rate of 21.3\%. We also present {ImageNet} samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of {ImageNet} classes.},
	journaltitle = {{arXiv}:1606.03498 [cs]},
	author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
	urldate = {2019-12-12},
	date = {2016-06-10},
	eprinttype = {arxiv},
	eprint = {1606.03498},
	file = {Salimans et al_2016_Improved Techniques for Training GANs.pdf:/Users/wpq/Dropbox (MIT)/zotero/Salimans et al_2016_Improved Techniques for Training GANs2.pdf:application/pdf}
}

@incollection{dentonDeepGenerativeImage2015,
	title = {Deep Generative Image Models using a ￼Laplacian Pyramid of Adversarial Networks},
	url = {http://papers.nips.cc/paper/5773-deep-generative-image-models-using-a-laplacian-pyramid-of-adversarial-networks.pdf},
	pages = {1486--1494},
	booktitle = {Advances in Neural Information Processing Systems 28},
	publisher = {Curran Associates, Inc.},
	author = {Denton, Emily L and Chintala, Soumith and szlam, arthur and Fergus, Rob},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	urldate = {2019-12-12},
	date = {2015},
	file = {Denton et al_2015_Deep Generative Image Models using a ￼Laplacian Pyramid of Adversarial Networks.pdf:/Users/wpq/Dropbox (MIT)/zotero/Denton et al_2015_Deep Generative Image Models using a ￼Laplacian Pyramid of Adversarial Networks.pdf:application/pdf}
}

@article{goodfellowDistinguishabilityCriteriaEstimating2015,
	title = {On distinguishability criteria for estimating generative models},
	url = {http://arxiv.org/abs/1412.6515},
	abstract = {Two recently introduced criteria for estimation of generative models are both based on a reduction to binary classification. Noise-contrastive estimation ({NCE}) is an estimation procedure in which a generative model is trained to be able to distinguish data samples from noise samples. Generative adversarial networks ({GANs}) are pairs of generator and discriminator networks, with the generator network learning to generate samples by attempting to fool the discriminator network into believing its samples are real data. Both estimation procedures use the same function to drive learning, which naturally raises questions about how they are related to each other, as well as whether this function is related to maximum likelihood estimation ({MLE}). {NCE} corresponds to training an internal data model belonging to the \{{\textbackslash}em discriminator\} network but using a fixed generator network. We show that a variant of {NCE}, with a dynamic generator network, is equivalent to maximum likelihood estimation. Since pairing a learned discriminator with an appropriate dynamically selected generator recovers {MLE}, one might expect the reverse to hold for pairing a learned generator with a certain discriminator. However, we show that recovering {MLE} for a learned generator requires departing from the distinguishability game. Specifically: (i) The expected gradient of the {NCE} discriminator can be made to match the expected gradient of {MLE}, if one is allowed to use a non-stationary noise distribution for {NCE}, (ii) No choice of discriminator network can make the expected gradient for the {GAN} generator match that of {MLE}, and (iii) The existing theory does not guarantee that {GANs} will converge in the non-convex case. This suggests that the key next step in {GAN} research is to determine whether {GANs} converge, and if not, to modify their training algorithm to force convergence.},
	journaltitle = {{arXiv}:1412.6515 [stat]},
	author = {Goodfellow, Ian J.},
	urldate = {2019-12-12},
	date = {2015-05-21},
	eprinttype = {arxiv},
	eprint = {1412.6515},
	file = {Goodfellow_2015_On distinguishability criteria for estimating generative models.pdf:/Users/wpq/Dropbox (MIT)/zotero/Goodfellow_2015_On distinguishability criteria for estimating generative models.pdf:application/pdf}
}

@article{radfordUnsupervisedRepresentationLearning2016,
	title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks ({CNNs}) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with {CNNs} has received less attention. In this work we hope to help bridge the gap between the success of {CNNs} for supervised learning and unsupervised learning. We introduce a class of {CNNs} called deep convolutional generative adversarial networks ({DCGANs}), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	journaltitle = {{arXiv}:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	urldate = {2019-12-12},
	date = {2016-01-07},
	eprinttype = {arxiv},
	eprint = {1511.06434},
	file = {Radford et al_2016_Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.pdf:/Users/wpq/Dropbox (MIT)/zotero/Radford et al_2016_Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.pdf:application/pdf}
}

@article{chenInfoGANInterpretableRepresentation2016,
	title = {{InfoGAN}: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets},
	url = {http://arxiv.org/abs/1606.03657},
	shorttitle = {{InfoGAN}},
	abstract = {This paper describes {InfoGAN}, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. {InfoGAN} is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, {InfoGAN} successfully disentangles writing styles from digit shapes on the {MNIST} dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the {SVHN} dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the {CelebA} face dataset. Experiments show that {InfoGAN} learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
	journaltitle = {{arXiv}:1606.03657 [cs, stat]},
	author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
	urldate = {2019-12-04},
	date = {2016-06-11},
	eprinttype = {arxiv},
	eprint = {1606.03657},
	file = {Chen et al_2016_InfoGAN - Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.pdf:/Users/wpq/Dropbox (MIT)/zotero/Chen et al_2016_InfoGAN - Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.pdf:application/pdf}
}

@article{jahanianSteerabilityGenerativeAdversarial2019,
	title = {On the ''steerability" of generative adversarial networks},
	url = {http://arxiv.org/abs/1907.07171},
	abstract = {An open secret in contemporary machine learning is that many models work beautifully on standard benchmarks but fail to generalize outside the lab. This has been attributed to training on biased data, which provide poor coverage over real world events. Generative models are no exception, but recent advances in generative adversarial networks ({GANs}) suggest otherwise -- these models can now synthesize strikingly realistic and diverse images. Is generative modeling of photos a solved problem? We show that although current {GANs} can fit standard datasets very well, they still fall short of being comprehensive models of the visual manifold. In particular, we study their ability to fit simple transformations such as camera movements and color changes. We find that the models reflect the biases of the datasets on which they are trained (e.g., centered objects), but that they also exhibit some capacity for generalization: by "steering" in latent space, we can shift the distribution while still creating realistic images. We hypothesize that the degree of distributional shift is related to the breadth of the training data distribution, and conduct experiments that demonstrate this. Code is released on our project page: https://ali-design.github.io/gan\_steerability/},
	journaltitle = {{arXiv}:1907.07171 [cs]},
	author = {Jahanian, Ali and Chai, Lucy and Isola, Phillip},
	urldate = {2019-09-14},
	date = {2019-07-16},
	eprinttype = {arxiv},
	eprint = {1907.07171},
	file = {Jahanian et al_2019_On the ''steerability of generative adversarial networks.pdf:/Users/wpq/Dropbox (MIT)/zotero/Jahanian et al_2019_On the ''steerability of generative adversarial networks.pdf:application/pdf}
}

@article{mohamedLearningImplicitGenerative2017,
	title = {Learning in Implicit Generative Models},
	url = {http://arxiv.org/abs/1610.03483},
	abstract = {Generative adversarial networks ({GANs}) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop our understanding of {GANs} with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame {GANs} within the wider landscape of algorithms for learning in implicit generative models--models that only specify a stochastic procedure with which to generate data--and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by {GANs}, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the {GAN} literature, and we synthesise these views to form an understanding in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.},
	journaltitle = {{arXiv}:1610.03483 [cs, stat]},
	author = {Mohamed, Shakir and Lakshminarayanan, Balaji},
	urldate = {2020-01-27},
	date = {2017-02-27},
	eprinttype = {arxiv},
	eprint = {1610.03483},
	file = {Mohamed_Lakshminarayanan_2017_Learning in Implicit Generative Models.pdf:/Users/wpq/Dropbox (MIT)/zotero/Mohamed_Lakshminarayanan_2017_Learning in Implicit Generative Models.pdf:application/pdf}
}

@article{miyatoCGANsProjectionDiscriminator2018,
	title = {{cGANs} with Projection Discriminator},
	url = {http://arxiv.org/abs/1802.05637},
	abstract = {We propose a novel, projection based way to incorporate the conditional information into the discriminator of {GANs} that respects the role of the conditional information in the underlining probabilistic model. This approach is in contrast with most frameworks of conditional {GANs} used in application today, which use the conditional information by concatenating the (embedded) conditional vector to the feature vectors. With this modification, we were able to significantly improve the quality of the class conditional image generation on {ILSVRC}2012 ({ImageNet}) 1000-class image dataset from the current state-of-the-art result, and we achieved this with a single pair of a discriminator and a generator. We were also able to extend the application to super-resolution and succeeded in producing highly discriminative super-resolution images. This new structure also enabled high quality category transformation based on parametric functional transformation of conditional batch normalization layers in the generator.},
	journaltitle = {{arXiv}:1802.05637 [cs, stat]},
	author = {Miyato, Takeru and Koyama, Masanori},
	urldate = {2020-02-03},
	date = {2018-08-14},
	eprinttype = {arxiv},
	eprint = {1802.05637},
	file = {Miyato_Koyama_2018_cGANs with Projection Discriminator.pdf:/Users/wpq/Dropbox (MIT)/zotero/Miyato_Koyama_2018_cGANs with Projection Discriminator.pdf:application/pdf}
}

@inproceedings{odenaConditionalImageSynthesis2017,
	title = {Conditional Image Synthesis with Auxiliary Classifier {GANs}},
	url = {http://proceedings.mlr.press/v70/odena17a.html},
	abstract = {In this paper we introduce new methods for the improved training of generative adversarial networks ({GANs}) for image synthesis. We construct a variant of {GANs} employing label conditioning that resu...},
	eventtitle = {International Conference on Machine Learning},
	pages = {2642--2651},
	booktitle = {International Conference on Machine Learning},
	author = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
	urldate = {2020-02-03},
	date = {2017-07-17},
	langid = {english},
	file = {Odena et al_2017_Conditional Image Synthesis with Auxiliary Classifier GANs.pdf:/Users/wpq/Dropbox (MIT)/zotero/Odena et al_2017_Conditional Image Synthesis with Auxiliary Classifier GANs2.pdf:application/pdf}
}

@article{arjovskyWassersteinGAN2017a,
	title = {Wasserstein {GAN}},
	url = {http://arxiv.org/abs/1701.07875},
	abstract = {We introduce a new algorithm named {WGAN}, an alternative to traditional {GAN} training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
	journaltitle = {{arXiv}:1701.07875 [cs, stat]},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	urldate = {2020-02-14},
	date = {2017-12-06},
	eprinttype = {arxiv},
	eprint = {1701.07875},
	file = {Arjovsky et al_2017_Wasserstein GAN.pdf:/Users/wpq/Dropbox (MIT)/zotero/Arjovsky et al_2017_Wasserstein GAN2.pdf:application/pdf}
}

@article{gulrajaniImprovedTrainingWasserstein2017,
	title = {Improved Training of Wasserstein {GANs}},
	url = {http://arxiv.org/abs/1704.00028},
	abstract = {Generative Adversarial Networks ({GANs}) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein {GAN} ({WGAN}) makes progress toward stable training of {GANs}, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in {WGAN} to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard {WGAN} and enables stable training of a wide variety of {GAN} architectures with almost no hyperparameter tuning, including 101-layer {ResNets} and language models over discrete data. We also achieve high quality generations on {CIFAR}-10 and {LSUN} bedrooms.},
	journaltitle = {{arXiv}:1704.00028 [cs, stat]},
	author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
	urldate = {2020-02-14},
	date = {2017-12-25},
	eprinttype = {arxiv},
	eprint = {1704.00028},
	file = {Gulrajani et al_2017_Improved Training of Wasserstein GANs.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gulrajani et al_2017_Improved Training of Wasserstein GANs.pdf:application/pdf}
}

@article{huUnifyingDeepGenerative2018,
	title = {On Unifying Deep Generative Models},
	url = {http://arxiv.org/abs/1706.00550},
	abstract = {Deep generative models have achieved impressive success in recent years. Generative Adversarial Networks ({GANs}) and Variational Autoencoders ({VAEs}), as emerging families for generative model learning, have largely been considered as two distinct paradigms and received extensive independent studies respectively. This paper aims to establish formal connections between {GANs} and {VAEs} through a new formulation of them. We interpret sample generation in {GANs} as performing posterior inference, and show that {GANs} and {VAEs} involve minimizing {KL} divergences of respective posterior and inference distributions with opposite directions, extending the two learning phases of classic wake-sleep algorithm, respectively. The unified view provides a powerful tool to analyze a diverse set of existing model variants, and enables to transfer techniques across research lines in a principled way. For example, we apply the importance weighting method in {VAE} literatures for improved {GAN} learning, and enhance {VAEs} with an adversarial mechanism that leverages generated samples. Experiments show generality and effectiveness of the transferred techniques.},
	journaltitle = {{arXiv}:1706.00550 [cs, stat]},
	author = {Hu, Zhiting and Yang, Zichao and Salakhutdinov, Ruslan and Xing, Eric P.},
	urldate = {2020-02-15},
	date = {2018-07-11},
	eprinttype = {arxiv},
	eprint = {1706.00550},
	file = {Hu et al_2018_On Unifying Deep Generative Models.pdf:/Users/wpq/Dropbox (MIT)/zotero/Hu et al_2018_On Unifying Deep Generative Models.pdf:application/pdf}
}

@article{limGeometricGAN2017,
	title = {Geometric {GAN}},
	url = {http://arxiv.org/abs/1705.02894},
	abstract = {Generative Adversarial Nets ({GANs}) represent an important milestone for effective generative models, which has inspired numerous variants seemingly different from each other. One of the main contributions of this paper is to reveal a unified geometric structure in {GAN} and its variants. Specifically, we show that the adversarial generative model training can be decomposed into three geometric steps: separating hyperplane search, discriminator parameter update away from the separating hyperplane, and the generator update along the normal vector direction of the separating hyperplane. This geometric intuition reveals the limitations of the existing approaches and leads us to propose a new formulation called geometric {GAN} using {SVM} separating hyperplane that maximizes the margin. Our theoretical analysis shows that the geometric {GAN} converges to a Nash equilibrium between the discriminator and generator. In addition, extensive numerical results show that the superior performance of geometric {GAN}.},
	journaltitle = {{arXiv}:1705.02894 [cond-mat, stat]},
	author = {Lim, Jae Hyun and Ye, Jong Chul},
	urldate = {2020-02-15},
	date = {2017-05-08},
	eprinttype = {arxiv},
	eprint = {1705.02894},
	file = {Lim_Ye_2017_Geometric GAN.pdf:/Users/wpq/Dropbox (MIT)/zotero/Lim_Ye_2017_Geometric GAN.pdf:application/pdf}
}

@article{zhouActivationMaximizationGenerative2018,
	title = {Activation Maximization Generative Adversarial Nets},
	url = {http://arxiv.org/abs/1703.02000},
	abstract = {Class labels have been empirically shown useful in improving the sample quality of generative adversarial nets ({GANs}). In this paper, we mathematically study the properties of the current variants of {GANs} that make use of class label information. With class aware gradient and cross-entropy decomposition, we reveal how class labels and associated losses influence {GAN}'s training. Based on that, we propose Activation Maximization Generative Adversarial Networks ({AM}-{GAN}) as an advanced solution. Comprehensive experiments have been conducted to validate our analysis and evaluate the effectiveness of our solution, where {AM}-{GAN} outperforms other strong baselines and achieves state-of-the-art Inception Score (8.91) on {CIFAR}-10. In addition, we demonstrate that, with the Inception {ImageNet} classifier, Inception Score mainly tracks the diversity of the generator, and there is, however, no reliable evidence that it can reflect the true sample quality. We thus propose a new metric, called {AM} Score, to provide a more accurate estimation of the sample quality. Our proposed model also outperforms the baseline methods in the new metric.},
	journaltitle = {{arXiv}:1703.02000 [cs, stat]},
	author = {Zhou, Zhiming and Cai, Han and Rong, Shu and Song, Yuxuan and Ren, Kan and Zhang, Weinan and Yu, Yong and Wang, Jun},
	urldate = {2020-02-15},
	date = {2018-11-16},
	eprinttype = {arxiv},
	eprint = {1703.02000},
	file = {Zhou et al_2018_Activation Maximization Generative Adversarial Nets.pdf:/Users/wpq/Dropbox (MIT)/zotero/Zhou et al_2018_Activation Maximization Generative Adversarial Nets.pdf:application/pdf}
}

@article{nowozinFGANTrainingGenerative2016,
	title = {f-{GAN}: Training Generative Neural Samplers using Variational Divergence Minimization},
	url = {http://arxiv.org/abs/1606.00709},
	shorttitle = {f-{GAN}},
	abstract = {Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution deﬁned by the network weights. These models are expressive and allow efﬁcient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generativeadversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f -divergence can be used for training generative neural samplers. We discuss the beneﬁts of various choices of divergence functions on training complexity and the quality of the obtained generative models.},
	journaltitle = {{arXiv}:1606.00709 [cs, stat]},
	author = {Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
	urldate = {2020-02-15},
	date = {2016-06-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1606.00709},
	file = {Nowozin et al_2016_f-GAN - Training Generative Neural Samplers using Variational Divergence Minimization.pdf:/Users/wpq/Dropbox (MIT)/zotero/Nowozin et al_2016_f-GAN - Training Generative Neural Samplers using Variational Divergence Minimization.pdf:application/pdf}
}

@article{dziugaiteTrainingGenerativeNeural2015,
	title = {Training generative neural networks via Maximum Mean Discrepancy optimization},
	url = {http://arxiv.org/abs/1505.03906},
	abstract = {We consider training a deep neural network to generate samples from an unknown distribution given i.i.d. data. We frame learning as an optimization minimizing a two-sample test statistic---informally speaking, a good generator network produces samples that cause a two-sample test to fail to reject the null hypothesis. As our two-sample test statistic, we use an unbiased estimate of the maximum mean discrepancy, which is the centerpiece of the nonparametric kernel two-sample test proposed by Gretton et al. (2012). We compare to the adversarial nets framework introduced by Goodfellow et al. (2014), in which learning is a two-player game between a generator network and an adversarial discriminator network, both trained to outwit the other. From this perspective, the {MMD} statistic plays the role of the discriminator. In addition to empirical comparisons, we prove bounds on the generalization error incurred by optimizing the empirical {MMD}.},
	journaltitle = {{arXiv}:1505.03906 [cs, stat]},
	author = {Dziugaite, Gintare Karolina and Roy, Daniel M. and Ghahramani, Zoubin},
	urldate = {2020-02-15},
	date = {2015-05-14},
	eprinttype = {arxiv},
	eprint = {1505.03906},
	file = {Dziugaite et al_2015_Training generative neural networks via Maximum Mean Discrepancy optimization.pdf:/Users/wpq/Dropbox (MIT)/zotero/Dziugaite et al_2015_Training generative neural networks via Maximum Mean Discrepancy optimization.pdf:application/pdf}
}

@inproceedings{baoCVAEGANFineGrainedImage2017,
	location = {Venice},
	title = {{CVAE}-{GAN}: Fine-Grained Image Generation through Asymmetric Training},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237561/},
	doi = {10.1109/ICCV.2017.299},
	shorttitle = {{CVAE}-{GAN}},
	abstract = {We present variational generative adversarial networks, a general learning framework that combines a variational auto-encoder with a generative adversarial network, for synthesizing images in ﬁne-grained categories, such as faces of a speciﬁc person or objects in a category. Our approach models an image as a composition of label and latent attributes in a probabilistic model. By varying the ﬁne-grained category label fed into the resulting generative model, we can generate images in a speciﬁc category with randomly drawn values on a latent attribute vector. Our approach has two novel aspects. First, we adopt a cross entropy loss for the discriminative and classiﬁer network, but a mean discrepancy objective for the generative network. This kind of asymmetric loss function makes the {GAN} training more stable. Second, we adopt an encoder network to learn the relationship between the latent space and the real image space, and use pairwise feature matching to keep the structure of generated images. We experiment with natural images of faces, ﬂowers, and birds, and demonstrate that the proposed models are capable of generating realistic and diverse samples with ﬁne-grained category labels. We further show that our models can be applied to other tasks, such as image inpainting, super-resolution, and data augmentation for training better face recognition models.},
	eventtitle = {2017 {IEEE} International Conference on Computer Vision ({ICCV})},
	pages = {2764--2773},
	booktitle = {2017 {IEEE} International Conference on Computer Vision ({ICCV})},
	publisher = {{IEEE}},
	author = {Bao, Jianmin and Chen, Dong and Wen, Fang and Li, Houqiang and Hua, Gang},
	urldate = {2020-02-15},
	date = {2017-10},
	langid = {english},
	file = {Bao et al_2017_CVAE-GAN - Fine-Grained Image Generation through Asymmetric Training.pdf:/Users/wpq/Dropbox (MIT)/zotero/Bao et al_2017_CVAE-GAN - Fine-Grained Image Generation through Asymmetric Training.pdf:application/pdf}
}

@article{miyatoSpectralNormalizationGenerative2018,
	title = {Spectral Normalization for Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1802.05957},
	abstract = {One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on {CIFAR}10, {STL}-10, and {ILSVRC}2012 dataset, and we experimentally confirmed that spectrally normalized {GANs} ({SN}-{GANs}) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.},
	journaltitle = {{arXiv}:1802.05957 [cs, stat]},
	author = {Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
	urldate = {2020-02-15},
	date = {2018-02-16},
	eprinttype = {arxiv},
	eprint = {1802.05957},
	file = {Miyato et al_2018_Spectral Normalization for Generative Adversarial Networks.pdf:/Users/wpq/Dropbox (MIT)/zotero/Miyato et al_2018_Spectral Normalization for Generative Adversarial Networks2.pdf:application/pdf}
}

@article{brockLargeScaleGAN2019,
	title = {Large Scale {GAN} Training for High Fidelity Natural Image Synthesis},
	url = {http://arxiv.org/abs/1809.11096},
	abstract = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as {ImageNet} remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple "truncation trick," allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on {ImageNet} at 128x128 resolution, our models ({BigGANs}) achieve an Inception Score ({IS}) of 166.5 and Frechet Inception Distance ({FID}) of 7.4, improving over the previous best {IS} of 52.52 and {FID} of 18.6.},
	journaltitle = {{arXiv}:1809.11096 [cs, stat]},
	author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
	urldate = {2020-02-15},
	date = {2019-02-25},
	eprinttype = {arxiv},
	eprint = {1809.11096},
	file = {Brock et al_2019_Large Scale GAN Training for High Fidelity Natural Image Synthesis.pdf:/Users/wpq/Dropbox (MIT)/zotero/Brock et al_2019_Large Scale GAN Training for High Fidelity Natural Image Synthesis.pdf:application/pdf}
}

@article{zhaoEnergybasedGenerativeAdversarial2017a,
	title = {Energy-based Generative Adversarial Network},
	url = {http://arxiv.org/abs/1609.03126},
	abstract = {We introduce the "Energy-based Generative Adversarial Network" model ({EBGAN}) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic {GANs}, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of {EBGAN} framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of {EBGAN} exhibits more stable behavior than regular {GANs} during training. We also show that a single-scale architecture can be trained to generate high-resolution images.},
	journaltitle = {{arXiv}:1609.03126 [cs, stat]},
	author = {Zhao, Junbo and Mathieu, Michael and {LeCun}, Yann},
	urldate = {2020-02-20},
	date = {2017-03-06},
	eprinttype = {arxiv},
	eprint = {1609.03126},
	file = {Zhao et al_2017_Energy-based Generative Adversarial Network.pdf:/Users/wpq/Dropbox (MIT)/zotero/Zhao et al_2017_Energy-based Generative Adversarial Network2.pdf:application/pdf}
}

@article{arjovskyPrincipledMethodsTraining2017,
	title = {Towards Principled Methods for Training Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1701.04862},
	abstract = {The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.},
	journaltitle = {{arXiv}:1701.04862 [cs, stat]},
	author = {Arjovsky, Martin and Bottou, Léon},
	urldate = {2020-02-20},
	date = {2017-01-17},
	eprinttype = {arxiv},
	eprint = {1701.04862},
	file = {Arjovsky_Bottou_2017_Towards Principled Methods for Training Generative Adversarial Networks.pdf:/Users/wpq/Dropbox (MIT)/zotero/Arjovsky_Bottou_2017_Towards Principled Methods for Training Generative Adversarial Networks.pdf:application/pdf}
}

@inproceedings{zhangStackGANTextPhotoRealistic2017,
	title = {{StackGAN}: Text to Photo-Realistic Image Synthesis With Stacked Generative Adversarial Networks},
	url = {http://openaccess.thecvf.com/content_iccv_2017/html/Zhang_StackGAN_Text_to_ICCV_2017_paper.html},
	shorttitle = {{StackGAN}},
	eventtitle = {Proceedings of the {IEEE} International Conference on Computer Vision},
	pages = {5907--5915},
	author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris N.},
	urldate = {2020-02-20},
	date = {2017},
	file = {Zhang et al_2017_StackGAN - Text to Photo-Realistic Image Synthesis With Stacked Generative Adversarial Networks.pdf:/Users/wpq/Dropbox (MIT)/zotero/Zhang et al_2017_StackGAN - Text to Photo-Realistic Image Synthesis With Stacked Generative Adversarial Networks.pdf:application/pdf}
}

@article{karrasProgressiveGrowingGANs2018,
	title = {Progressive Growing of {GANs} for Improved Quality, Stability, and Variation},
	url = {http://arxiv.org/abs/1710.10196},
	abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., {CelebA} images at 1024{\textasciicircum}2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised {CIFAR}10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating {GAN} results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the {CelebA} dataset.},
	journaltitle = {{arXiv}:1710.10196 [cs, stat]},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	urldate = {2020-02-20},
	date = {2018-02-26},
	eprinttype = {arxiv},
	eprint = {1710.10196},
	file = {Karras et al_2018_Progressive Growing of GANs for Improved Quality, Stability, and Variation.pdf:/Users/wpq/Dropbox (MIT)/zotero/Karras et al_2018_Progressive Growing of GANs for Improved Quality, Stability, and Variation.pdf:application/pdf}
}

@article{zhangSelfAttentionGenerativeAdversarial2019,
	title = {Self-Attention Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1805.08318},
	abstract = {In this paper, we propose the Self-Attention Generative Adversarial Network ({SAGAN}) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional {GANs} generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In {SAGAN}, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects {GAN} performance. Leveraging this insight, we apply spectral normalization to the {GAN} generator and find that this improves training dynamics. The proposed {SAGAN} achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging {ImageNet} dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.},
	journaltitle = {{arXiv}:1805.08318 [cs, stat]},
	author = {Zhang, Han and Goodfellow, Ian and Metaxas, Dimitris and Odena, Augustus},
	urldate = {2020-02-20},
	date = {2019-06-14},
	eprinttype = {arxiv},
	eprint = {1805.08318},
	file = {Zhang et al_2019_Self-Attention Generative Adversarial Networks.pdf:/Users/wpq/Dropbox (MIT)/zotero/Zhang et al_2019_Self-Attention Generative Adversarial Networks.pdf:application/pdf}
}

@article{heuselGANsTrainedTwo2018,
	title = {{GANs} Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
	url = {http://arxiv.org/abs/1706.08500},
	abstract = {Generative Adversarial Networks ({GANs}) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of {GAN} training has still not been proved. We propose a two time-scale update rule ({TTUR}) for training {GANs} with stochastic gradient descent on arbitrary {GAN} loss functions. {TTUR} has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the {TTUR} converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of {GANs} at image generation, we introduce the "Fr{\textbackslash}'echet Inception Distance" ({FID}) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, {TTUR} improves learning for {DCGANs} and Improved Wasserstein {GANs} ({WGAN}-{GP}) outperforming conventional {GAN} training on {CelebA}, {CIFAR}-10, {SVHN}, {LSUN} Bedrooms, and the One Billion Word Benchmark.},
	journaltitle = {{arXiv}:1706.08500 [cs, stat]},
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	urldate = {2020-02-20},
	date = {2018-01-12},
	eprinttype = {arxiv},
	eprint = {1706.08500},
	file = {Heusel et al_2018_GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.pdf:/Users/wpq/Dropbox (MIT)/zotero/Heusel et al_2018_GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.pdf:application/pdf}
}

@article{goukRegularisationNeuralNetworks2018,
	title = {Regularisation of Neural Networks by Enforcing Lipschitz Continuity},
	url = {http://arxiv.org/abs/1804.04368},
	abstract = {We investigate the effect of explicitly enforcing the Lipschitz continuity of neural networks with respect to their inputs. To this end, we provide a simple technique for computing an upper bound to the Lipschitz constant of a feed forward neural network composed of commonly used layer types and demonstrate inaccuracies in previous work on this topic. Our technique is then used to formulate training a neural network with a bounded Lipschitz constant as a constrained optimisation problem that can be solved using projected stochastic gradient methods. Our evaluation study shows that, in isolation, our method performs comparatively to state-of-the-art regularisation techniques. Moreover, when combined with existing approaches to regularising neural networks the performance gains are cumulative. We also provide evidence that the hyperparameters are intuitive to tune and demonstrate how the choice of norm for computing the Lipschitz constant impacts the resulting model.},
	journaltitle = {{arXiv}:1804.04368 [cs, stat]},
	author = {Gouk, Henry and Frank, Eibe and Pfahringer, Bernhard and Cree, Michael},
	urldate = {2020-02-20},
	date = {2018-09-14},
	eprinttype = {arxiv},
	eprint = {1804.04368},
	file = {Gouk et al_2018_Regularisation of Neural Networks by Enforcing Lipschitz Continuity.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gouk et al_2018_Regularisation of Neural Networks by Enforcing Lipschitz Continuity.pdf:application/pdf}
}

@article{meschederWhichTrainingMethods2018,
	title = {Which Training Methods for {GANs} do actually Converge?},
	url = {http://arxiv.org/abs/1801.04406},
	abstract = {Recent work has shown local convergence of {GAN} training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized {GAN} training is not always convergent. Furthermore, we discuss regularization strategies that were recently proposed to stabilize {GAN} training. Our analysis shows that {GAN} training with instance noise or zero-centered gradient penalties converges. On the other hand, we show that Wasserstein-{GANs} and {WGAN}-{GP} with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of {GAN} training. Based on our analysis, we extend our convergence results to more general {GANs} and prove local convergence for simplified gradient penalties even if the generator and data distribution lie on lower dimensional manifolds. We find these penalties to work well in practice and use them to learn high-resolution generative image models for a variety of datasets with little hyperparameter tuning.},
	journaltitle = {{arXiv}:1801.04406 [cs]},
	author = {Mescheder, Lars and Geiger, Andreas and Nowozin, Sebastian},
	urldate = {2020-03-07},
	date = {2018-07-31},
	eprinttype = {arxiv},
	eprint = {1801.04406},
	file = {Mescheder et al_2018_Which Training Methods for GANs do actually Converge.pdf:/Users/wpq/Dropbox (MIT)/zotero/Mescheder et al_2018_Which Training Methods for GANs do actually Converge.pdf:application/pdf}
}

@article{mrouehMcGanMeanCovariance2017,
	title = {{McGan}: Mean and Covariance Feature Matching {GAN}},
	url = {http://arxiv.org/abs/1702.08398},
	shorttitle = {{McGan}},
	abstract = {We introduce new families of Integral Probability Metrics ({IPM}) for training Generative Adversarial Networks ({GAN}). Our {IPMs} are based on matching statistics of distributions embedded in a finite dimensional feature space. Mean and covariance feature matching {IPMs} allow for stable training of {GANs}, which we will call {McGan}. {McGan} minimizes a meaningful loss between distributions.},
	journaltitle = {{arXiv}:1702.08398 [cs, stat]},
	author = {Mroueh, Youssef and Sercu, Tom and Goel, Vaibhava},
	urldate = {2020-03-12},
	date = {2017-06-08},
	eprinttype = {arxiv},
	eprint = {1702.08398},
	file = {Mroueh et al_2017_McGan - Mean and Covariance Feature Matching GAN.pdf:/Users/wpq/Dropbox (MIT)/zotero/Mroueh et al_2017_McGan - Mean and Covariance Feature Matching GAN.pdf:application/pdf}
}

@article{rothStabilizingTrainingGenerative2017,
	title = {Stabilizing Training of Generative Adversarial Networks through Regularization},
	url = {http://arxiv.org/abs/1705.09367},
	abstract = {Deep generative models based on Generative Adversarial Networks ({GANs}) have demonstrated impressive sample quality but in order to work they require a careful choice of architecture, parameter initialization, and selection of hyper-parameters. This fragility is in part due to a dimensional mismatch or non-overlapping support between the model distribution and the data distribution, causing their density ratio and the associated f-divergence to be undefined. We overcome this fundamental limitation and propose a new regularization approach with low computational cost that yields a stable {GAN} training procedure. We demonstrate the effectiveness of this regularizer across several architectures trained on common benchmark image generation tasks. Our regularization turns {GAN} models into reliable building blocks for deep learning.},
	journaltitle = {{arXiv}:1705.09367 [cs, stat]},
	author = {Roth, Kevin and Lucchi, Aurelien and Nowozin, Sebastian and Hofmann, Thomas},
	urldate = {2020-03-17},
	date = {2017-11-07},
	eprinttype = {arxiv},
	eprint = {1705.09367},
	file = {Roth et al_2017_Stabilizing Training of Generative Adversarial Networks through Regularization.pdf:/Users/wpq/Dropbox (MIT)/zotero/Roth et al_2017_Stabilizing Training of Generative Adversarial Networks through Regularization.pdf:application/pdf}
}

@incollection{meschederNumericsGANs2017,
	title = {The Numerics of {GANs}},
	url = {http://papers.nips.cc/paper/6779-the-numerics-of-gans.pdf},
	pages = {1825--1835},
	booktitle = {Advances in Neural Information Processing Systems 30},
	publisher = {Curran Associates, Inc.},
	author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	urldate = {2020-03-17},
	date = {2017},
	file = {Mescheder et al_2017_The Numerics of GANs.pdf:/Users/wpq/Dropbox (MIT)/zotero/Mescheder et al_2017_The Numerics of GANs.pdf:application/pdf}
}

@article{cheModeRegularizedGenerative2017,
	title = {Mode Regularized Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1612.02136},
	abstract = {Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of {GANs} are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of {GAN} models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem.},
	journaltitle = {{arXiv}:1612.02136 [cs]},
	author = {Che, Tong and Li, Yanran and Jacob, Athul Paul and Bengio, Yoshua and Li, Wenjie},
	urldate = {2020-03-17},
	date = {2017-03-02},
	eprinttype = {arxiv},
	eprint = {1612.02136},
	file = {Che et al_2017_Mode Regularized Generative Adversarial Networks.pdf:/Users/wpq/Dropbox (MIT)/zotero/Che et al_2017_Mode Regularized Generative Adversarial Networks.pdf:application/pdf}
}

@article{nagarajanGradientDescentGAN2018,
	title = {Gradient descent {GAN} optimization is locally stable},
	url = {http://arxiv.org/abs/1706.04156},
	abstract = {Despite the growing prominence of generative adversarial networks ({GANs}), optimization in {GANs} is still a poorly understood topic. In this paper, we analyze the "gradient descent" form of {GAN} optimization i.e., the natural setting where we simultaneously take small gradient steps in both generator and discriminator parameters. We show that even though {GAN} optimization does not correspond to a convex-concave game (even for simple parameterizations), under proper conditions, equilibrium points of this optimization procedure are still {\textbackslash}emph\{locally asymptotically stable\} for the traditional {GAN} formulation. On the other hand, we show that the recently proposed Wasserstein {GAN} can have non-convergent limit cycles near equilibrium. Motivated by this stability analysis, we propose an additional regularization term for gradient descent {GAN} updates, which {\textbackslash}emph\{is\} able to guarantee local stability for both the {WGAN} and the traditional {GAN}, and also shows practical promise in speeding up convergence and addressing mode collapse.},
	journaltitle = {{arXiv}:1706.04156 [cs, math, stat]},
	author = {Nagarajan, Vaishnavh and Kolter, J. Zico},
	urldate = {2020-03-17},
	date = {2018-01-13},
	eprinttype = {arxiv},
	eprint = {1706.04156},
	file = {Nagarajan_Kolter_2018_Gradient descent GAN optimization is locally stable.pdf:/Users/wpq/Dropbox (MIT)/zotero/Nagarajan_Kolter_2018_Gradient descent GAN optimization is locally stable.pdf:application/pdf}
}

@article{pfauConnectingGenerativeAdversarial2017,
	title = {Connecting Generative Adversarial Networks and Actor-Critic Methods},
	url = {http://arxiv.org/abs/1610.01945},
	abstract = {Both generative adversarial networks ({GAN}) in unsupervised learning and actor-critic methods in reinforcement learning ({RL}) have gained a reputation for being difficult to optimize. Practitioners in both fields have amassed a large number of strategies to mitigate these instabilities and improve training. Here we show that {GANs} can be viewed as actor-critic methods in an environment where the actor cannot affect the reward. We review the strategies for stabilizing training for each class of models, both those that generalize between the two and those that are particular to that model. We also review a number of extensions to {GANs} and {RL} algorithms with even more complicated information flow. We hope that by highlighting this formal connection we will encourage both {GAN} and {RL} communities to develop general, scalable, and stable algorithms for multilevel optimization with deep networks, and to draw inspiration across communities.},
	journaltitle = {{arXiv}:1610.01945 [cs, stat]},
	author = {Pfau, David and Vinyals, Oriol},
	urldate = {2020-03-17},
	date = {2017-01-18},
	eprinttype = {arxiv},
	eprint = {1610.01945},
	file = {Pfau_Vinyals_2017_Connecting Generative Adversarial Networks and Actor-Critic Methods.pdf:/Users/wpq/Dropbox (MIT)/zotero/Pfau_Vinyals_2017_Connecting Generative Adversarial Networks and Actor-Critic Methods.pdf:application/pdf}
}

@article{kocaogluCausalGANLearningCausal2017,
	title = {{CausalGAN}: Learning Causal Implicit Generative Models with Adversarial Training},
	url = {http://arxiv.org/abs/1709.02023},
	shorttitle = {{CausalGAN}},
	abstract = {We propose an adversarial training procedure for learning a causal implicit generative model for a given causal graph. We show that adversarial training can be used to learn a generative model with true observational and interventional distributions if the generator architecture is consistent with the given causal graph. We consider the application of generating faces based on given binary labels where the dependency structure between the labels is preserved with a causal graph. This problem can be seen as learning a causal implicit generative model for the image and labels. We devise a two-stage procedure for this problem. First we train a causal implicit generative model over binary labels using a neural network consistent with a causal graph as the generator. We empirically show that {WassersteinGAN} can be used to output discrete labels. Later, we propose two new conditional {GAN} architectures, which we call {CausalGAN} and {CausalBEGAN}. We show that the optimal generator of the {CausalGAN}, given the labels, samples from the image distributions conditioned on these labels. The conditional {GAN} combined with a trained causal implicit generative model for the labels is then a causal implicit generative model over the labels and the generated image. We show that the proposed architectures can be used to sample from observational and interventional image distributions, even for interventions which do not naturally occur in the dataset.},
	journaltitle = {{arXiv}:1709.02023 [cs, math, stat]},
	author = {Kocaoglu, Murat and Snyder, Christopher and Dimakis, Alexandros G. and Vishwanath, Sriram},
	urldate = {2020-03-21},
	date = {2017-09-14},
	eprinttype = {arxiv},
	eprint = {1709.02023},
	file = {Kocaoglu et al_2017_CausalGAN - Learning Causal Implicit Generative Models with Adversarial Training.pdf:/Users/wpq/Dropbox (MIT)/zotero/Kocaoglu et al_2017_CausalGAN - Learning Causal Implicit Generative Models with Adversarial Training.pdf:application/pdf}
}

@article{gempGlobalConvergenceEquilibrium2019,
	title = {Global Convergence to the Equilibrium of {GANs} using Variational Inequalities},
	url = {http://arxiv.org/abs/1808.01531},
	abstract = {In optimization, the negative gradient of a function denotes the direction of steepest descent. Furthermore, traveling in any direction orthogonal to the gradient maintains the value of the function. In this work, we show that these orthogonal directions that are ignored by gradient descent can be critical in equilibrium problems. Equilibrium problems have drawn heightened attention in machine learning due to the emergence of the Generative Adversarial Network ({GAN}). We use the framework of Variational Inequalities to analyze popular training algorithms for a fundamental {GAN} variant: the Wasserstein Linear-Quadratic {GAN}. We show that the steepest descent direction causes divergence from the equilibrium, and convergence to the equilibrium is achieved through following a particular orthogonal direction. We call this successful technique Crossing-the-Curl, named for its mathematical derivation as well as its intuition: identify the game's axis of rotation and move "across" space in the direction towards smaller "curling".},
	journaltitle = {{arXiv}:1808.01531 [cs, stat]},
	author = {Gemp, Ian and Mahadevan, Sridhar},
	urldate = {2020-04-11},
	date = {2019-05-20},
	eprinttype = {arxiv},
	eprint = {1808.01531},
	file = {Gemp_Mahadevan_2019_Global Convergence to the Equilibrium of GANs using Variational Inequalities.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gemp_Mahadevan_2019_Global Convergence to the Equilibrium of GANs using Variational Inequalities.pdf:application/pdf}
}

@article{daskalakisTrainingGANsOptimism2018,
	title = {Training {GANs} with Optimism},
	url = {http://arxiv.org/abs/1711.00141},
	abstract = {We address the issue of limit cycling behavior in training Generative Adversarial Networks and propose the use of Optimistic Mirror Decent ({OMD}) for training Wasserstein {GANs}. Recent theoretical results have shown that optimistic mirror decent ({OMD}) can enjoy faster regret rates in the context of zero-sum games. {WGANs} is exactly a context of solving a zero-sum game with simultaneous no-regret dynamics. Moreover, we show that optimistic mirror decent addresses the limit cycling problem in training {WGANs}. We formally show that in the case of bi-linear zero-sum games the last iterate of {OMD} dynamics converges to an equilibrium, in contrast to {GD} dynamics which are bound to cycle. We also portray the huge qualitative difference between {GD} and {OMD} dynamics with toy examples, even when {GD} is modified with many adaptations proposed in the recent literature, such as gradient penalty or momentum. We apply {OMD} {WGAN} training to a bioinformatics problem of generating {DNA} sequences. We observe that models trained with {OMD} achieve consistently smaller {KL} divergence with respect to the true underlying distribution, than models trained with {GD} variants. Finally, we introduce a new algorithm, Optimistic Adam, which is an optimistic variant of Adam. We apply it to {WGAN} training on {CIFAR}10 and observe improved performance in terms of inception score as compared to Adam.},
	journaltitle = {{arXiv}:1711.00141 [cs, stat]},
	author = {Daskalakis, Constantinos and Ilyas, Andrew and Syrgkanis, Vasilis and Zeng, Haoyang},
	urldate = {2020-04-11},
	date = {2018-02-13},
	eprinttype = {arxiv},
	eprint = {1711.00141},
	file = {Daskalakis et al_2018_Training GANs with Optimism.pdf:/Users/wpq/Dropbox (MIT)/zotero/Daskalakis et al_2018_Training GANs with Optimism.pdf:application/pdf}
}

@inproceedings{jordonKnockoffGANGeneratingKnockoffs2018,
	title = {{KnockoffGAN}: Generating Knockoffs for Feature Selection using Generative Adversarial Networks},
	url = {https://openreview.net/forum?id=ByeZ5jC5YQ&source=post_page---------------------------},
	shorttitle = {{KnockoffGAN}},
	abstract = {Feature selection is a pervasive problem. The discovery of relevant features can be as important for performing a particular task (such as to avoid overfitting in prediction) as it can be for...},
	eventtitle = {International Conference on Learning Representations},
	author = {Jordon, James and Yoon, Jinsung and Schaar, Mihaela van der},
	urldate = {2020-04-17},
	date = {2018-09-27},
	file = {Jordon et al_2018_KnockoffGAN - Generating Knockoffs for Feature Selection using Generative Adversarial Networks.pdf:/Users/wpq/Dropbox (MIT)/zotero/Jordon et al_2018_KnockoffGAN - Generating Knockoffs for Feature Selection using Generative Adversarial Networks.pdf:application/pdf}
}