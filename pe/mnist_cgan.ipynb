{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mnist CGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "import torchvision \n",
    "import torchvision.transforms as tv_transforms\n",
    "import torchvision.datasets as tv_datasets\n",
    "import torchvision.utils as tv_utils\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from fid import calculate_activation_statistics\n",
    "from models_classifier import MnistCNN\n",
    "from models_cgan import apply_sn, Discriminator, ConditionalDiscriminator, ConditionalGenerator, ConditionalResidualBlock, ConditionalBatchNorm2d, conv3x3\n",
    "from inception import InceptionV3\n",
    "from datasets import ColorMNIST\n",
    "from plot_tools import plot_im\n",
    "from utils import makedirs_exists_ok, seed_rng, set_cuda_visible_devices, load_weights_from_file, bin_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cgan_jpt'\n",
    "data_root = './data'\n",
    "model_root = f'./models/{model_name}'\n",
    "figure_root = f'./figures/{model_name}'\n",
    "log_root = f'./logs/{model_name}'\n",
    "\n",
    "image_size = 32\n",
    "batch_size = 32\n",
    "seed = 1\n",
    "gpu_id = '0'\n",
    "n_workers = 8\n",
    "load_weights = ''\n",
    "lr = 0.0002\n",
    "beta1 = 0\n",
    "beta2 = 0.9\n",
    "n_epochs = 20\n",
    "log_interval = 100\n",
    "\n",
    "target_type = 'color'\n",
    "dim_z = 50\n",
    "num_classes = 10\n",
    "im_channels = 3\n",
    "conditional = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinalConditionalDiscriminator(Discriminator):\n",
    "    \"\"\" conditional discriminator where the conditioned variable is ordinal\n",
    "    \n",
    "        Taken from:\n",
    "            https://github.com/batmanlab/Explanation_by_Progressive_Exaggeration/blob/master/src/explainer.py\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, conv_channels, conv_dnsample, num_classes, use_sn=True):\n",
    "        \"\"\"\n",
    "            Projection cGAN (ImageNet)\n",
    "                conv_channels = [3, 64, 128, 256, 512, 1024, 1024]\n",
    "                conv_dnsample = [True, True, True, True, True, False]\n",
    "        \"\"\"\n",
    "        super(OrdinalConditionalDiscriminator, self).__init__(conv_channels, conv_dnsample, use_sn=use_sn)\n",
    "        \n",
    "        self.c_embed = apply_sn(nn.Embedding(num_classes, conv_channels[-1]), use_sn)\n",
    "        \n",
    "    def forward(self, x, c):\n",
    "        \"\"\" x    batch_size x im_channels x h x w\n",
    "            c    batch_size\n",
    "        \"\"\"\n",
    "        c = c.view(-1)\n",
    "        # conv_channels = [3, 64, 128, 256, 512, 1024, 1024]\n",
    "        # conv_dnsample = [True, True, True, True, True]\n",
    "        #\n",
    "        # 3x128x128\n",
    "        x = self.residual_blocks(x)\n",
    "        # 1024x4x4\n",
    "        x = self.nonlinearity(x)\n",
    "        x = torch.sum(x, dim=(2,3))   # (global sum pooling)\n",
    "        # 1024\n",
    "        \n",
    "        # sigmoid^-1(p(real/fake|x,c)) =\n",
    "        #     log(p_data(x)/p_model(x)) + \n",
    "        #     log(p_data(c|x)/p_model(c|x))\n",
    "        all_classes = torch.arange(0, num_classes, dtype=torch.long, device=x.device)\n",
    "        W = x @ self.c_embed(all_classes).T\n",
    "        W = torch.cumsum(W, dim=1)\n",
    "        # 10\n",
    "        f_1 = W.gather(dim=1, index=c.view(-1,1))\n",
    "        f_2 = self.linear(x)\n",
    "        \n",
    "        x = f_1 + f_2\n",
    "        # 1\n",
    "        return x\n",
    "    \n",
    "\n",
    "class ConditionalAutoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, enc_channels, dec_channels, num_classes,\n",
    "                 dim_z = 128,\n",
    "                 im_channels = 3):\n",
    "        \"\"\"\n",
    "            enc_channels\n",
    "                [64, 128, 256, 256]\n",
    "                   c1   c2   c3\n",
    "            dec_channels\n",
    "                [256, 128, 64, 64]\n",
    "                   c1   c2   c3\n",
    "            num_classes\n",
    "                if not None, use conditional batchnorm\n",
    "        \"\"\"\n",
    "        super(ConditionalAutoencoder, self).__init__()\n",
    "        \n",
    "        n_enc_blks = len(enc_channels) - 1\n",
    "        n_dec_blks = len(dec_channels) - 1\n",
    "        assert(n_enc_blks > 0)\n",
    "        assert(n_dec_blks > 0)\n",
    "        \n",
    "        self.n_enc_blks = n_enc_blks\n",
    "        self.n_dec_blks = n_dec_blks\n",
    "        self.bottom_width = 4\n",
    "        self.nonlinearity = nn.ReLU()\n",
    "        \n",
    "        resblk_cls = ConditionalResidualBlock\n",
    "        norm_layer = lambda num_features: ConditionalBatchNorm2d(num_features, num_classes)\n",
    "        \n",
    "        self.normalization_initial = norm_layer(im_channels)\n",
    "        self.conv_initial = conv3x3(im_channels, enc_channels[0])\n",
    "        \n",
    "        for i in range(n_enc_blks):\n",
    "            self.add_module(\n",
    "                f'residual_block_enc_{i}',\n",
    "                resblk_cls(enc_channels[i], enc_channels[i+1],\n",
    "                           resample = \"dn\",\n",
    "                           norm_layer = norm_layer,\n",
    "                           nonlinearity = self.nonlinearity,\n",
    "                           resblk_1st = True if i == 0 else False))\n",
    "        \n",
    "        for i in range(n_dec_blks):\n",
    "            self.add_module(\n",
    "                f'residual_block_dec_{i}',\n",
    "                resblk_cls(dec_channels[i], dec_channels[i+1],\n",
    "                           resample = \"up\",\n",
    "                           norm_layer = norm_layer,\n",
    "                           nonlinearity = self.nonlinearity))\n",
    "            \n",
    "        self.normalization_final = norm_layer(dec_channels[-1])\n",
    "        self.conv_final = conv3x3(dec_channels[-1], im_channels)\n",
    "        self.nonlinearity_final = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        \"\"\" x    batch_size x im_channels x h x w\n",
    "            c    batch_size\n",
    "            Returns  \n",
    "                 batch_size x im_channels x h x w\n",
    "        \"\"\"\n",
    "        c = c.view(-1)\n",
    "        # bottom_width = 4\n",
    "        # enc_channels = [64, 128, 256, 256]\n",
    "        # dec_channels = [256, 128, 64, 64]\n",
    "        # im_channnels = 3\n",
    "        #\n",
    "        # 3x32x32\n",
    "        x = self.normalization_initial(x, c)\n",
    "        x = self.nonlinearity(x)\n",
    "        x = self.conv_initial(x)\n",
    "        # 64x32x32\n",
    "        for i in range(self.n_enc_blks):\n",
    "            x = getattr(self, f'residual_block_enc_{i}')(x, c)\n",
    "        # 256x4x4\n",
    "        z = x\n",
    "        # 256x4x4\n",
    "        for i in range(self.n_dec_blks):\n",
    "            x = getattr(self, f'residual_block_dec_{i}')(x, c)\n",
    "        # 64x32x32\n",
    "        x = self.normalization_final(x, c)\n",
    "        x = self.nonlinearity(x)\n",
    "        x = self.conv_final(x)\n",
    "        x = self.nonlinearity_final(x)\n",
    "        # 3x32x32\n",
    "        return x, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 3, 32, 32]) torch.Size([50, 1])\n",
      "torch.Size([50, 3, 32, 32]) torch.Size([50, 3, 32, 32]) torch.Size([50, 256, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "## conditional D\n",
    "##############################\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "conv_channels = [3, 64, 128, 256]\n",
    "conv_dnsample = [True, True, True]\n",
    "D = OrdinalConditionalDiscriminator(conv_channels, conv_dnsample, num_classes)\n",
    "\n",
    "\n",
    "x = torch.rand((50, 3, 32, 32))\n",
    "c = torch.empty((50, 1), dtype=torch.long).random_(0, num_classes)\n",
    "out = D(x, c)\n",
    "\n",
    "print(x.shape, out.shape)\n",
    "\n",
    "##############################\n",
    "## conditional autoencoder\n",
    "##############################\n",
    "\n",
    "num_classes = 10\n",
    "enc_channels = [64, 128, 256, 256]\n",
    "dec_channels = [256, 128, 64, 64]\n",
    "im_channnels = 3\n",
    "\n",
    "G = ConditionalAutoencoder(enc_channels, dec_channels, num_classes, im_channels=im_channels)\n",
    "\n",
    "x = torch.rand((50, 3, 32, 32))\n",
    "c = torch.empty((50, 1), dtype=torch.long).random_(0, num_classes)\n",
    "xhat, z = G(x, c)\n",
    "\n",
    "print(x.shape, xhat.shape, z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(figure_root, exist_ok=True)\n",
    "os.makedirs(model_root,  exist_ok=True)\n",
    "os.makedirs(log_root,    exist_ok=True)\n",
    "\n",
    "writer = SummaryWriter(log_root)\n",
    "writer.flush()\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_id\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transforms = tv_transforms.Compose([\n",
    "    tv_transforms.Resize(image_size),\n",
    "    tv_transforms.ToTensor(),\n",
    "    tv_transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    ColorMNIST(root=data_root, download=True, train=True, transform=transforms),\n",
    "    batch_size=batch_size, shuffle=True, num_workers=n_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conv_channels = [256, 256, 128, 64]\n",
    "conv_upsample = [True, True, True]\n",
    "\n",
    "enc_channels = [64, 128, 256, 256]\n",
    "dec_channels = [256, 128, 64, 64]\n",
    "\n",
    "conv_channels = [im_channels, 64, 128, 256]\n",
    "conv_dnsample = [True, True, True]\n",
    "\n",
    "# G = ConditionalAutoencoder(enc_channels, dec_channels, num_classes=num_classes, dim_z=dim_z, im_channels=im_channels).to(device)\n",
    "G = ConditionalGenerator(conv_channels, conv_upsample, num_classes=num_classes, dim_z=dim_z, im_channels=im_channels).to(device)\n",
    "D = OrdinalConditionalDiscriminator(conv_channels, conv_dnsample, num_classes, use_sn=True).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_D = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr, (beta1, beta2))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr, (beta1, beta2))\n",
    "\n",
    "fixed_z = torch.randn(100, dim_z).to(device)\n",
    "fixed_c = torch.arange(10).repeat(10).to(device)\n",
    "\n",
    "real_label, fake_label = 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/20]\t[3200/60000 (5%)]\tloss: 2.323\tloss_D: 1.241\tloss_G: 1.081\t\n",
      "[1/20]\t[6400/60000 (11%)]\tloss: 2.249\tloss_D: 1.24\tloss_G: 1.009\t\n",
      "[1/20]\t[9600/60000 (16%)]\tloss: 2.379\tloss_D: 1.41\tloss_G: 0.9699\t\n",
      "[1/20]\t[12800/60000 (21%)]\tloss: 2.406\tloss_D: 1.28\tloss_G: 1.126\t\n",
      "[1/20]\t[16000/60000 (27%)]\tloss: 2.379\tloss_D: 1.272\tloss_G: 1.108\t\n",
      "[1/20]\t[19200/60000 (32%)]\tloss: 2.312\tloss_D: 1.243\tloss_G: 1.069\t\n",
      "[1/20]\t[22400/60000 (37%)]\tloss: 2.267\tloss_D: 1.246\tloss_G: 1.021\t\n",
      "[1/20]\t[25600/60000 (43%)]\tloss: 2.245\tloss_D: 1.416\tloss_G: 0.8286\t\n",
      "[1/20]\t[28800/60000 (48%)]\tloss: 2.444\tloss_D: 1.351\tloss_G: 1.093\t\n",
      "[1/20]\t[32000/60000 (53%)]\tloss: 2.145\tloss_D: 1.303\tloss_G: 0.8417\t\n",
      "[1/20]\t[35200/60000 (59%)]\tloss: 2.187\tloss_D: 1.346\tloss_G: 0.841\t\n",
      "[1/20]\t[38400/60000 (64%)]\tloss: 2.336\tloss_D: 1.397\tloss_G: 0.9387\t\n",
      "[1/20]\t[41600/60000 (69%)]\tloss: 2.273\tloss_D: 1.186\tloss_G: 1.087\t\n",
      "[1/20]\t[44800/60000 (75%)]\tloss: 2.314\tloss_D: 1.164\tloss_G: 1.15\t\n",
      "[1/20]\t[48000/60000 (80%)]\tloss: 2.301\tloss_D: 1.157\tloss_G: 1.144\t\n",
      "[1/20]\t[51200/60000 (85%)]\tloss: 2.273\tloss_D: 1.394\tloss_G: 0.8796\t\n",
      "[1/20]\t[54400/60000 (91%)]\tloss: 2.571\tloss_D: 1.373\tloss_G: 1.198\t\n",
      "[1/20]\t[57600/60000 (96%)]\tloss: 2.386\tloss_D: 1.356\tloss_G: 1.031\t\n",
      "[2/20]\t[3200/60000 (5%)]\tloss: 2.265\tloss_D: 1.315\tloss_G: 0.9501\t\n",
      "[2/20]\t[6400/60000 (11%)]\tloss: 2.167\tloss_D: 1.364\tloss_G: 0.8036\t\n",
      "[2/20]\t[9600/60000 (16%)]\tloss: 2.021\tloss_D: 1.271\tloss_G: 0.7495\t\n",
      "[2/20]\t[12800/60000 (21%)]\tloss: 1.976\tloss_D: 1.248\tloss_G: 0.7279\t\n",
      "[2/20]\t[16000/60000 (27%)]\tloss: 2.234\tloss_D: 1.477\tloss_G: 0.7565\t\n",
      "[2/20]\t[19200/60000 (32%)]\tloss: 1.889\tloss_D: 1.235\tloss_G: 0.6544\t\n",
      "[2/20]\t[22400/60000 (37%)]\tloss: 2.464\tloss_D: 1.309\tloss_G: 1.155\t\n",
      "[2/20]\t[25600/60000 (43%)]\tloss: 2.175\tloss_D: 1.353\tloss_G: 0.8218\t\n",
      "[2/20]\t[28800/60000 (48%)]\tloss: 2.038\tloss_D: 1.28\tloss_G: 0.7582\t\n",
      "[2/20]\t[32000/60000 (53%)]\tloss: 2.369\tloss_D: 1.355\tloss_G: 1.014\t\n",
      "[2/20]\t[35200/60000 (59%)]\tloss: 2.205\tloss_D: 1.282\tloss_G: 0.9229\t\n",
      "[2/20]\t[38400/60000 (64%)]\tloss: 2.377\tloss_D: 1.346\tloss_G: 1.031\t\n",
      "[2/20]\t[41600/60000 (69%)]\tloss: 2.115\tloss_D: 1.257\tloss_G: 0.8577\t\n",
      "[2/20]\t[44800/60000 (75%)]\tloss: 2.053\tloss_D: 1.281\tloss_G: 0.7719\t\n",
      "[2/20]\t[48000/60000 (80%)]\tloss: 2.2\tloss_D: 1.298\tloss_G: 0.9016\t\n",
      "[2/20]\t[51200/60000 (85%)]\tloss: 2.08\tloss_D: 1.353\tloss_G: 0.7269\t\n",
      "[2/20]\t[54400/60000 (91%)]\tloss: 2.4\tloss_D: 1.304\tloss_G: 1.096\t\n",
      "[2/20]\t[57600/60000 (96%)]\tloss: 2.107\tloss_D: 1.263\tloss_G: 0.8445\t\n",
      "[3/20]\t[3200/60000 (5%)]\tloss: 2.221\tloss_D: 1.283\tloss_G: 0.9387\t\n",
      "[3/20]\t[6400/60000 (11%)]\tloss: 2.274\tloss_D: 1.37\tloss_G: 0.9038\t\n",
      "[3/20]\t[9600/60000 (16%)]\tloss: 2.149\tloss_D: 1.32\tloss_G: 0.8289\t\n",
      "[3/20]\t[12800/60000 (21%)]\tloss: 2.237\tloss_D: 1.36\tloss_G: 0.8772\t\n",
      "[3/20]\t[16000/60000 (27%)]\tloss: 2.173\tloss_D: 1.36\tloss_G: 0.8136\t\n",
      "[3/20]\t[19200/60000 (32%)]\tloss: 2.121\tloss_D: 1.267\tloss_G: 0.8535\t\n",
      "[3/20]\t[22400/60000 (37%)]\tloss: 2.036\tloss_D: 1.231\tloss_G: 0.8051\t\n",
      "[3/20]\t[25600/60000 (43%)]\tloss: 2.011\tloss_D: 1.293\tloss_G: 0.7176\t\n",
      "[3/20]\t[28800/60000 (48%)]\tloss: 2.208\tloss_D: 1.367\tloss_G: 0.8414\t\n",
      "[3/20]\t[32000/60000 (53%)]\tloss: 2.06\tloss_D: 1.248\tloss_G: 0.8113\t\n",
      "[3/20]\t[35200/60000 (59%)]\tloss: 2.262\tloss_D: 1.352\tloss_G: 0.9105\t\n",
      "[3/20]\t[38400/60000 (64%)]\tloss: 2.034\tloss_D: 1.319\tloss_G: 0.7152\t\n",
      "[3/20]\t[41600/60000 (69%)]\tloss: 2.269\tloss_D: 1.417\tloss_G: 0.8525\t\n",
      "[3/20]\t[44800/60000 (75%)]\tloss: 2.014\tloss_D: 1.351\tloss_G: 0.6632\t\n",
      "[3/20]\t[48000/60000 (80%)]\tloss: 2.046\tloss_D: 1.278\tloss_G: 0.7676\t\n",
      "[3/20]\t[51200/60000 (85%)]\tloss: 2.201\tloss_D: 1.349\tloss_G: 0.8522\t\n",
      "[3/20]\t[54400/60000 (91%)]\tloss: 2.03\tloss_D: 1.26\tloss_G: 0.7701\t\n",
      "[3/20]\t[57600/60000 (96%)]\tloss: 1.998\tloss_D: 1.264\tloss_G: 0.7341\t\n",
      "[4/20]\t[3200/60000 (5%)]\tloss: 2.074\tloss_D: 1.335\tloss_G: 0.739\t\n",
      "[4/20]\t[6400/60000 (11%)]\tloss: 2.144\tloss_D: 1.323\tloss_G: 0.8214\t\n",
      "[4/20]\t[9600/60000 (16%)]\tloss: 2.244\tloss_D: 1.343\tloss_G: 0.9009\t\n",
      "[4/20]\t[12800/60000 (21%)]\tloss: 2.396\tloss_D: 1.356\tloss_G: 1.04\t\n",
      "[4/20]\t[16000/60000 (27%)]\tloss: 2.041\tloss_D: 1.313\tloss_G: 0.7279\t\n",
      "[4/20]\t[19200/60000 (32%)]\tloss: 1.961\tloss_D: 1.267\tloss_G: 0.6947\t\n",
      "[4/20]\t[22400/60000 (37%)]\tloss: 2.115\tloss_D: 1.336\tloss_G: 0.779\t\n",
      "[4/20]\t[25600/60000 (43%)]\tloss: 2.172\tloss_D: 1.311\tloss_G: 0.8617\t\n",
      "[4/20]\t[28800/60000 (48%)]\tloss: 2.0\tloss_D: 1.239\tloss_G: 0.7609\t\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for it, (x_real, c_digit, c_color) in enumerate(train_loader):\n",
    "\n",
    "        # batch_size for last batch might be different ...\n",
    "        batch_size = x_real.size(0)\n",
    "        real_labels = torch.full((batch_size, 1), real_label, device=device)\n",
    "        fake_labels = torch.full((batch_size, 1), fake_label, device=device)\n",
    "        \n",
    "        \n",
    "        if target_type == 'digit':\n",
    "            c_real = c_digit\n",
    "        elif target_type == 'color':\n",
    "            c_real = bin_index(c_color, num_classes)\n",
    "        else:\n",
    "            raise Exception()\n",
    "        \n",
    "        \n",
    "        ##############################################################\n",
    "        # Update Discriminator\n",
    "        ##############################################################\n",
    "\n",
    "        # a minibatch of samples from data distribution\n",
    "        x_real, c_real = x_real.to(device), c_real.to(device)\n",
    "        \n",
    "        y = D(x_real, c_real)\n",
    "        loss_D_real = criterion_D(y, real_labels)\n",
    "        \n",
    "        # a minibatch of samples from the model distribution\n",
    "        z = torch.randn(batch_size, dim_z).to(device)\n",
    "        c_fake = torch.empty(batch_size, dtype=torch.long).random_(0, num_classes).to(device)\n",
    "\n",
    "        x_fake = G(z, c_fake)\n",
    "        y = D(x_fake, c_fake)\n",
    "        loss_D_fake = criterion_D(y, fake_labels)\n",
    "        \n",
    "        # backprop\n",
    "        optimizer_D.zero_grad()\n",
    "        loss_D = loss_D_real + loss_D_fake\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        ##############################################################\n",
    "        # Update Generator/Encoder\n",
    "        ##############################################################\n",
    "        \n",
    "        \n",
    "        # a minibatch of samples from the model distribution\n",
    "        z = torch.randn(batch_size, dim_z).to(device)\n",
    "        c_fake = torch.empty(batch_size, dtype=torch.long).random_(0, num_classes).to(device)\n",
    "        x_fake = G(z, c_fake)\n",
    "        y = D(x_fake, c_fake)\n",
    "        loss_G = criterion_D(y, real_labels)\n",
    "        \n",
    "        optimizer_G.zero_grad()\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        ##############################################################\n",
    "        # print\n",
    "        ##############################################################\n",
    "\n",
    "        loss_D = loss_D.item()\n",
    "        loss_G = loss_G.item()\n",
    "        loss_total = loss_D + loss_G\n",
    "\n",
    "        global_step = epoch*len(train_loader)+it\n",
    "        writer.add_scalar('loss/total', loss_total, global_step)\n",
    "        writer.add_scalar('loss/D', loss_D, global_step)\n",
    "        writer.add_scalar('loss/G', loss_G, global_step)\n",
    "\n",
    "        if it % log_interval == log_interval-1:\n",
    "            print(f'[{epoch+1}/{n_epochs}]\\t'\n",
    "                  f'[{(it+1)*batch_size}/{len(train_loader.dataset)} ({100.*(it+1)/len(train_loader):.0f}%)]\\t'\n",
    "                  f'loss: {loss_total:.4}\\t'\n",
    "                  f'loss_D: {loss_D:.4}\\t'\n",
    "                  f'loss_G: {loss_G:.4}\\t')\n",
    "            \n",
    "            x_fake = G(fixed_z, fixed_c).detach()\n",
    "            tv_utils.save_image(x_fake,\n",
    "                os.path.join(figure_root,\n",
    "                    f'{model_name}_fake_samples_epoch={epoch}_it={it}.png'), nrow=10, normalize=True)\n",
    "\n",
    "            writer.add_image('mnist', tv_utils.make_grid(x_fake, nrow=10, normalize=True), global_step)\n",
    "        \n",
    "\n",
    "#     torch.save(G.state_dict(), os.path.join(model_root, f'G_epoch_{epoch}.pt'))\n",
    "#     torch.save(D.state_dict(), os.path.join(model_root, f'D_epoch_{epoch}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:misc_impl] *",
   "language": "python",
   "name": "conda-env-misc_impl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
