{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://github.com/pytorch/examples/blob/master/dcgan/main.py\n",
    "#\n",
    "import os\n",
    "import itertools\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision \n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = 100\n",
    "nf = 64\n",
    "nc = 1\n",
    "data_root = '../data'\n",
    "figure_root = './figures'\n",
    "model_root = './models'\n",
    "log_root = f'./logs/{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "image_size = 64\n",
    "batch_size = 64\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "n_epochs = 10\n",
    "n_batches_print = 100\n",
    "seed = 1\n",
    "load_weights_generator = ''\n",
    "load_weights_discriminator = ''\n",
    "n_workers = 8\n",
    "model_name = 'dcgan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, nz, nf, nc):\n",
    "        \"\"\"\n",
    "            nz      dimension of noise \n",
    "            nf      dimension of features in last conv layer\n",
    "            nc      number of channels in the image\n",
    "\n",
    "            In DCGAN paper for LSUN dataset, nz=100, nf=128, nc=3\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_channels, out_channels, stride=2, padding=1, batch_norm=True, nonlinearity=nn.ReLU(True)):\n",
    "            \"\"\" stride=1, padding=0: H_out = H_in + 3       # 1 -> 4\n",
    "                stride=2, padding=1: H_out = 2 * H_in       # doubles\n",
    "            \"\"\"\n",
    "            return [\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=stride, padding=padding, bias=False),\n",
    "                *( [nn.BatchNorm2d(out_channels)] if batch_norm else [] ),\n",
    "                nonlinearity,\n",
    "            ]\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # (nz)   x 1 x 1\n",
    "            *block(nz,   8*nf, stride=1, padding=0),\n",
    "            # (8*nf) x 4 x 4\n",
    "            *block(8*nf, 4*nf),\n",
    "            # (4*nf) x 8 x 8\n",
    "            *block(4*nf, 2*nf),\n",
    "            # (2*nf) x 16 x 16\n",
    "            *block(2*nf,   nf),\n",
    "            # (nf) x 32 x 32\n",
    "            *block(nf,     nc, batch_norm=False, nonlinearity=nn.Tanh()),\n",
    "            # (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "            z       (N, nz, 1, 1)\n",
    "                noise vector\n",
    "            Returns (N, nc, h, w)\n",
    "                image generated from model distribution\n",
    "                \n",
    "        \"\"\"\n",
    "        return self.model(z)\n",
    "    \n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, nc, nf):\n",
    "        \"\"\"\n",
    "            nc      number of channels in the image\n",
    "            nf      dimension of features of first conv layer\n",
    "\n",
    "            In DCGAN paper for LSUN dataset, nc=3\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        def block(in_channels, out_channels,\n",
    "                  stride=2, padding=1,\n",
    "                  batch_norm=True,\n",
    "                  nonlinearity=nn.LeakyReLU(0.2, inplace=True)):\n",
    "            \"\"\" stride=1, padding=0: H_out = H_in - 3              # 4 -> 1\n",
    "                stride=2, padding=1: H_out = floor((H_in-1)/2 +1)  # roughly halves\n",
    "            \"\"\"\n",
    "            return [\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=stride, padding=padding, bias=False),\n",
    "                *( [nn.BatchNorm2d(out_channels)] if batch_norm else [] ),\n",
    "                nonlinearity,\n",
    "            ]\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # (nc) x 64 x 64\n",
    "            *block(nc,     nf, batch_norm=False),\n",
    "            # (nf) x 32 x 32\n",
    "            *block(nf,   2*nf),\n",
    "            # (2*nf) x 16 x 16\n",
    "            *block(2*nf, 4*nf),\n",
    "            # (4*nf) x 8 x 8\n",
    "            *block(4*nf, 8*nf),\n",
    "            # (8*nf) x 4 x 4\n",
    "            *block(8*nf, 1, stride=1, padding=0, batch_norm=False, nonlinearity=nn.Sigmoid()),\n",
    "            # 1 x 1 x 1\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            x        (N, nc, h, w)\n",
    "            Returns  (N,)\n",
    "                classification probability that x comes from data distribution\n",
    "        \"\"\"\n",
    "        x = self.model(x)\n",
    "        return  x.view(-1, 1).squeeze(1)\n",
    "\n",
    "\n",
    "# custom weights initialization called on G/D\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def plot_one(x):\n",
    "    x = x.detach().cpu().numpy().transpose((1,2,0)).squeeze()\n",
    "    plt.imshow(x)\n",
    "    plt.axis('off')\n",
    "    plt.colorbar(extend='both')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.MNIST(root=data_root, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Resize(image_size),\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5,), (0.5,)),\n",
    "                   ]))\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(trainloader))\n",
    "\n",
    "plot_one(x[0][0])\n",
    "\n",
    "torch.mean(x[0]), torch.std(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "G = Generator(nz, nf, nc).to(device)\n",
    "G.apply(weights_init)\n",
    "if load_weights_generator != '':\n",
    "    G.load_state_dict(torch.load(load_weights_generator))\n",
    "    \n",
    "    \n",
    "D = Discriminator(nc, nf).to(device)\n",
    "D.apply(weights_init)\n",
    "if load_weights_discriminator != '':\n",
    "    D.load_state_dict(torch.load(load_weights_discriminator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = torch.optim.Adam(D.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(G.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "os.makedirs(data_root, exist_ok=True)\n",
    "os.makedirs(model_root, exist_ok=True)\n",
    "os.makedirs(figure_root, exist_ok=True)\n",
    "os.makedirs\n",
    "\n",
    "writer = SummaryWriter(log_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "\n",
    "    for it, (x_real, _) in enumerate(trainloader):\n",
    "\n",
    "        # batch_size for last batch might be different ...\n",
    "        batch_size = x_real.size(0)\n",
    "        real_labels = torch.full((batch_size,), real_label, device=device)\n",
    "        fake_labels = torch.full((batch_size,), fake_label, device=device)\n",
    "\n",
    "        ##############################################################\n",
    "        # Update Discriminator: Maximize E[log(D(x))] + E[log(1 - D(G(z)))]\n",
    "        ##############################################################\n",
    "\n",
    "        D.zero_grad()\n",
    "\n",
    "        # a minibatch of samples from data distribution\n",
    "        x_real = x_real.to(device)\n",
    "\n",
    "        y = D(x_real)\n",
    "        loss_D_real = criterion(y, real_labels)\n",
    "        loss_D_real.backward()\n",
    "\n",
    "        D_x = y.mean().item()\n",
    "\n",
    "        # a minibatch of samples from the model distribution\n",
    "        z = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "\n",
    "        x_fake = G(z)\n",
    "        # https://github.com/pytorch/examples/issues/116\n",
    "        # If we do not detach, then, although x_fake is not needed for gradient update of D,\n",
    "        #   as a consequence of backward pass which clears all the variables in the graph\n",
    "        #   graph for G will not be available for gradient update of G\n",
    "        # Also for performance considerations, detaching x_fake will prevent computing \n",
    "        #   gradients for parameters in G\n",
    "        y = D(x_fake.detach())\n",
    "        loss_D_fake = criterion(y, fake_labels)\n",
    "        loss_D_fake.backward()\n",
    "\n",
    "        D_G_z1 = y.mean().item()\n",
    "        loss_D = loss_D_real + loss_D_fake\n",
    "\n",
    "        optimizerD.step()\n",
    "\n",
    "        ##############################################################\n",
    "        # Update Generator: Minimize E[log(1 - D(G(z)))] => Maximize E[log(D(G(z))))]\n",
    "        ##############################################################\n",
    "\n",
    "        G.zero_grad()\n",
    "\n",
    "        y = D(x_fake)\n",
    "        loss_G = criterion(y, real_labels)\n",
    "        loss_G.backward()\n",
    "\n",
    "        D_G_z2 = y.mean().item()\n",
    "\n",
    "        optimizerG.step()\n",
    "\n",
    "        ##############################################################\n",
    "        # write/print\n",
    "        ##############################################################\n",
    "\n",
    "        if it % n_batches_print == n_batches_print-1:\n",
    "\n",
    "            # best loss: -log 4 = -1.38\n",
    "            # D_x: 1 D_G_z1: 0 D_G_z2: 1\n",
    "            print(f\"[{epoch+1}/{n_epochs}][{it+1}/{len(trainloader)}] loss: {loss_D.item()+loss_G.item():.4} loss_D: {loss_D.item():.4}  loss_G: {loss_G.item():.4} D_x: {D_x:.4} D(G(z1)): {D_G_z1:.4} D(G(z2)): {D_G_z2:.4}\" ) \n",
    "\n",
    "            x_fake = G(fixed_noise)\n",
    "            vutils.save_image(x_fake.detach(), os.path.join(figure_root, f'{model_name}_fake_samples_epoch={epoch}_it={it}.png'))\n",
    "\n",
    "\n",
    "        if it == 0:\n",
    "            global_step = epoch*len(trainloader)+it\n",
    "            writer.add_scalar('discriminator/D(x)', D_x, global_step)\n",
    "            writer.add_scalar('discriminator/D(G(z1))', D_G_z1, global_step)\n",
    "            writer.add_scalar('discriminator/D(G(z2))', D_G_z2, global_step)\n",
    "            writer.add_scalar('loss/total', loss_D.item()+loss_G.item(), global_step)\n",
    "            writer.add_scalar('loss/D', loss_D.item(), global_step)\n",
    "            writer.add_scalar('loss/G', loss_G.item(), global_step)\n",
    "            writer.add_scalar('gradient/G_conv_W_first', G.model[0].weight.grad.mean().detach().cpu().item())\n",
    "            writer.add_scalar('gradient/G_conv_W_last', G.model[-2].weight.grad.mean().detach().cpu().item())\n",
    "            writer.add_scalar('gradient/D_conv_W_first', D.model[0].weight.grad.mean().detach().cpu().item())\n",
    "            writer.add_scalar('gradient/D_conv_W_last', D.model[-2].weight.grad.mean().detach().cpu().item())\n",
    "            writer.add_image('mnist', torchvision.utils.make_grid(x_fake), global_step)\n",
    "\n",
    "    torch.save(G.state_dict(), os.path.join(model_root, f'G_epoch_{epoch}.pt'))\n",
    "    torch.save(D.state_dict(), os.path.join(model_root, f'D_epoch_{epoch}.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:misc_impl] *",
   "language": "python",
   "name": "conda-env-misc_impl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
