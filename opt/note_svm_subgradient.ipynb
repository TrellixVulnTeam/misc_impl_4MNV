{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving SVM using subgradient method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg; Pkg.activate(\".\")\n",
    "using Revise\n",
    "using PyPlot\n",
    "using LinearAlgebra\n",
    "using RDatasets\n",
    "\n",
    "# Install Dependencies\n",
    "#\n",
    "# Pkg.add(\"LIBSVM\")\n",
    "# Pkg.add(\"RDatasets\")\n",
    "# Pkg.precompile()\n",
    "\n",
    "include(\"./src/Opt.jl\")\n",
    "import .Opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "#\n",
    "iris = dataset(\"datasets\", \"iris\")\n",
    "\n",
    "categories = Dict(\n",
    "    \"setosa\"     => 1,\n",
    "    \"versicolor\" => -1,\n",
    "    \"virginica\"  => 1\n",
    ")\n",
    "\n",
    "Y = convert(Vector, iris[:Species])\n",
    "X = convert(Array, iris[:, 1:4])\n",
    "X = [X ones(size(X,1),1)]\n",
    "\n",
    "# Binary Classification\n",
    "is = findall(y -> y!=\"setosa\", Y)\n",
    "X, Y = X[is,:], Y[is]\n",
    "Y = map((x -> categories[x]), Y)\n",
    "\n",
    "X_train, X_test = X[1:2:end,:], X[2:2:end,:]\n",
    "Y_train, Y_test = Y[1:2:end], Y[2:2:end,:]\n",
    "\n",
    "size(X_train), size(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function svm(X, Y, λ)\n",
    "    \n",
    "    function f(x)\n",
    "        0.5norm(x[1:end-1])^2 + λ*sum(max.(0, 1 .- Y.*(X*x)))\n",
    "    end\n",
    "\n",
    "    function grad!(g, x)\n",
    "        is = findall(z -> z>0, 1 .- Y.*(X*x))\n",
    "        g[1:end-1] .= x[1:end-1] .- λ*dropdims(sum(Y[is].*X[is,1:end-1], dims=1), dims=1)\n",
    "        g[end:end] .= -λ*sum(Y[is], dims=1)\n",
    "    end\n",
    "    \n",
    "    return f, grad!\n",
    "end\n",
    "\n",
    "\n",
    "# coefficient in front of risk term\n",
    "λ = 1\n",
    "f, grad! = svm(X_train,Y_train,λ)\n",
    "\n",
    "n = 4\n",
    "w = rand(n + 1)\n",
    "g = similar(w)\n",
    "grad!(g, w)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = size(X,2)-1\n",
    "x0 = rand(n+1)\n",
    "α = 0.001\n",
    "n_iterations = 1000\n",
    "\n",
    "fs = zeros(n_iterations)\n",
    "grad_norm = zeros(n_iterations)\n",
    "\n",
    "function access_state(state)\n",
    "    fs[state.k] = state.f\n",
    "    grad_norm[state.k] = norm(state.g)\n",
    "    if mod(state.k, 100) == 0\n",
    "        ŷ = map(y -> (y > 0) ? 1 : -1, X_test*state.x)\n",
    "        println(\"(k=$(state.k)) \\t misclassify: $(sum(Y_test .!= ŷ)) / $(size(Y_test,1))\")\n",
    "    end\n",
    "end\n",
    "\n",
    "Opt.gradient_descent(x0, f, grad!;\n",
    "    α=α, n_iterations = n_iterations, access_state=access_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(5,4))\n",
    "ax = subplot()\n",
    "ax.spines[\"right\"].set_visible(false)\n",
    "ax.spines[\"top\"].set_visible(false)\n",
    "plot(1:n_iterations, fs)\n",
    "ylabel(\"ℓ\")\n",
    "xlabel(\"k\")\n",
    "title(\"SVM on Iris dataset\")\n",
    "savefig(\"summary/assets/svm_subgradient.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
