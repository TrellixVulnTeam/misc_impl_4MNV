
@article{odonoghueAdaptiveRestartAccelerated2012,
	title = {Adaptive Restart for Accelerated Gradient Schemes},
	url = {http://arxiv.org/abs/1204.3982},
	abstract = {In this paper we demonstrate a simple heuristic adaptive restart technique that can dramatically improve the convergence rate of accelerated gradient schemes. The analysis of the technique relies on the observation that these schemes exhibit two modes of behavior depending on how much momentum is applied. In what we refer to as the 'high momentum' regime the iterates generated by an accelerated gradient scheme exhibit a periodic behavior, where the period is proportional to the square root of the local condition number of the objective function. This suggests a restart technique whereby we reset the momentum whenever we observe periodic behavior. We provide analysis to show that in many cases adaptively restarting allows us to recover the optimal rate of convergence with no prior knowledge of function parameters.},
	journaltitle = {{arXiv}:1204.3982 [math]},
	author = {O'Donoghue, Brendan and Candes, Emmanuel},
	urldate = {2020-03-23},
	date = {2012-04-18},
	eprinttype = {arxiv},
	eprint = {1204.3982},
	file = {O'Donoghue_Candes_2012_Adaptive Restart for Accelerated Gradient Schemes.pdf:/Users/wpq/Dropbox (MIT)/zotero/O'Donoghue_Candes_2012_Adaptive Restart for Accelerated Gradient Schemes.pdf:application/pdf}
}

@article{allen-zhuLinearCouplingUltimate2016,
	title = {Linear Coupling: An Ultimate Unification of Gradient and Mirror Descent},
	url = {http://arxiv.org/abs/1407.1537},
	shorttitle = {Linear Coupling},
	abstract = {First-order methods play a central role in large-scale machine learning. Even though many variations exist, each suited to a particular problem, almost all such methods fundamentally rely on two types of algorithmic steps: gradient descent, which yields primal progress, and mirror descent, which yields dual progress. We observe that the performances of gradient and mirror descent are complementary, so that faster algorithms can be designed by {LINEARLY} {COUPLING} the two. We show how to reconstruct Nesterov's accelerated gradient methods using linear coupling, which gives a cleaner interpretation than Nesterov's original proofs. We also discuss the power of linear coupling by extending it to many other settings that Nesterov's methods cannot apply to.},
	journaltitle = {{arXiv}:1407.1537 [cs, math, stat]},
	author = {Allen-Zhu, Zeyuan and Orecchia, Lorenzo},
	urldate = {2020-03-24},
	date = {2016-11-07},
	eprinttype = {arxiv},
	eprint = {1407.1537},
	file = {arXiv Fulltext PDF:/Users/wpq/Zotero/storage/H3P8NQUJ/Allen-Zhu and Orecchia - 2016 - Linear Coupling An Ultimate Unification of Gradie.pdf:application/pdf}
}

@article{barzilaiTwoPointStepSize1988,
	title = {Two-Point Step Size Gradient Methods},
	volume = {8},
	issn = {0272-4979},
	url = {https://academic.oup.com/imajna/article/8/1/141/802460},
	doi = {10.1093/imanum/8.1.141},
	abstract = {Abstract.  We derive two-point step sizes for the steepest-descent method by approximating the secant equation. At the cost of storage of an extra iterate and g},
	pages = {141--148},
	number = {1},
	journaltitle = {{IMA} Journal of Numerical Analysis},
	shortjournal = {{IMA} J Numer Anal},
	author = {Barzilai, Jonathan and Borwein, Jonathan M.},
	urldate = {2020-03-25},
	date = {1988-01-01},
	langid = {english},
	note = {Publisher: Oxford Academic},
	file = {Barzilai_Borwein_1988_Two-Point Step Size Gradient Methods.pdf:/Users/wpq/Dropbox (MIT)/zotero/Barzilai_Borwein_1988_Two-Point Step Size Gradient Methods.pdf:application/pdf}
}