
@article{odonoghueAdaptiveRestartAccelerated2012,
	title = {Adaptive Restart for Accelerated Gradient Schemes},
	url = {http://arxiv.org/abs/1204.3982},
	abstract = {In this paper we demonstrate a simple heuristic adaptive restart technique that can dramatically improve the convergence rate of accelerated gradient schemes. The analysis of the technique relies on the observation that these schemes exhibit two modes of behavior depending on how much momentum is applied. In what we refer to as the 'high momentum' regime the iterates generated by an accelerated gradient scheme exhibit a periodic behavior, where the period is proportional to the square root of the local condition number of the objective function. This suggests a restart technique whereby we reset the momentum whenever we observe periodic behavior. We provide analysis to show that in many cases adaptively restarting allows us to recover the optimal rate of convergence with no prior knowledge of function parameters.},
	journaltitle = {{arXiv}:1204.3982 [math]},
	author = {O'Donoghue, Brendan and Candes, Emmanuel},
	urldate = {2020-03-23},
	date = {2012-04-18},
	eprinttype = {arxiv},
	eprint = {1204.3982},
	file = {O'Donoghue_Candes_2012_Adaptive Restart for Accelerated Gradient Schemes.pdf:/Users/wpq/Dropbox (MIT)/zotero/O'Donoghue_Candes_2012_Adaptive Restart for Accelerated Gradient Schemes.pdf:application/pdf}
}

@article{allen-zhuLinearCouplingUltimate2016,
	title = {Linear Coupling: An Ultimate Unification of Gradient and Mirror Descent},
	url = {http://arxiv.org/abs/1407.1537},
	shorttitle = {Linear Coupling},
	abstract = {First-order methods play a central role in large-scale machine learning. Even though many variations exist, each suited to a particular problem, almost all such methods fundamentally rely on two types of algorithmic steps: gradient descent, which yields primal progress, and mirror descent, which yields dual progress. We observe that the performances of gradient and mirror descent are complementary, so that faster algorithms can be designed by {LINEARLY} {COUPLING} the two. We show how to reconstruct Nesterov's accelerated gradient methods using linear coupling, which gives a cleaner interpretation than Nesterov's original proofs. We also discuss the power of linear coupling by extending it to many other settings that Nesterov's methods cannot apply to.},
	journaltitle = {{arXiv}:1407.1537 [cs, math, stat]},
	author = {Allen-Zhu, Zeyuan and Orecchia, Lorenzo},
	urldate = {2020-03-24},
	date = {2016-11-07},
	eprinttype = {arxiv},
	eprint = {1407.1537},
	file = {arXiv Fulltext PDF:/Users/wpq/Zotero/storage/H3P8NQUJ/Allen-Zhu and Orecchia - 2016 - Linear Coupling An Ultimate Unification of Gradie.pdf:application/pdf}
}

@article{barzilaiTwoPointStepSize1988,
	title = {Two-Point Step Size Gradient Methods},
	volume = {8},
	issn = {0272-4979},
	url = {https://academic.oup.com/imajna/article/8/1/141/802460},
	doi = {10.1093/imanum/8.1.141},
	abstract = {Abstract.  We derive two-point step sizes for the steepest-descent method by approximating the secant equation. At the cost of storage of an extra iterate and g},
	pages = {141--148},
	number = {1},
	journaltitle = {{IMA} Journal of Numerical Analysis},
	shortjournal = {{IMA} J Numer Anal},
	author = {Barzilai, Jonathan and Borwein, Jonathan M.},
	urldate = {2020-03-25},
	date = {1988-01-01},
	langid = {english},
	note = {Publisher: Oxford Academic},
	file = {Barzilai_Borwein_1988_Two-Point Step Size Gradient Methods.pdf:/Users/wpq/Dropbox (MIT)/zotero/Barzilai_Borwein_1988_Two-Point Step Size Gradient Methods.pdf:application/pdf}
}

@article{nesterovMethodSolvingConvex1983,
	title = {A method of solving a convex programming problem with convergence rate O(1/k2).},
	volume = {27},
	pages = {372--376},
	issue = {(2)},
	journaltitle = {Soviet Mathematics Doklady},
	author = {Nesterov, Yurii},
	date = {1983},
	file = {Nesterov - 1983 - A method of solving a convex programming problem w:/Users/wpq/Dropbox (MIT)/zotero/Nesterov - 1983 - A method of solving a convex programming problem w:application/pdf}
}

@book{nesterovIntroductoryLecturesConvex2004,
	title = {Introductory lectures on convex optimization : a basic course},
	isbn = {978-1-4020-7553-7},
	url = {https://dial.uclouvain.be/pr/boreal/object/boreal:116858},
	shorttitle = {Introductory lectures on convex optimization},
	author = {Nesterov, Yurii},
	urldate = {2020-03-27},
	date = {2004},
	langid = {english}
}

@article{beckFastIterativeShrinkageThresholding2009a,
	title = {A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems},
	volume = {2},
	issn = {1936-4954},
	url = {http://epubs.siam.org/doi/10.1137/080716542},
	doi = {10.1137/080716542},
	abstract = {We consider the class of iterative shrinkage-thresholding algorithms ({ISTA}) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an extension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm ({FISTA}) which preserves the computational simplicity of {ISTA} but with a global rate of convergence which is proven to be signiÔ¨Åcantly better, both theoretically and practically. Initial promising numerical results for wavelet-based image deblurring demonstrate the capabilities of {FISTA} which is shown to be faster than {ISTA} by several orders of magnitude.},
	pages = {183--202},
	number = {1},
	journaltitle = {{SIAM} Journal on Imaging Sciences},
	shortjournal = {{SIAM} J. Imaging Sci.},
	author = {Beck, Amir and Teboulle, Marc},
	urldate = {2020-03-27},
	date = {2009-01},
	langid = {english},
	file = {Beck and Teboulle - 2009 - A Fast Iterative Shrinkage-Thresholding Algorithm .pdf:/Users/wpq/Zotero/storage/FQM6T9FT/Beck and Teboulle - 2009 - A Fast Iterative Shrinkage-Thresholding Algorithm .pdf:application/pdf}
}

@article{suDifferentialEquationModeling2015,
	title = {A Differential Equation for Modeling Nesterov's Accelerated Gradient Method: Theory and Insights},
	url = {http://arxiv.org/abs/1503.01243},
	shorttitle = {A Differential Equation for Modeling Nesterov's Accelerated Gradient Method},
	abstract = {We derive a second-order ordinary differential equation ({ODE}) which is the limit of Nesterov's accelerated gradient method. This {ODE} exhibits approximate equivalence to Nesterov's scheme and thus can serve as a tool for analysis. We show that the continuous time {ODE} allows for a better understanding of Nesterov's scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The {ODE} interpretation also suggests restarting Nesterov's scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.},
	journaltitle = {{arXiv}:1503.01243 [math, stat]},
	author = {Su, Weijie and Boyd, Stephen and Candes, Emmanuel J.},
	urldate = {2020-03-28},
	date = {2015-10-27},
	eprinttype = {arxiv},
	eprint = {1503.01243},
	file = {Su et al_2015_A Differential Equation for Modeling Nesterov's Accelerated Gradient Method - Theory and Insights.pdf:/Users/wpq/Dropbox (MIT)/zotero/Su et al_2015_A Differential Equation for Modeling Nesterov's Accelerated Gradient Method - Theory and Insights.pdf:application/pdf}
}