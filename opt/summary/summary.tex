\documentclass[11pt]{article}
\input{\string~/.macros}
\usepackage{mathrsfs}
\usepackage{bbm}
\usepackage[a4paper, total={7in, 9in}]{geometry}
\usepackage{graphicx}
\graphicspath{{./assets/}}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linktoc=all, linkcolor=blue, citecolor=red}
\usepackage[backend=bibtex,sorting=none]{biblatex}
\addbibresource{opt_convex.bib}


\newcommand\ry{\ensuremath{\mathsf{y}}}
\newcommand\rx{\ensuremath{\mathsf{x}}}
\newcommand\rb{\ensuremath{\mathsf{b}}}
\newcommand\rc{\ensuremath{\mathsf{c}}}
\newcommand\rz{\ensuremath{\mathsf{z}}}
\newcommand\ru{\ensuremath{\mathsf{u}}}


\newcommand\rbx{\ensuremath{\mathsf{\mathbf{x}}}}
\newcommand\rby{\ensuremath{\mathsf{\mathbf{y}}}}
\newcommand\rbu{\ensuremath{\mathsf{\mathbf{u}}}}

\renewcommand\bmu{\ensuremath{\boldsymbol{\mu}}}
\newcommand\bSigma{\ensuremath{\boldsymbol{\Sigma}}}

\newcommand\scrF{\ensuremath{\mathscr{F}}}
\newcommand\scrS{\ensuremath{\mathscr{S}}}



\begin{document} 


\section{Smooth Convex Optimization}


We are interested in unconstrained minimization of convex and smooth $f:\R^n \to \R$ given first order oracle
\[
    \text{minimize}_{x\in\R^n}\; f(x)
\]
We may impose additional assumption on $f$, i.e. $L$-lipschitz, $\mu$-strongly convex

\subsection{Gradient Descent}

Gradient descent achieves sublinear convergence $\sO(\frac{1}{\epsilon})$ for $f\in \scrF_{L}^1$ and $\sO(\log \frac{1}{\epsilon})$ for $f\in\scrS_{L,\mu}^1$. 
\begin{align*}
    x^{k+1} = x^k - \alpha_k \nabla f(x^k)
\end{align*}
for some stepsize $\alpha_k \geq 0$. Note $\alpha_k = \frac{1}{L}$ is the optimal stepsize. 

\subsection{Gradient Descent with Barzilai \& Borwein Stepsize}

Barzilai \& Borwein stepsize relaxes the constraint on monotonic descent \cite{barzilaiTwoPointStepSize1988}. The idea is to choose $\alpha_k$ such that $\alpha_k g^k$ approximates the Newton update.
\begin{align*}
    \alpha_k = \frac{\inner{u^k}{v^k}}{\norm{v^k}^2}
    \quad\quad\text{or}\quad\quad
    \alpha_k = \frac{\norm{u^k}^2}{\inner{u^k}{v^k}}
\end{align*}
where 
\[
    u^k = x^k - x^{k-1}
    \quad\quad
    v^k = \nabla f(x^k) - \nabla f(x^{k-1})
\]
This algorithm enjoys fast empirical convergence.

\subsection{Nesterov's Accelerated Gradient}

Nesterov's accelerated gradient achieves lower bound for minimization of function $f\in \scrS_{L,\mu}^1$ and improves the rate for gradient descent from $\sO(\kappa\log \frac{1}{\epsilon})$ to $\sO(\sqrt{\kappa}\log \frac{1}{\epsilon})$. Similarly, acceleration improves convergence rate for function $ f\in\scrF_{L}^1$ from $\sO(\frac{1}{\epsilon})$ to $\sO(\frac{1}{\sqrt{\epsilon}})$.

\subsubsection{Intuition}

The following comes from Nesterov's book \cite{nesterovIntroductoryLecturesConvex2004} and \href{https://nisheethvishnoi.files.wordpress.com/2018/05/lecture52.pdf}{lecture note}.
\begin{definition*}
    A pair of sequences $(\pc{\phi_k(x)}_{k=0}^{\infty}, \pc{\lambda_k}_{k=0}^{\infty})$ where $\lambda_k \geq 0$ are called the estimating sequences of the function $f(\cdot)$ if 
    \begin{enumerate}
        \item $\lambda_k \to 0$ and
        \item (\textbf{lower bound}) for any $x\in\R^n$ and for all $k\geq 0$, $\phi_k(x) \leq (1-\lambda_k)f(x) + \lambda_k \phi_0(x)$ 
    \end{enumerate}
\end{definition*}
\noindent In addition, If we can find some sequence of points $\pc{x^k}_{k=0}^{\infty}$ such that
\begin{enumerate}
    \setcounter{enumi}{2}
    \item \textit{\textbf{(upper bound)} for any $x\in\R^n$, $f(x^k) \leq \phi_k(x)$}
\end{enumerate}
then the rate of convergence can be derived from convergence rate of $\lambda_k$, i.e. 
\[
    f(x^k) - f^* \leq \lambda_k \pc{ \phi_0^* - f^* } \to 0
\]
where $\phi_k^* = \min_{x\in\R^n} \phi_k(x)$. Intuitively, $\phi_k(\cdot)$ are approximations for $f(\cdot)$, providing tighter and tighter bound on the optimality gap $f(x^k) - f^*$ as $\lambda_k \to 0$. In addition, from (2) and (3), we have that the sequence $\pc{x^k}$ converges to the minimizer of $f$. 
\[
    f(x^k) \leq \phi_k(x^*) \leq f(x^*)   
\]
In \cite{nesterovIntroductoryLecturesConvex2004}, Nesterov showed that for $f\in\scrS_{\mu,L}^1$, we can construct estimating sequences for $f$ recursively
\begin{align*}
    \lambda_{k+1}
        &= (1-\alpha_k) \lambda_k \\
    \phi_{k+1}(x)
        &= (1-\alpha_k) \phi_k(x) + \alpha_k L_k(x) \\
        &\quad \text{where} \quad 
        L_k(x) = f(y^k) + \inner{\nabla f(y^k)}{x - y^k} + \frac{\mu}{2} \norm{x-y^k}^2
\end{align*}
where $\pc{y^k}_{k=0}^{\infty}$ is an arbitrary sequence of points, coefficients $\pc{\alpha_k}_{k=0}^{\infty}$ satisfy $\alpha_k \in (0,1)$ and $\sum_k \alpha_k = \infty$ with $\lambda_0 = 1$ and that $\phi_0(\cdot)$ is an arbitrary convex function. Note that $\phi_k$ is simply a convex combination of the previous approximate $\phi_{k-1}$ and a quadratic lower bound $L_{k-1}$ on $f$, at some carefully chosen point $y^{k-1}$. If we let $\phi_0(x) = \phi_0^* + \frac{\gamma_0}{2} \norm{x-v_0}^2$ be a quadratic function, then $\phi_k(\cdot)$ has a convenient closed form expression
\[
    \phi_k(x)
        = \phi_k^* + \frac{\gamma_k}{2} \norm{x-v_k}^2    
\]
where $\pc{\gamma_k}, \pc{v_k}, \pc{\phi_k^*}$ follow certain recurrence relation detailed in \cite{nesterovIntroductoryLecturesConvex2004}. Additional constraint needs to be satisfied to ensure (3) holds. 
\begin{enumerate}
    \item For (3) to hold, it must be that $f(y^k) - \frac{1}{2L} \norm{\nabla f(y^k)}^2 \geq f(x^{k+1})$, which can be achieved if we obtain $x^{k+1}$ by taking a gradient step $x^{k+1} = y^k - \frac{1}{L} \nabla f(y^k)$ at $y^k$ and apply descent lemma.
    \item To apply the previous, we need the coefficient before $\norm{\nabla f(y^k)}^2$ to agree, i.e. want $\alpha_k$ such that $L\alpha_k^2 = (a-\alpha_k) \gamma_k + \alpha_k \mu$.
    \item Choose $y^k$ accordingly to ensure (3) holds
\end{enumerate}
By making these constraints invariant to iterative updates, we arrive at the accelerated gradient methods. In addition to the algebra tricks, there are efforts that tries to interpret what Nesterov's method is doing under the hoold. For example, \cite{allen-zhuLinearCouplingUltimate2016} interpreted Nesterov's accelerated method as a linear coupling of gradient descent and mirror descent . \cite{suDifferentialEquationModeling2015} showed that in the limit of small stepsizes (when taking the gradient step to obtain $x^{k+1}$) is equivalent to the dynamics of some continous second-order ODE.


\subsubsection{The Algorithm}

There are several equivalent algorithm for Nesterov's Accelerated Gradient Method. The following came from the original paper by Nesterov in 1983 \cite{nesterovMethodSolvingConvex1983} and later adapted to LASSO \cite{beckFastIterativeShrinkageThresholding2009a}. Assume $f\in \scrF_{L}^1$. Given $t_1 = 1$ and $y_1 = x_0$, accelerated gradient updates according to
\begin{align*}
    x^{k+1}
        &= y^k - \frac{1}{L} \nabla f(y^k) \\
    t_{k+1} 
        &= \frac{1 + \sqrt{1 + 4t_k^2} }{2} \\
    y^{k+1}
        &= x^{k+1} + \frac{t_k - 1}{t_{k+1}} \left( x^{k+1} - x^{k} \right)
\end{align*}
We can simplify the expression by noting that (\href{http://www.princeton.edu/~yc5/ele522_optimization/lectures/accelerated_gradient.pdf}{slides})
\[
        = \frac{t_k - 1}{t_{k+1}} 
        = 1 - \frac{3}{k} + o(\frac{1}{k})
        = \frac{k-3}{k} + o(\frac{1}{k})
\]
The momentum coefficient is asymptotically equivalent to $\frac{k-1}{k+2}$ ($\frac{t_1-1}{t_{2}} = 0$)
\fig{asymptotic_behavior_of_momentum_term}{4in}
And updates is now given by
\begin{align*}  
    x^{k+1} 
        &= y^k - \frac{1}{L} \nabla f(y^k) \\
    y^{k+1}
        &= x^{k+1} + \frac{k-1}{k+2} \left( x^{k+1} - x^k \right)
\end{align*}
Another formulation of the algorithm comes from Nesterov's textbook \cite{nesterovIntroductoryLecturesConvex2004}. If we take a constant step, i.e. $\frac{1}{L}$, to find the $x^{k+1}$, and that we pick $\alpha_0 = \sqrt{\frac{\mu}{L}} = 1/\sqrt{\kappa}$, which is the interpolating coefficient for recursive construction of the estimating sequence. Then we have the follwing updates
\begin{align*}
    x^{k+1} 
        &=  y^k - \frac{1}{L} \nabla f(y^k) \\
    y^{k+1}
        &= x^{k+1} + \frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1} (x^{k+1} - x^k)
\end{align*}
However, in practice the condition number $\kappa$ is hard to compute.


\subsection{Numerical Experiments}

We are given a hard least squares problem of minimizing $f(x) = \frac{1}{2}\norm{D^Tx-b}_2^2$ where $D\in\R^{n\times (n+1)}$ is the differencing matrix, with all -1 on the main diagonal and all 1 on the superdiagonal. The gradient is given by $\nabla f(x) = D(D^Tx-b)$. We compare gradient descent with either constant stepsize or using barzilai borwein stepsize, and nesterov's accelerated gradient descent.
\fig{compare_gd_10000}{7in}
We see that the barzilai borwein stepsize is the fastest method, followed by nesterov's accelerated gradient, then the naive gradient descent method.

\section{Nonsmooth Convex Optimization}

We are interested in unconstrained minimization of convex, possibly nondifferentiable, $f:\R^n \to \R$ 
\[
    \text{minimize}_{x\in\R^n}\; f(x)
\]
given first order oracle

\subsection{Subgradient Method}

Given bounded subgradient $\norm{g^k}\leq G$ and bounded domain $\norm{x^0 - x^*} \leq R$, subgradient method is in a sense optimal as it achieves the lower bound $\sO(\frac{1}{\epsilon^2})$ for this problem class. Subgradient method iteratively updates as follows
\[
    x^{k+1}
        = x^k - \alpha_k g^k    
\]
where $g^k \in\partial f(x^k)$ is \textit{any} subgradient of $f$. First order optimality condition is now $0\in\partial f(x^*)$, which is impossible to test for nontrivial function $f$. Therefore, using $\norm{g^k}\leq \epsilon$ is not informative and subgradient method does not really have a stopping criterion.

\subsubsection{Solving Support Vector Machine w/ Subgradient Method}

We are given data $\sD = \pc{(x_i,y_i) \mid x_i\in\R^n \; y_i \in \pc{\pm 1}}$, support vector machine is supervised learning model that tries to find $w\in\R^n$ and $b\in\R$ such that the empirical risk and regularizer on $w$ is minimized
\begin{align*}
    \text{minimize}_{w,b} \quad
        \frac{1}{2} \norm{w}_2^2 + C \sum_{i=1}^m \max\pb{0, 1 - y_i(w^Tx_i + b)} 
        \quad \left( := f(w,b) \right)
\end{align*}
Support vector machines can be solved using subgradient method. We first find a subgradient of $f$
\begin{align*}
    g_w
        &= w - C\sum_{i=[m]: y_i(w^Tx_i + b) < 1 } y_i x_i \\
    g_b
        &= -C\sum_{i=[m]: y_i(w^Tx_i + b) < 1 } y_i
\end{align*}
where we haved picked $0\in\partial (\max{0, 1- y_i(w^Tx_i + b)})$ when $y_i(w^Tx_i + b) = 1$, the only case where the \textit{max term} is non-differentiable. When tested on the Iris dataset, subgradient method worked!
\fig{svm_subgradient}{4in}

\printbibliography 

\end{document}