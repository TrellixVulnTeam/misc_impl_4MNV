{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Inference https://pyro.ai/examples/intro_part_ii.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import math\n",
    "\n",
    "import pyro\n",
    "import pyro.infer\n",
    "import pyro.optim\n",
    "import pyro.distributions as dist\n",
    "import pyro.distributions.transforms as T\n",
    "\n",
    "from torch.distributions import constraints\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "\n",
    "\n",
    "pyro.set_rng_seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "def scale(guess):\n",
    "    weight = pyro.sample(\"weight\", dist.Normal(guess, 1.0))\n",
    "    return pyro.sample(\"measurement\", dist.Normal(weight, 0.75))\n",
    "# Conditioned Model\n",
    "conditioned_scale = pyro.condition(scale, data={\"measurement\": 9.5})\n",
    "# Guide, (approx) posterior distribution \n",
    "def perfect_guide(guess):\n",
    "    loc =(0.75**2 * guess + 9.5) / (1 + 0.75**2) # 9.14\n",
    "    scale = np.sqrt(0.75**2/(1 + 0.75**2)) # 0.6\n",
    "    return pyro.sample(\"weight\", dist.Normal(loc, scale))\n",
    "# Guide from a Gaussian family \n",
    "def scale_parametrized_guide(guess):\n",
    "    a = pyro.param(\"a\", torch.tensor(guess))\n",
    "    b = pyro.param(\"b\", torch.tensor(1.), constraint=constraints.positive)\n",
    "    return pyro.sample(\"weight\", dist.Normal(a, b))\n",
    "# SVI\n",
    "guess = 8.5\n",
    "pyro.clear_param_store()\n",
    "svi = pyro.infer.SVI(model=conditioned_scale,\n",
    "                     guide=scale_parametrized_guide,\n",
    "                     optim=pyro.optim.SGD({\"lr\": 0.001, \"momentum\":0.1}),\n",
    "                     loss=pyro.infer.Trace_ELBO())\n",
    "\n",
    "losses, a,b  = [], [], []\n",
    "num_steps = 2500\n",
    "for t in range(num_steps):\n",
    "    losses.append(svi.step(guess))\n",
    "    a.append(pyro.param(\"a\").item())\n",
    "    b.append(pyro.param(\"b\").item())\n",
    "    \n",
    "plt.plot(losses)\n",
    "plt.title(\"ELBO\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('a = ',pyro.param(\"a\").item())\n",
    "print('b = ', pyro.param(\"b\").item())\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot([0,num_steps],[9.14,9.14], 'k:')\n",
    "plt.plot(a)\n",
    "plt.ylabel('a')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.ylabel('b')\n",
    "plt.plot([0,num_steps],[0.6,0.6], 'k:')\n",
    "plt.plot(b)\n",
    "plt.tight_layout()\n",
    "\n",
    "# close to true value ùñ≠ùóàùóãùóÜùñ∫ùóÖ(a=9.14,b=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVI Part I https://pyro.ai/examples/svi_part_i.html\n",
    "\n",
    "+ guide\n",
    "    + parameterized variational distribution q(z)\n",
    "    + need to have matching latent variable using `pyro.sample` without `obs` argument\n",
    "+ SVI\n",
    "    + maximization of evidence lower bound ... minimize `KL(q(z)||p(z|x))`\n",
    "    + model/guide has same signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fair Coin\n",
    "    \n",
    "\n",
    "# create some data with 6 observed heads and 4 observed tails\n",
    "data = []\n",
    "for _ in range(6):\n",
    "    data.append(torch.tensor(1.0))\n",
    "for _ in range(4):\n",
    "    data.append(torch.tensor(0.0))\n",
    "\n",
    "def model(data):\n",
    "    # define the hyperparameters that control the beta prior\n",
    "    alpha0 = torch.tensor(10.0)\n",
    "    beta0 = torch.tensor(10.0)\n",
    "    # sample f from the beta prior\n",
    "    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
    "    # loop over the observed data\n",
    "    for i in range(len(data)):\n",
    "        # observe datapoint i using the bernoulli\n",
    "        # likelihood Bernoulli(f)\n",
    "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n",
    "        \n",
    "def guide(data):\n",
    "    # register the two variational parameters with Pyro.\n",
    "    alpha_q = pyro.param(\"alpha_q\", torch.tensor(15.0),\n",
    "                         constraint=constraints.positive)\n",
    "    beta_q = pyro.param(\"beta_q\", torch.tensor(15.0),\n",
    "                        constraint=constraints.positive)\n",
    "    # sample latent_fairness from the distribution Beta(alpha_q, beta_q)\n",
    "    pyro.sample(\"latent_fairness\", dist.Beta(alpha_q, beta_q))\n",
    "\n",
    "\n",
    "# set up the optimizer\n",
    "adam_params = {\"lr\": 0.0005, \"betas\": (0.90, 0.999)}\n",
    "optimizer = Adam(adam_params)\n",
    "\n",
    "# clear the param store in case we're in a REPL\n",
    "pyro.clear_param_store()\n",
    "# setup the inference algorithm\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "# Do Gradient Step\n",
    "losses, alpha,beta  = [], [], []\n",
    "num_steps = 5000\n",
    "for t in range(num_steps):\n",
    "    losses.append(svi.step(data))\n",
    "    alpha.append(pyro.param(\"alpha_q\").item())\n",
    "    beta.append(pyro.param(\"beta_q\").item())\n",
    "    \n",
    "plt.plot(losses)\n",
    "plt.title(\"ELBO\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"loss\");\n",
    "\n",
    "# grab the learned variational parameters\n",
    "alpha_q = pyro.param(\"alpha_q\").item()\n",
    "beta_q = pyro.param(\"beta_q\").item()\n",
    "\n",
    "# here we use some facts about the beta distribution\n",
    "# compute the inferred mean of the coin's fairness\n",
    "inferred_mean = alpha_q / (alpha_q + beta_q)\n",
    "# compute inferred standard deviation\n",
    "factor = beta_q / (alpha_q * (1.0 + alpha_q + beta_q))\n",
    "inferred_std = inferred_mean * math.sqrt(factor)\n",
    "\n",
    "print(\"\\nbased on the data and our prior belief, the fairness \" +\n",
    "      \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std))\n",
    "print(\"Exact Posterior Mean is 16/30 = 0.53\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVI Part II https://pyro.ai/examples/svi_part_ii.html\n",
    "\n",
    "+ scaling to large dataset\n",
    "    + conditional independence of observation given latents (i.i.d. samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Plate\n",
    "def model_sequential_plate(data):\n",
    "    # sample f from the beta prior\n",
    "    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
    "    # loop over the observed data using pyro.sample with the obs keyword argument\n",
    "    # conditional independence given latent_fairness because latent_fairness\n",
    "    #     is sampled outside of the context of data_loop\n",
    "    # Note conditional independence is enforced via context!\n",
    "    for i in pyro.plate(\"data_loop\", len(data)):\n",
    "        # observe datapoint i using the bernoulli likelihood\n",
    "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n",
    "        \n",
    "# Vectorization\n",
    "data = torch.zeros(10)\n",
    "data[0:6] = torch.ones(6)  # 6 heads and 4 tails\n",
    "def model_vectorized_plate(data):\n",
    "    # sample f from the beta prior\n",
    "    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
    "    # vectorized plate\n",
    "    with plate('observed_data'):\n",
    "        pyro.sample('obs', dist.Bernoulli(f), obs=data)\n",
    "        \n",
    "# Subsampling\n",
    "# Evaluate log-likelihood for 5 randomly chosen datapoint in data\n",
    "# And log-likelihood scaled by factor of 10/5 = 2\n",
    "\n",
    "\"\"\"\n",
    "for i in pyro.plate(\"data_loop\", len(data), subsample_size=5):\n",
    "    pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n",
    "# OR\n",
    "with plate('observe_data', size=10, subsample_size=5) as ind:\n",
    "    pyro.sample('obs', dist.Bernoulli(f),\n",
    "                obs=data.index_select(0, ind))\n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "# Subsampling w/ both global and local random variables\n",
    "\"\"\"\n",
    "def model(data):\n",
    "    beta = pyro.sample(\"beta\", ...) # sample the global RV\n",
    "    for i in pyro.plate(\"locals\", len(data)):\n",
    "        z_i = pyro.sample(\"z_{}\".format(i), ...)\n",
    "        # compute the parameter used to define the observation\n",
    "        # likelihood using the local random variable\n",
    "        theta_i = compute_something(z_i)\n",
    "        pyro.sample(\"obs_{}\".format(i), dist.MyDist(theta_i), obs=data[i])\n",
    "    \n",
    "def guide(data):\n",
    "    beta = pyro.sample(\"beta\", ...) # sample the global RV\n",
    "    for i in pyro.plate(\"locals\", len(data), subsample_size=5):\n",
    "        # sample the local RVs\n",
    "        # Here lambda_i is a local variational parameter\n",
    "        pyro.sample(\"z_{}\".format(i), ..., lambda_i)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Amortization\n",
    "#     - learn parametric function f() to predict lambda (used by VAE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVI Part III https://pyro.ai/examples/svi_part_iii.html\n",
    "\n",
    "+ ELBO gradient estimator\n",
    "\n",
    "## Tensor Shapes in Pyro https://pyro.ai/examples/tensor_shapes.html\n",
    "\n",
    "+ distribution \n",
    "    + `batch_shape` same as `log_prob(x).shape`\n",
    "    + `event_shape` shape of a single sample without batching\n",
    "    + `expand` return new distribution instance with batch dimension expanded ... does not allocate new mem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import Bernoulli, Categorical, MultivariateNormal, Normal\n",
    "from pyro.distributions.util import broadcast_shape\n",
    "from pyro.infer import Trace_ELBO, TraceEnum_ELBO, config_enumerate\n",
    "import pyro.poutine as poutine\n",
    "\n",
    "\n",
    "pyro.enable_validation(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Bernoulli(0.5)\n",
    "assert d.batch_shape == ()\n",
    "assert d.event_shape == ()\n",
    "x = d.sample()\n",
    "assert x.shape == ()\n",
    "assert d.log_prob(x).shape == ()\n",
    "\n",
    "d = Bernoulli(0.5 * torch.ones(3,4))\n",
    "assert d.batch_shape == (3, 4)\n",
    "assert d.event_shape == ()\n",
    "x = d.sample()\n",
    "assert x.shape == (3, 4)\n",
    "assert d.log_prob(x).shape == (3, 4)\n",
    "\n",
    "d = MultivariateNormal(torch.zeros(3), torch.eye(3, 3))\n",
    "assert d.batch_shape == ()\n",
    "assert d.event_shape == (3,)\n",
    "x = d.sample()\n",
    "assert x.shape == (3,)            # == batch_shape + event_shape\n",
    "assert d.log_prob(x).shape == ()  # == batch_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE and MAP estimation https://pyro.ai/examples/mle_map.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.zeros(10)\n",
    "data[0:6] = 1.0\n",
    "\n",
    "def original_model(data):\n",
    "    f = pyro.sample(\"latent_fairness\", dist.Beta(10.0, 10.0))\n",
    "    with pyro.plate(\"data\", data.size(0)):\n",
    "        pyro.sample(\"obs\", dist.Bernoulli(f), obs=data)\n",
    "        \n",
    "# Wraps training loop and optimizer\n",
    "def train(model, guide, lr=0.01):\n",
    "    pyro.clear_param_store()\n",
    "    adam = pyro.optim.Adam({\"lr\": lr})\n",
    "    svi = SVI(model, guide, adam, loss=Trace_ELBO())\n",
    "\n",
    "    n_steps = 101\n",
    "    for step in range(n_steps):\n",
    "        loss = svi.step(data)\n",
    "        if step % 50 == 0:\n",
    "            print('[iter {}]  loss: {:.4f}'.format(step, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_mle(data):\n",
    "    # note that we need to include the interval constraint;\n",
    "    # in original_model() this constraint appears implicitly in\n",
    "    # the support of the Beta distribution.\n",
    "    f = pyro.param(\"latent_fairness\", torch.tensor(0.5),\n",
    "                   constraint=constraints.unit_interval)\n",
    "    with pyro.plate(\"data\", data.size(0)):\n",
    "        pyro.sample(\"obs\", dist.Bernoulli(f), obs=data)          \n",
    "def guide_mle(data):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "train(model_mle, guide_mle)\n",
    "print(\"Our MLE estimate of the latent fairness is {:.3f}\".format(\n",
    "      pyro.param(\"latent_fairness\").item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP by using delta distribution for the guide ...\n",
    "def guide_map(data):\n",
    "    f_map = pyro.param(\"f_map\", torch.tensor(0.5),\n",
    "                       constraint=constraints.unit_interval)\n",
    "    pyro.sample(\"latent_fairness\", dist.Delta(f_map))\n",
    "    \n",
    "train(original_model, guide_map)\n",
    "print(\"Our MAP estimate of the latent fairness is {:.3f}\".format(\n",
    "      pyro.param(\"f_map\").item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Flow https://pyro.ai/examples/normalizing_flows_i.html\n",
    "\n",
    "+ motivation \n",
    "    + more flexible parametric family, to model multimodal or heavy-tail distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_x = dist.Normal(torch.zeros(1), torch.ones(1))\n",
    "affine_transform = T.AffineTransform(loc=3, scale=0.5)\n",
    "exp_transform = T.ExpTransform()\n",
    "# Y = Exp(Affine(X))\n",
    "dist_y = dist.TransformedDistribution(dist_x, [affine_transform, exp_transform])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(dist_x.sample([1000]).numpy(), bins=50)\n",
    "plt.title('Standard Normal')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(dist_y.sample([1000]).numpy(), bins=50)\n",
    "plt.title('Log-Normal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learnable \n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "n_samples = 1000\n",
    "X, y = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "\n",
    "plt.title(r'Samples from $p(x_1,x_2)$')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.scatter(X[:,0], X[:,1], alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(X[:,0], hist=False, kde=True,\n",
    "             bins=None,\n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 2})\n",
    "plt.title(r'$p(x_1)$')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.distplot(X[:,1], hist=False, kde=True,\n",
    "             bins=None,\n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 2})\n",
    "plt.title(r'$p(x_2)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learnable Splines\n",
    "#     knots and derivatives at knots are parameters to be learnt \n",
    "#     Fitting marginal p(x_1) p(x_2) ... \n",
    "#     There are dependence between x_1,x_2 ... need to learn them \n",
    "base_dist = dist.Normal(torch.zeros(2), torch.ones(2))\n",
    "spline_transform = T.Spline(2, count_bins=16)\n",
    "flow_dist = dist.TransformedDistribution(base_dist, [spline_transform])\n",
    "\n",
    "# indicate element-wise transform ... \n",
    "assert T.Spline.event_dim == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "steps = 1001\n",
    "dataset = torch.tensor(X, dtype=torch.float)  # (1000, 2)\n",
    "optimizer = torch.optim.Adam(spline_transform.parameters(), lr=1e-2)\n",
    "for step in range(steps):\n",
    "    optimizer.zero_grad()\n",
    "    loss = -flow_dist.log_prob(dataset).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Clears forward-inverse cache ... \n",
    "    # Since spline_tranform is stateful. \n",
    "    # Need to clear the tranform cache manually\n",
    "    flow_dist.clear_cache()\n",
    "\n",
    "    if step % 200 == 0:\n",
    "        print('step: {}, loss: {}'.format(step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flow = flow_dist.sample(torch.Size([1000,])).detach().numpy()\n",
    "plt.title(r'Joint Distribution')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.scatter(X[:,0], X[:,1], label='data', alpha=0.5)\n",
    "plt.scatter(X_flow[:,0], X_flow[:,1], color='firebrick', label='flow', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(X[:,0], hist=False, kde=True,\n",
    "             bins=None,\n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 2},\n",
    "             label='data')\n",
    "sns.distplot(X_flow[:,0], hist=False, kde=True,\n",
    "             bins=None, color='firebrick',\n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 2},\n",
    "             label='flow')\n",
    "plt.title(r'$p(x_1)$')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.distplot(X[:,1], hist=False, kde=True,\n",
    "             bins=None,\n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 2},\n",
    "             label='data')\n",
    "sns.distplot(X_flow[:,1], hist=False, kde=True,\n",
    "             bins=None, color='firebrick',\n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 2},\n",
    "             label='flow')\n",
    "plt.title(r'$p(x_2)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Flow\n",
    "- Motivation is to learn flexible family of distribution that models dependencies between coordinates ... not just matching the marginal\n",
    "- Idea is to design multivariate bijection that have closed \n",
    "  form expression for g,g^-1, a tractable jacobian scales O(D^3)\n",
    "- transform.event_dim indicate how many dependent dimensions \n",
    "- A sequence of splinecoupling layer w/ random permutations to introduce dependencies across all dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dist = dist.Normal(torch.zeros(2), torch.ones(2))\n",
    "spline_transform = T.spline_coupling(2, count_bins=16)\n",
    "flow_dist = dist.TransformedDistribution(base_dist, [spline_transform])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "steps = 5001\n",
    "dataset = torch.tensor(X, dtype=torch.float)\n",
    "optimizer = torch.optim.Adam(spline_transform.parameters(), lr=5e-3)\n",
    "for step in range(steps+1):\n",
    "    optimizer.zero_grad()\n",
    "    loss = -flow_dist.log_prob(dataset).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    flow_dist.clear_cache()\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print('step: {}, loss: {}'.format(step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flow = flow_dist.sample(torch.Size([1000,])).detach().numpy()\n",
    "plt.title(r'Joint Distribution')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.scatter(X[:,0], X[:,1], label='data', alpha=0.5)\n",
    "plt.scatter(X_flow[:,0], X_flow[:,1], color='firebrick', label='flow', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(X[:,0], hist=False, kde=True,\n",
    "             bins=None,\n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 2},\n",
    "             label='data')\n",
    "sns.distplot(X_flow[:,0], hist=False, kde=True,\n",
    "             bins=None, color='firebrick',\n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 2},\n",
    "             label='flow')\n",
    "plt.title(r'$p(x_1)$')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.distplot(X[:,1], hist=False, kde=True,\n",
    "             bins=None,\n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 2},\n",
    "             label='data')\n",
    "sns.distplot(X_flow[:,1], hist=False, kde=True,\n",
    "             bins=None, color='firebrick',\n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 2},\n",
    "             label='flow')\n",
    "plt.title(r'$p(x_2)$')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling conditional distribution \n",
    "- `Y = g(X;C=c)`\n",
    "    - implemented as making parameters of known normalizing flow bijection `g` the output of a hypernetwork that inputs `c`\n",
    "- motivated by usage in amortized inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_base = dist.Normal(torch.zeros(1), torch.ones(1))\n",
    "x1_transform = T.spline(1)\n",
    "dist_x1 = dist.TransformedDistribution(dist_base, [x1_transform])\n",
    "x2_transform = T.conditional_spline(1, context_dim=1)\n",
    "dist_x2_given_x1 = dist.ConditionalTransformedDistribution(dist_base, [x2_transform])\n",
    "\n",
    "# conditioned_dist.condition(x_1).sample()\n",
    "x1 = torch.ones(1)\n",
    "print(dist_x2_given_x1.condition(x1).sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "steps = 5001\n",
    "modules = torch.nn.ModuleList([x1_transform, x2_transform])\n",
    "optimizer = torch.optim.Adam(modules.parameters(), lr=3e-3)\n",
    "x1 = dataset[:,0][:,None]\n",
    "x2 = dataset[:,1][:,None]\n",
    "for step in range(steps):\n",
    "    optimizer.zero_grad()\n",
    "    ln_p_x1 = dist_x1.log_prob(x1)\n",
    "    ln_p_x2_given_x1 = dist_x2_given_x1.condition(x1.detach()).log_prob(x2.detach())\n",
    "    loss = -(ln_p_x1 + ln_p_x2_given_x1).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    dist_x1.clear_cache()\n",
    "    dist_x2_given_x1.clear_cache()\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print('step: {}, loss: {}'.format(step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = torch.cat((x1, x2), dim=-1)\n",
    "x1_flow = dist_x1.sample(torch.Size([1000,]))\n",
    "x2_flow = dist_x2_given_x1.condition(x1_flow).sample(torch.Size([1000,]))\n",
    "X_flow = torch.cat((x1_flow, x2_flow), dim=-1)\n",
    "\n",
    "plt.title(r'Joint Distribution')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.scatter(X[:,0], X[:,1], label='data', alpha=0.5)\n",
    "plt.scatter(X_flow[:,0], X_flow[:,1], color='firebrick', label='flow', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(X[:,0], hist=False, kde=True,\n",
    "             bins=None,\n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 2},\n",
    "             label='data')\n",
    "sns.distplot(X_flow[:,0], hist=False, kde=True,\n",
    "             bins=None, color='firebrick',\n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 2},\n",
    "             label='flow')\n",
    "plt.title(r'$p(x_1)$')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.distplot(X[:,1], hist=False, kde=True,\n",
    "             bins=None,\n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 2},\n",
    "             label='data')\n",
    "sns.distplot(X_flow[:,1], hist=False, kde=True,\n",
    "             bins=None, color='firebrick',\n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 2},\n",
    "             label='flow')\n",
    "plt.title(r'$p(x_2)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoders https://pyro.ai/examples/vae.html\n",
    "\n",
    "- each `z_i` describes local structure (private attrib) to each data point `x_i`\n",
    "- conditional independence allows for SGD optimization over batches with subsampling\n",
    "- Use amortization to capture q(z|x) to keep the number of variational parameters under control, i.e. global parameter shared by all datapoints \n",
    "- optimization \n",
    "    - want large log evidence `log p_theta(x)` (data fit) and approx posterior `q_theta(z|x)` provides good approximation to the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.contrib.examples.util  # patches torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "pyro.enable_validation(True)\n",
    "pyro.distributions.enable_validation(False)\n",
    "pyro.set_rng_seed(0)\n",
    "\n",
    "# for loading and batching MNIST dataset\n",
    "def setup_data_loaders(batch_size=128, use_cuda=False):\n",
    "    root = './data'\n",
    "    download = True\n",
    "    trans = transforms.ToTensor()\n",
    "    train_set = dset.MNIST(root=root, train=True, transform=trans,\n",
    "                           download=download)\n",
    "    test_set = dset.MNIST(root=root, train=False, transform=trans)\n",
    "\n",
    "    kwargs = {'num_workers': 2, 'pin_memory': use_cuda}\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "        batch_size=batch_size, shuffle=False, **kwargs)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # setup the three linear transformations used\n",
    "        self.fc1 = nn.Linear(784, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, z_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, z_dim)\n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define the forward computation on the image x\n",
    "        # first shape the mini-batch to have pixels in the rightmost dimension\n",
    "        x = x.reshape(-1, 784)\n",
    "        # then compute the hidden units\n",
    "        hidden = self.softplus(self.fc1(x))\n",
    "        # then return a mean vector and a (positive) square root covariance\n",
    "        # each of size batch_size x z_dim\n",
    "        z_loc = self.fc21(hidden)\n",
    "        z_scale = torch.exp(self.fc22(hidden))\n",
    "        return z_loc, z_scale\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # setup the two linear transformations used\n",
    "        self.fc1 = nn.Linear(z_dim, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, 784)\n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Returns (batch_size, 784)\n",
    "        \n",
    "        # define the forward computation on the latent z\n",
    "        # first compute the hidden units\n",
    "        hidden = self.softplus(self.fc1(z))\n",
    "        # return the parameter for the output Bernoulli\n",
    "        # each is of size batch_size x 784\n",
    "        loc_img = self.sigmoid(self.fc21(hidden))\n",
    "        return loc_img\n",
    "    \n",
    "class VAE(nn.Module):\n",
    "    # by default our latent space is 50-dimensional\n",
    "    # and we use 400 hidden units\n",
    "    def __init__(self, z_dim=50, hidden_dim=400, use_cuda=False):\n",
    "        super().__init__()\n",
    "        # create the encoder and decoder networks\n",
    "        self.encoder = Encoder(z_dim, hidden_dim)\n",
    "        self.decoder = Decoder(z_dim, hidden_dim)\n",
    "\n",
    "        if use_cuda:\n",
    "            # calling cuda() here will put all the parameters of\n",
    "            # the encoder and decoder networks into gpu memory\n",
    "            self.cuda()\n",
    "        self.use_cuda = use_cuda\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    # define the model p(x|z)p(z)\n",
    "    # takes in minibatch of images x\n",
    "    def model(self, x):\n",
    "        # register PyTorch module `decoder` parameters with Pyro\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            # setup hyperparameters for prior p(z) ~ N(0,I)\n",
    "            z_loc = x.new_zeros(torch.Size((x.shape[0], self.z_dim)))\n",
    "            z_scale = x.new_ones(torch.Size((x.shape[0], self.z_dim)))\n",
    "            # sample from prior (value will be sampled by guide when computing the ELBO)\n",
    "            # `to_event` ensures dim=1 is dependent, i.e. treat as multivariate normal\n",
    "            z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "            # decode the latent code z\n",
    "            loc_img = self.decoder.forward(z)\n",
    "            # score against actual images\n",
    "            pyro.sample(\"obs\", dist.Bernoulli(loc_img).to_event(1), obs=x.reshape(-1, 784))\n",
    "\n",
    "\n",
    "    # define the guide (i.e. variational distribution) q(z|x)\n",
    "    def guide(self, x):\n",
    "        # register PyTorch module `encoder` with Pyro\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            # use the encoder to get the parameters used to define q(z|x)\n",
    "            z_loc, z_scale = self.encoder.forward(x)\n",
    "            # sample the latent code z\n",
    "            # Use same name \"latent\" as in model !\n",
    "            pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "\n",
    "    # define a helper function for reconstructing images\n",
    "    def reconstruct_img(self, x):\n",
    "        # encode image x\n",
    "        z_loc, z_scale = self.encoder(x)\n",
    "        # sample in latent space\n",
    "        z = dist.Normal(z_loc, z_scale).sample()\n",
    "        # decode the image (note we don't sample in image space)\n",
    "        loc_img = self.decoder(z)\n",
    "        return loc_img\n",
    "\n",
    "def train(svi, train_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for x, _ in train_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        epoch_loss += svi.step(x)\n",
    "\n",
    "    # return epoch loss\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    return total_epoch_loss_train\n",
    "\n",
    "def evaluate(svi, test_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    test_loss = 0.\n",
    "    # compute the loss over the entire test set\n",
    "    for x, _ in test_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # compute ELBO estimate and accumulate loss\n",
    "        # won't take gradient step!\n",
    "        test_loss += svi.evaluate_loss(x)\n",
    "    normalizer_test = len(test_loader.dataset)\n",
    "    total_epoch_loss_test = test_loss / normalizer_test\n",
    "    return total_epoch_loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run options\n",
    "LEARNING_RATE = 1.0e-3\n",
    "USE_CUDA = True\n",
    "\n",
    "# Run only for a single iteration for testing\n",
    "NUM_EPOCHS = 30\n",
    "TEST_FREQUENCY = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = setup_data_loaders(batch_size=256, use_cuda=USE_CUDA)\n",
    "\n",
    "# clear param store\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# setup the VAE\n",
    "vae = VAE(use_cuda=USE_CUDA)\n",
    "\n",
    "# setup the optimizer\n",
    "adam_args = {\"lr\": LEARNING_RATE}\n",
    "optimizer = Adam(adam_args)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "train_elbo = []\n",
    "test_elbo = []\n",
    "# training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_epoch_loss_train = train(svi, train_loader, use_cuda=USE_CUDA)\n",
    "    train_elbo.append(-total_epoch_loss_train)\n",
    "    print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
    "\n",
    "    if epoch % TEST_FREQUENCY == 0:\n",
    "        # report test diagnostics\n",
    "        total_epoch_loss_test = evaluate(svi, test_loader, use_cuda=USE_CUDA)\n",
    "        test_elbo.append(-total_epoch_loss_test)\n",
    "        print(\"[epoch %03d] average test loss: %.4f\" % (epoch, total_epoch_loss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_loader))[0]\n",
    "x = x[:10]\n",
    "x_recon = vae.reconstruct_img(x).reshape(x.shape).cpu()\n",
    "ims = torch.cat([x,x_recon],dim=0)\n",
    "ims = torchvision.utils.make_grid(ims,nrow=10)\n",
    "plt.imshow(ims.detach().numpy().transpose([1,2,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
