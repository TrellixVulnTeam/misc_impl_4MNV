{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder\n",
    "\n",
    "The goal of this tutorial is to break down all the parts of the variational autoencoder and discuss them from fundamentals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Review: Learning Distributions\n",
    "\n",
    "Suppose we have samples from an (unknown) underlying distribution $w_i \\sim p_D(w)$, and we want to learn what $p_D(w)$ is.\n",
    "\n",
    "  - This problem is extremely general, and many learning problems fall under this viewpoint. Take $w = y|x$, then this would be a supervised learning problem of learning $p_D(y|x)$.\n",
    "\n",
    "  - We would want to approximate the true distribution $p_D(w)$ with a parametric approximation $q_\\theta(w)$. Any objective between distributions can be used. A notable one is the (forward) __KL divergence__ $KL(p_D(w)\\;||\\; q_\\theta(w))$.\n",
    "\n",
    "\n",
    "### Relating back to what we know...\n",
    "In the special case of a classification problem with $K$ classes, this objective is the _cross entropy_ (CE) loss.\n",
    "\n",
    "$$\n",
    "\\min_\\theta KL(p_D(y|x)\\;||\\; q_\\theta(y|x)) = \\underbrace{\\mathbb{E}_{t \\sim p_D(y|x)}[\\log p_D(t|x) - \\log q_\\theta(t|x)]}_{\\text{definition}} = \\underbrace{\\mathbb{E}_{t \\sim p_D(y|x)}[\\log p_D(t|x)]}_{\\text{no effect on optimization of $\\theta$}} - \\mathbb{E}_{t \\sim p_D(y|x)} [\\log q_\\theta(t|x)]\n",
    "$$\n",
    "\n",
    "The first term does not depend on $\\theta$, and we evaluate the second term with a single sample to obtain the CE loss:\n",
    "\n",
    "$$\n",
    "-\\mathbb{E}_{t \\sim p_D(y|x)} [\\log q_\\theta(t|x)] = - \\log q_\\theta(t|x)\n",
    "$$\n",
    "\n",
    "### More generally...\n",
    "\n",
    "More generally, if we just want to learn an approximate distribution $q_\\theta(w)$ to $p_D(w)$, many problems arise.\n",
    "\n",
    "1. __Model constraint.__ $q_\\theta$ needs to be __normalized__, ie. $\\int_w q_\\theta(w) dw = 1$. In classification, $w = y|x$ is a discrete distribution with only $K$ possible values, so we can just enforce the sum to be 1 (e.g. by using softmax).\n",
    "\n",
    "2. __Problem knowledge.__ (Partial) information about $p_D(w)$ can come in many forms. Typical scenarios are (i) i.i.d. samples can be obtained from $p_D$, (ii) $p_D$ can be evaluated at any value of $w$, perhaps up to a normalizing constant. It may be that you only have (i) but not (ii) or vice-versa. _The training objective for $q_\\theta$ needs to take into account what information is available._\n",
    "\n",
    "#### Parameterizing q to always be normalized.\n",
    "\n",
    "One way to tackle the normalization problem is to output the _statistics_ of a known distribution instead of the probability density function itself. e.g. if we take\n",
    "\n",
    "$$q_\\theta(w) = \\mathcal{N}(\\mu_\\theta(w), \\sigma^2_\\theta(w))$$\n",
    "\n",
    "then we can model $q_\\theta(w)$ by having a neural network that takes a value $w$ and outputs $\\mu_\\theta(w)$ and $\\sigma_\\theta(w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference\n",
    "\n",
    "Variational inference can be used to approximate an unnormalized density $\\tilde{p}(z) = \\frac{p(z)}{C}$ with a learned distribution $q_\\theta(z)$.\n",
    "\n",
    "### Motivation\n",
    "\n",
    "The main application (and why it's referred to as inference) is to learn an approximation to the conditional distribution $p(z|x)$ when we only have access to the joint distribution $p(z,x)=p(z)p(x|z)$. \n",
    "\n",
    "Bayes rule:\n",
    "$$\n",
    "\\underbrace{p(z|x)}_{\\text{posterior}} = \\frac{p(x|z)p(z)}{p(x)} \\propto \\underbrace{p(x|z)}_{\\text{likelihood}}\\underbrace{p(z)}_{\\text{prior}}\n",
    "$$\n",
    "\n",
    "Trying to figure out what the posterior should be given the likelihood and prior is an inference problem.\n",
    "\n",
    "This is in the same form as\n",
    "$$\n",
    "p(z) = \\frac{\\tilde{p}(z)}{C}\n",
    "$$\n",
    "if we fix the variable $x$, which means we are trying to learn the posterior $p(z|x)$.\n",
    "\n",
    "The \"variational\" part of the name comes from approximating the true distribution $p(z|x)$ with an approximate (a.k.a. \"variational\") distribution $q_\\theta(z)$.\n",
    "\n",
    "### Training Objective: _Forward_ KL\n",
    "\n",
    "We want an objective such that the minmizer is $q_\\theta(z) = p(z)$.\n",
    "\n",
    "Since we only have an unnormalized density $\\tilde{p}(z)$ (that we can evaluate at any of $z$), a natural objective is the _forward_ KL\n",
    "\n",
    "$$\n",
    "L(\\theta) := KL(q_\\theta\\;||\\;p) = \\mathbb{E}_{z\\sim q(z)}[\\log q(z) - \\log p(z)] = \\mathbb{E}_{z\\sim q(z)}[\\log q(z) - \\log \\tilde{p}(z) + \\log C]\n",
    "$$\n",
    "\n",
    "The constant $C$ does not affect the optimization of $L(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_logprob(z, mean, log_std):\n",
    "    mean = mean + torch.tensor(0.)\n",
    "    log_std = log_std + torch.tensor(0.)\n",
    "    c = torch.tensor([math.log(2 * math.pi)]).to(z)\n",
    "    inv_sigma = torch.exp(-log_std)\n",
    "    tmp = (z - mean) * inv_sigma\n",
    "    return -0.5 * (tmp * tmp + 2 * log_std + c)\n",
    "\n",
    "def log_density_funnel(x):\n",
    "    x1, x2 = x[:, 0], x[:, 1]\n",
    "    x2_logdensity = normal_logprob(x2, 0, np.log(1.35))\n",
    "    x1_logdensity = normal_logprob(x1, 0, x2)\n",
    "    return x2_logdensity + x1_logdensity\n",
    "\n",
    "def plot_isocontours(ax, func, xlimits=[-3, 3], ylimits=[-5, 3], numticks=101, alpha=1.0, cmap=plt.get_cmap('viridis')):\n",
    "    x = np.linspace(*xlimits, num=numticks)\n",
    "    y = np.linspace(*ylimits, num=numticks)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    pts = torch.from_numpy(np.concatenate([np.atleast_2d(X.ravel()), np.atleast_2d(Y.ravel())]).T)\n",
    "    zs = torch.exp(func(pts)).detach().cpu().numpy()\n",
    "    Z = zs.reshape(X.shape)\n",
    "    ax.contour(X, Y, Z, alpha=alpha, cmap=cmap)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xlim(xlimits)\n",
    "    ax.set_ylim(ylimits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6), facecolor='white')\n",
    "ax = fig.add_subplot(111, frameon=False)\n",
    "plot_isocontours(ax, log_density_funnel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we try to fit this funnel-like distribution with a Gaussian distribution $q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters.\n",
    "mean = nn.Parameter(torch.randn(1, 2))\n",
    "log_std = nn.Parameter(torch.randn(1, 2))\n",
    "\n",
    "def q_logprob(x):\n",
    "    return normal_logprob(x.float(), mean, log_std).sum(1)\n",
    "\n",
    "# Objective is the forward KL: E_q[logq - logp].\n",
    "def calculate_kl(logprob_func, n_samples):\n",
    "    ###########################################\n",
    "    # Note: Sample from Q, evaluate logQ - logP\n",
    "    ###########################################\n",
    "    samples = torch.randn(n_samples, 2).to(mean) * torch.exp(log_std) + mean\n",
    "    logq = q_logprob(samples)\n",
    "    logp = logprob_func(samples)\n",
    "    return torch.mean(logq - logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4), facecolor='white')\n",
    "ax = fig.add_subplot(121, frameon=False)\n",
    "plot_isocontours(ax, log_density_funnel)\n",
    "plot_isocontours(ax, q_logprob)\n",
    "ax.set_title('Initial')\n",
    "\n",
    "# Optimizer.\n",
    "optimizer = torch.optim.Adam([mean, log_std], lr=1e-2)\n",
    "\n",
    "# Training loop.\n",
    "for i in range(3000):\n",
    "    optimizer.zero_grad()\n",
    "    kl = calculate_kl(log_density_funnel, 100)\n",
    "    kl.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "ax = fig.add_subplot(122, frameon=False)\n",
    "plot_isocontours(ax, log_density_funnel)\n",
    "plot_isocontours(ax, q_logprob)\n",
    "ax.set_title('After Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Note on Forward vs reverse KL\n",
    "\n",
    "KL divergence is asymmetric, implying $KL(q_\\theta\\;||\\;p_D) \\ne KL(p_D\\;||\\;q_\\theta)$.\n",
    "We may want to use either one depending on what we know about $p_D$.\n",
    "\n",
    "If we only have samples from $p_D$, use _forward KL_: $KL(p_D\\;||\\;q_\\theta)$. This objective is equivalent to performing \"maximum likelihood\".\n",
    "\n",
    "If we can only evaluate $p_D$ up to a normalizing constant, use _reverse KL_: $KL(q_\\theta\\;||\\;p_D)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent-Variable Models (LVM)\n",
    "\n",
    "Now instead of having a fixed $p(x|z)$, suppose we also want to learn this generative model $p_\\theta(x|z)$.\n",
    "\n",
    "#### Motivation\n",
    "Instead of directly learning a distribution on $p(x)$, we may want to learn $p(x|z)$ because:\n",
    "\n",
    "  - The latent variable $z$ is typically lower dimensional and may be more _interpretable_ or understandable. \n",
    "  - $p(x|z)$ can be _easier_ to model. (We can choose $z$ to be anything we want, including having no information about $x$, so then $p(x|z) = p(x) \\;\\forall z$. Thus directly learning $p(x)$ can be seen as a degenerate case of a latent variable model.)\n",
    "  - The marginal distribution $p(x)$ can be more _flexible_.\n",
    "\n",
    "Why not:\n",
    "  \n",
    "  - Access to $p(x)$ is now only implicitly defined using an integral $p(x) = \\int p(x|z)p(z) dz$.\n",
    "\n",
    "Let's go back to Bayes rule:\n",
    "\n",
    "$$\n",
    "p_\\theta(z|x) = \\frac{p_\\theta(x|z)p(z)}{p_\\theta(x)}\n",
    "$$\n",
    "\n",
    "Here we use $\\theta$ to denote that the distribution depends on $\\theta$ even if we're only directly parameterizing $p_\\theta(x|z)$. We can no longer treat $p_\\theta(x) = \\int p_\\theta(x|z)p(z) dz $ as a constant because it depends on the parameters $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evidence Lower BOund (ELBO)\n",
    "\n",
    "Ideally, we directly optimize based on $p_\\theta(x)$, but this is hidden behind an expensive integral. Instead, the ELBO is typically used to train latent variable models by introducing a variational distribution $q(z)$.\n",
    "\n",
    "The ELBO can be derived in many different ways. Here's a short one: take Bayes' rule and apply to both sides, (i) log, (ii) subtract $\\log q(z)$, (iii) take expectation w.r.t. $q(z)$.\n",
    "\n",
    "$$\n",
    "\\log p(z|x) = \\log p(x|z) + \\log p(z) - \\log p(x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\log p(z|x) - \\log q(z) = \\log p(x|z) + \\log p(z) - \\log p(x) - \\log q(z)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\underbrace{\\mathbb{E}_{z\\sim q(z)} [ \\log p(z|x) - \\log q(z) ]}_{- \\text{reverse KL}} = \\underbrace{\\mathbb{E}_{z\\sim q(z)} [ \\log p(x|z) + \\log p(z) - \\log q(z) ]}_{\\text{ELBO}} - \\log p(x)\n",
    "$$\n",
    "\n",
    "Re-arrange this, and we get\n",
    "\n",
    "$$\n",
    "\\log p(x) = \\text{ELBO(x)} + \\underbrace{KL(q(z)\\;||\\;p(z|x))}_{\\geq 0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{ELBO(x)} := \\mathbb{E}_{z\\sim q(z)} [ \\log p(x|z) + \\log p(z) - \\log q(z)]\n",
    "$$\n",
    "\n",
    "  - The ELBO is a function of $x$, so we can use $q(z|x)$ instead of $q(z)$.\n",
    "  - Previously, we had an example where we minimized $KL(q(z)\\;||\\;p(z|x))$ for $p(x|z)$ and $x$ fixed. This is equivalent to maximizing the ELBO because $\\log p_\\theta(x)$ is constant.\n",
    "\n",
    "When we want to train a generative model $p_\\theta(x|z)$, maximizing the ELBO simultaneously:\n",
    "  - maximizes $p_\\theta(x)$, thus learning a generative model, and\n",
    "  - minimizes $KL(q(z|x)\\;||\\;p(z|x))$, thus performing variational inference on $p(z|x)$.\n",
    "  \n",
    "The ELBO is a function of $x$, so if we have a dataset of samples $x_i$, we can maximize\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n ELBO(x_i)\n",
    "$$\n",
    "\n",
    "_Aside: This is equivalent to minimizing an upper bound on the forward KL divergence $KL(p_D(x)\\;||\\; p_\\theta(x))$ with $n$ samples from $p_D$. Can you derive this equivalence?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(2, 100), nn.ReLU(), nn.Linear(100, 4))\n",
    "        self.decoder = nn.Sequential(nn.Linear(2, 100), nn.ReLU(), nn.Linear(100, 2))\n",
    "    \n",
    "    def elbo(self, x):\n",
    "        # Get q(z|x). Get p(x|z). Kind of like an autoencoder.\n",
    "        q_params = self.encoder(x)\n",
    "        q_mean, q_logstd = q_params[:, :2], q_params[:, 2:]\n",
    "        q_samples = torch.randn(x.shape[0], 2).to(q_mean) * torch.exp(q_logstd) + q_mean\n",
    "        px_mean = self.decoder(q_samples)\n",
    "        \n",
    "        # Compute ELBO = logp(x|z) + logp(z) - logq(z)\n",
    "        logqz = normal_logprob(q_samples, q_mean, q_logstd).sum(1)\n",
    "        logpz = normal_logprob(q_samples, 0., 0.).sum(1)\n",
    "        logpxz = normal_logprob(x, px_mean, 0.).sum(1)\n",
    "        \n",
    "        elbo = torch.mean(logpxz + logpz - logqz)\n",
    "        return elbo\n",
    "\n",
    "    \n",
    "# We train on a dataset of just four points.\n",
    "data = torch.tensor([\n",
    "    [5, 5],\n",
    "    [0, 5],\n",
    "    [5, 0],\n",
    "    [-5, -5],\n",
    "]).float()\n",
    "\n",
    "# VAE model.\n",
    "model = VAE()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "# Just plotting code.\n",
    "fig = plt.figure(figsize=(12, 3), facecolor='white')\n",
    "ax = fig.add_subplot(141, frameon=False)\n",
    "plot_isocontours(ax, lambda x: normal_logprob(x, 0., 0.).sum(1))\n",
    "ax.set_title('Prior')\n",
    "ax.set_ylim([-3, 3])\n",
    "ax.set_xlim([-3, 3])\n",
    "\n",
    "ax = fig.add_subplot(142, frameon=False)\n",
    "q_params = model.encoder(data)\n",
    "q_mean, q_logstd = q_params[:, :2], q_params[:, 2:]\n",
    "for i in range(4):\n",
    "    plot_isocontours(ax, lambda x: normal_logprob(x.float(), q_mean[i], q_logstd[i]).sum(1))\n",
    "ax.set_title('q(z|x) (Init)')\n",
    "ax.set_ylim([-3, 3])\n",
    "ax.set_xlim([-3, 3])\n",
    "\n",
    "# Training loop.\n",
    "for _ in tqdm(range(10000)):\n",
    "    optimizer.zero_grad()\n",
    "    elbo = model.elbo(data)\n",
    "    elbo.mul(-1).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# More plotting code.\n",
    "ax = fig.add_subplot(143, frameon=False)\n",
    "q_params = model.encoder(data)\n",
    "q_mean, q_logstd = q_params[:, :2], q_params[:, 2:]\n",
    "for i in range(q_mean.shape[0]):\n",
    "    plot_isocontours(ax, lambda x: normal_logprob(x.float(), q_mean[i], q_logstd[i]).sum(1))\n",
    "ax.set_title('q(z|x) (After Training)')\n",
    "ax.set_ylim([-3, 3])\n",
    "ax.set_xlim([-3, 3])\n",
    "\n",
    "ax = fig.add_subplot(144, frameon=False)\n",
    "for i in range(q_mean.shape[0]):\n",
    "    samples = torch.randn(1000, 2).to(q_mean) * torch.exp(q_logstd[i][None]) + q_mean[i][None]\n",
    "    ax.scatter(samples[:, 0].detach().numpy(), samples[:, 1].detach().numpy())\n",
    "ax.set_title('q(z|x) (Samples)')\n",
    "ax.set_ylim([-3, 3])\n",
    "ax.set_xlim([-3, 3])\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks([])\n",
    "\n",
    "print('Final ELBO: {:.3f}'.format(elbo.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Flows\n",
    "\n",
    "Previously, we stuck to modeling using Gaussians because we know the normalization constant of a Gaussian distribution. Normalizing flows offer a more flexible way to tackle the normalization constant.\n",
    "\n",
    "The change of variables applied to random variables:\n",
    "\n",
    "$$\n",
    "\\log p(x) = \\underbrace{\\log p(x_0)}_{\\text{simple distribution}} - \\underbrace{\\log \\left| \\det \\frac{\\partial T(x_0)}{\\partial x_0} \\right|}_{\\text{automatically normalizes $p(x)$}}\n",
    "$$\n",
    "\n",
    "where $T$ is an __invertible__ transformation applied to samples from a simple distribution $p(x_0)$ into samples from $p(x)$. Here $p_\\theta(x)$ is defined by the transformation $T_\\theta$ itself.\n",
    "\n",
    "An example of an invertible transformation is (in two dimensions):\n",
    "\n",
    "$$T(z_0) = z_0$$\n",
    "$$T(z_1) = z_1 + f(z_0)$$\n",
    "\n",
    "Then the Jacobian is\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "\\frac{\\partial f}{\\partial z_0} & 1\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Since this is a triangular matrix, the determinant is the product of the diagonal elements, which is 1. The log determinant is 0. So for this type of transformation $p(x) = p(x_0)$ where $x = T(x_0)$. A transformation that doesn't change the probability density of the transformed sample is called _volume preserving_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying normalizing flows (NF) to variational inference.\n",
    "\n",
    "Previously, we made the Gaussian assumption in two places:\n",
    "\n",
    "  - In $q(z|x)$, in the variational inference part of the VAE.\n",
    "  - In $p(x|z)$, in the generative model part of the VAE.\n",
    "\n",
    "We could replace either with a NF, but we'll focus on using NF to express a more complex variational posterior, since this would help make training the generative model easier as well (because it can now have any posterior, not just ones easily approximated by a Gaussian).\n",
    "\n",
    "To model $q(z|x)$ using a normalizing flow, we first define a simple distribution on $z_0$ then apply an invertible transformation to obtain:\n",
    "\n",
    "$$\n",
    "\\log q(z|x) = \\log q(z_0|x) - \\log \\left| \\det \\frac{\\partial T(z_0)}{\\partial z_0} \\right|\n",
    "$$\n",
    "\n",
    "The second term is zero when using the volume preserving transformation from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!! The new parts of the code are commented with two hash symbols \n",
    "##!! and two exclaimation marks.\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(2, 100), nn.Tanh(), nn.Linear(100, 4))\n",
    "        self.decoder = nn.Sequential(nn.Linear(2, 100), nn.Tanh(), nn.Linear(100, 2))\n",
    "        \n",
    "        ##!! We use 10 series of invertible transformations.\n",
    "        self.transforms = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(1, 20), nn.Tanh(), nn.Linear(20, 1)) for _ in range(10)\n",
    "        ])\n",
    "        \n",
    "    def transform(self, z):\n",
    "        ##!! Alternate dimensions for transformation.\n",
    "        for i, transform in enumerate(self.transforms):\n",
    "            if i % 2 == 0:\n",
    "                z = torch.cat([z[:, 0:1], z[:, 1:2] + transform(z[:, 0:1])], dim=1)\n",
    "            else:\n",
    "                z = torch.cat([z[:, 0:1] + transform(z[:, 1:2]), z[:, 1:2]], dim=1)\n",
    "        return z\n",
    "    \n",
    "    def elbo(self, x):\n",
    "        # Get q(z|x). Get p(x|z). Kind of like an autoencoder.\n",
    "        q_params = self.encoder(x)\n",
    "        q_mean, q_logstd = q_params[:, :2], q_params[:, 2:]\n",
    "        q_samples = torch.randn(x.shape[0], 2).to(q_mean) * torch.exp(q_logstd) + q_mean\n",
    "        \n",
    "        ##!! Compute log q(z_0) based on pre-transform samples.\n",
    "        logqz = normal_logprob(q_samples, q_mean, q_logstd).sum(1)\n",
    "        \n",
    "        ##!! Transformed samples.\n",
    "        q_samples = self.transform(q_samples)\n",
    "        \n",
    "        px_mean = self.decoder(q_samples)\n",
    "        \n",
    "        # Compute ELBO = logp(x|z) + logp(z) - logq(z)\n",
    "        logpz = normal_logprob(q_samples, 0., 0.).sum(1)\n",
    "        logpxz = normal_logprob(x, px_mean, 0.).sum(1)\n",
    "        \n",
    "        elbo = torch.mean(logpxz + logpz - logqz)\n",
    "        return elbo\n",
    "\n",
    "    \n",
    "# We train on a dataset of just four points.\n",
    "data = torch.tensor([\n",
    "    [5, 5],\n",
    "    [0, 5],\n",
    "    [5, 0],\n",
    "    [-5, -5],\n",
    "]).float()\n",
    "\n",
    "# VAE model.\n",
    "model = VAE()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "# Just plotting code.\n",
    "fig = plt.figure(figsize=(6, 3), facecolor='white')\n",
    "ax = fig.add_subplot(121, frameon=False)\n",
    "plot_isocontours(ax, lambda x: normal_logprob(x, 0., 0.).sum(1))\n",
    "ax.set_title('Prior')\n",
    "ax.set_ylim([-3, 3])\n",
    "ax.set_xlim([-3, 3])\n",
    "    \n",
    "# Training loop.\n",
    "for _ in tqdm(range(10000)):\n",
    "    optimizer.zero_grad()\n",
    "    elbo = model.elbo(data)\n",
    "    elbo.mul(-1).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# More plotting code.\n",
    "ax = fig.add_subplot(122, frameon=False)\n",
    "for i in range(q_mean.shape[0]):\n",
    "    samples = torch.randn(1000, 2).to(q_mean) * torch.exp(q_logstd[i][None]) + q_mean[i][None]\n",
    "    samples = model.transform(samples)\n",
    "    ax.scatter(samples[:, 0].detach().numpy(), samples[:, 1].detach().numpy())\n",
    "ax.set_title('q(z|x) (Samples)')\n",
    "ax.set_ylim([-3, 3])\n",
    "ax.set_xlim([-3, 3])\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks([])\n",
    "print('Final ELBO: {:.3f}'.format(elbo.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
