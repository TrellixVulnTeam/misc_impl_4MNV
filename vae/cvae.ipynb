{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary # pip only\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision \n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "import models\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([794, 500, 2], [12, 500, 784], [10, 5, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "torch.manual_seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(0)\n",
    "\n",
    "latent_size = 2\n",
    "enc_sizes = [784,500,latent_size]\n",
    "dec_sizes = [latent_size,500,784]\n",
    "batch_size = 100\n",
    "train = False\n",
    "\n",
    "n_epochs = 200\n",
    "n_batches_print = 100\n",
    "\n",
    "conditional_size = 10\n",
    "\n",
    "cvae_enc_sizes = enc_sizes.copy()\n",
    "cvae_enc_sizes[0] += conditional_size\n",
    "\n",
    "cvae_dec_sizes = dec_sizes.copy()\n",
    "cvae_dec_sizes[0] += conditional_size\n",
    "\n",
    "cvae_prior_network_sizes = [conditional_size, 5, latent_size]\n",
    "\n",
    "cvae_enc_sizes, cvae_dec_sizes, cvae_prior_network_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Lambda(lambda x: torch.round(x)),\n",
    "    torchvision.transforms.Lambda(lambda x: x.view(-1)),\n",
    "])\n",
    "\n",
    "trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transforms)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "testset = datasets.MNIST(root='../data', train=False, download=True, transform=transforms)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 784]) torch.Size([100]) tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2b5f892e50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x,y = next(iter(trainloader))\n",
    "print(x.shape, y.shape, torch.nn.functional.one_hot(y,10)[0])\n",
    "\n",
    "plt.imshow(x[0].view(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((x, torch.nn.functional.one_hot(y,10).float()), 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVAE(\n",
      "  (recognition_network): Encoder(\n",
      "    (mlp): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=794, out_features=500, bias=True)\n",
      "        (1): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (fc_mu): Linear(in_features=500, out_features=2, bias=True)\n",
      "    (fc_logvariance): Linear(in_features=500, out_features=2, bias=True)\n",
      "  )\n",
      "  (recognition_sampling_layer): StochasticLayer()\n",
      "  (prior_network): Encoder(\n",
      "    (mlp): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=10, out_features=5, bias=True)\n",
      "        (1): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (fc_mu): Linear(in_features=5, out_features=2, bias=True)\n",
      "    (fc_logvariance): Linear(in_features=5, out_features=2, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (mlp): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=12, out_features=500, bias=True)\n",
      "        (1): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (last_layer): Sequential(\n",
      "      (0): Linear(in_features=500, out_features=784, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cvae = models.CVAE(cvae_enc_sizes, cvae_dec_sizes, cvae_prior_network_sizes)\n",
    "cvae.to(device)\n",
    "print(cvae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 100] loss: 223.18182373046875\n",
      "[1 200] loss: 167.0593719482422\n",
      "[1 300] loss: 157.74334716796875\n",
      "[1 400] loss: 154.23959350585938\n",
      "[1 500] loss: 151.65328979492188\n",
      "[1 600] loss: 149.1203155517578\n",
      "[2 100] loss: 146.91635131835938\n",
      "[2 200] loss: 144.77456665039062\n",
      "[2 300] loss: 143.46336364746094\n",
      "[2 400] loss: 141.84164428710938\n",
      "[2 500] loss: 141.88943481445312\n",
      "[2 600] loss: 139.6652069091797\n",
      "[3 100] loss: 139.5169219970703\n",
      "[3 200] loss: 137.48915100097656\n",
      "[3 300] loss: 138.18275451660156\n",
      "[3 400] loss: 135.80918884277344\n",
      "[3 500] loss: 135.9220428466797\n",
      "[3 600] loss: 136.08078002929688\n",
      "[4 100] loss: 135.43186950683594\n",
      "[4 200] loss: 134.73333740234375\n",
      "[4 300] loss: 133.71017456054688\n",
      "[4 400] loss: 133.71449279785156\n",
      "[4 500] loss: 133.6317901611328\n",
      "[4 600] loss: 132.6844024658203\n",
      "[5 100] loss: 132.9744873046875\n",
      "[5 200] loss: 133.35203552246094\n",
      "[5 300] loss: 131.6920623779297\n",
      "[5 400] loss: 132.23805236816406\n",
      "[5 500] loss: 130.8469696044922\n",
      "[5 600] loss: 131.3266143798828\n",
      "[6 100] loss: 130.48208618164062\n",
      "[6 200] loss: 130.94613647460938\n",
      "[6 300] loss: 130.6638641357422\n",
      "[6 400] loss: 130.78855895996094\n",
      "[6 500] loss: 130.35153198242188\n",
      "[6 600] loss: 131.143798828125\n",
      "[7 100] loss: 129.9066925048828\n",
      "[7 200] loss: 129.72415161132812\n",
      "[7 300] loss: 129.74330139160156\n",
      "[7 400] loss: 130.19943237304688\n",
      "[7 500] loss: 129.5372772216797\n",
      "[7 600] loss: 129.1055145263672\n",
      "[8 100] loss: 128.77224731445312\n",
      "[8 200] loss: 129.42074584960938\n",
      "[8 300] loss: 128.4838409423828\n",
      "[8 400] loss: 128.76327514648438\n",
      "[8 500] loss: 129.13214111328125\n",
      "[8 600] loss: 128.2353515625\n",
      "[9 100] loss: 128.3371124267578\n",
      "[9 200] loss: 128.5458221435547\n",
      "[9 300] loss: 128.47792053222656\n",
      "[9 400] loss: 128.19309997558594\n",
      "[9 500] loss: 127.29068756103516\n",
      "[9 600] loss: 127.4482192993164\n",
      "[10 100] loss: 127.52689361572266\n",
      "[10 200] loss: 127.04105377197266\n",
      "[10 300] loss: 127.90272521972656\n",
      "[10 400] loss: 127.09742736816406\n",
      "[10 500] loss: 126.86106872558594\n",
      "[10 600] loss: 127.90000915527344\n",
      "[11 100] loss: 127.07275390625\n",
      "[11 200] loss: 126.00045013427734\n",
      "[11 300] loss: 127.29412078857422\n",
      "[11 400] loss: 127.28716278076172\n",
      "[11 500] loss: 126.32544708251953\n",
      "[11 600] loss: 127.0481185913086\n",
      "[12 100] loss: 127.04678344726562\n",
      "[12 200] loss: 126.32688903808594\n",
      "[12 300] loss: 126.3397216796875\n",
      "[12 400] loss: 126.9892578125\n",
      "[12 500] loss: 125.94908142089844\n",
      "[12 600] loss: 125.3984146118164\n",
      "[13 100] loss: 126.33197784423828\n",
      "[13 200] loss: 125.56861114501953\n",
      "[13 300] loss: 126.69416046142578\n",
      "[13 400] loss: 124.67264556884766\n",
      "[13 500] loss: 125.56954956054688\n",
      "[13 600] loss: 126.59352111816406\n",
      "[14 100] loss: 125.53805541992188\n",
      "[14 200] loss: 126.16111755371094\n",
      "[14 300] loss: 125.56192779541016\n",
      "[14 400] loss: 124.64918518066406\n",
      "[14 500] loss: 125.83004760742188\n",
      "[14 600] loss: 125.40190124511719\n",
      "[15 100] loss: 124.92198181152344\n",
      "[15 200] loss: 125.4808349609375\n",
      "[15 300] loss: 124.67742919921875\n",
      "[15 400] loss: 125.58212280273438\n",
      "[15 500] loss: 125.08463287353516\n",
      "[15 600] loss: 125.25126647949219\n",
      "[16 100] loss: 125.29319763183594\n",
      "[16 200] loss: 125.16839599609375\n",
      "[16 300] loss: 124.90560913085938\n",
      "[16 400] loss: 124.5697021484375\n",
      "[16 500] loss: 124.4831771850586\n",
      "[16 600] loss: 124.79317474365234\n",
      "[17 100] loss: 124.6077880859375\n",
      "[17 200] loss: 124.79476165771484\n",
      "[17 300] loss: 125.32655334472656\n",
      "[17 400] loss: 123.80062103271484\n",
      "[17 500] loss: 124.97663116455078\n",
      "[17 600] loss: 124.13998413085938\n",
      "[18 100] loss: 124.60267639160156\n",
      "[18 200] loss: 123.86554718017578\n",
      "[18 300] loss: 124.07373809814453\n",
      "[18 400] loss: 124.2123794555664\n",
      "[18 500] loss: 124.25775146484375\n",
      "[18 600] loss: 125.17851257324219\n",
      "[19 100] loss: 125.58580017089844\n",
      "[19 200] loss: 123.82715606689453\n",
      "[19 300] loss: 124.29647827148438\n",
      "[19 400] loss: 123.53260803222656\n",
      "[19 500] loss: 123.77244567871094\n",
      "[19 600] loss: 123.93207550048828\n",
      "[20 100] loss: 123.7174072265625\n",
      "[20 200] loss: 124.19383239746094\n",
      "[20 300] loss: 123.9372787475586\n",
      "[20 400] loss: 123.40580749511719\n",
      "[20 500] loss: 124.3876953125\n",
      "[20 600] loss: 123.98724365234375\n",
      "[21 100] loss: 124.39513397216797\n",
      "[21 200] loss: 123.57646942138672\n",
      "[21 300] loss: 123.03889465332031\n",
      "[21 400] loss: 124.25806427001953\n",
      "[21 500] loss: 124.08702087402344\n",
      "[21 600] loss: 123.03491973876953\n",
      "[22 100] loss: 123.1125259399414\n",
      "[22 200] loss: 123.49588775634766\n",
      "[22 300] loss: 123.93428802490234\n",
      "[22 400] loss: 123.22064971923828\n",
      "[22 500] loss: 124.00498962402344\n",
      "[22 600] loss: 123.5252914428711\n",
      "[23 100] loss: 123.54364776611328\n",
      "[23 200] loss: 123.26510620117188\n",
      "[23 300] loss: 122.97120666503906\n",
      "[23 400] loss: 123.77403259277344\n",
      "[23 500] loss: 123.59790802001953\n",
      "[23 600] loss: 123.08296966552734\n",
      "[24 100] loss: 123.2011489868164\n",
      "[24 200] loss: 123.2494888305664\n",
      "[24 300] loss: 122.80846405029297\n",
      "[24 400] loss: 122.65998077392578\n",
      "[24 500] loss: 122.94623565673828\n",
      "[24 600] loss: 124.5180892944336\n",
      "[25 100] loss: 123.23807525634766\n",
      "[25 200] loss: 122.79930114746094\n",
      "[25 300] loss: 123.0832290649414\n",
      "[25 400] loss: 123.13103485107422\n",
      "[25 500] loss: 123.42755889892578\n",
      "[25 600] loss: 122.85820007324219\n",
      "[26 100] loss: 123.065185546875\n",
      "[26 200] loss: 122.41839599609375\n",
      "[26 300] loss: 122.92359924316406\n",
      "[26 400] loss: 122.6810531616211\n",
      "[26 500] loss: 122.74191284179688\n",
      "[26 600] loss: 123.7398910522461\n",
      "[27 100] loss: 122.36797332763672\n",
      "[27 200] loss: 122.52937316894531\n",
      "[27 300] loss: 123.13107299804688\n",
      "[27 400] loss: 122.44515228271484\n",
      "[27 500] loss: 122.69047546386719\n",
      "[27 600] loss: 123.582275390625\n",
      "[28 100] loss: 122.43223571777344\n",
      "[28 200] loss: 122.53824615478516\n",
      "[28 300] loss: 122.9070053100586\n",
      "[28 400] loss: 123.05731201171875\n",
      "[28 500] loss: 122.5888671875\n",
      "[28 600] loss: 122.45762634277344\n",
      "[29 100] loss: 122.83155059814453\n",
      "[29 200] loss: 122.27473449707031\n",
      "[29 300] loss: 122.47138214111328\n",
      "[29 400] loss: 121.51810455322266\n",
      "[29 500] loss: 123.2807388305664\n",
      "[29 600] loss: 122.9776611328125\n",
      "[30 100] loss: 122.5110092163086\n",
      "[30 200] loss: 122.84857940673828\n",
      "[30 300] loss: 122.38141632080078\n",
      "[30 400] loss: 121.94332122802734\n",
      "[30 500] loss: 122.76513671875\n",
      "[30 600] loss: 122.10955047607422\n",
      "[31 100] loss: 122.28907775878906\n",
      "[31 200] loss: 122.58126831054688\n",
      "[31 300] loss: 122.8933334350586\n",
      "[31 400] loss: 121.51908111572266\n",
      "[31 500] loss: 122.44979095458984\n",
      "[31 600] loss: 122.02372741699219\n",
      "[32 100] loss: 121.49311065673828\n",
      "[32 200] loss: 122.97396087646484\n",
      "[32 300] loss: 122.22802734375\n",
      "[32 400] loss: 122.29463958740234\n",
      "[32 500] loss: 122.2538833618164\n",
      "[32 600] loss: 122.134033203125\n",
      "[33 100] loss: 122.39457702636719\n",
      "[33 200] loss: 120.6703109741211\n",
      "[33 300] loss: 122.9886703491211\n",
      "[33 400] loss: 122.1251220703125\n",
      "[33 500] loss: 122.58354949951172\n",
      "[33 600] loss: 121.88678741455078\n",
      "[34 100] loss: 121.727783203125\n",
      "[34 200] loss: 122.32633209228516\n",
      "[34 300] loss: 121.77289581298828\n",
      "[34 400] loss: 122.83688354492188\n",
      "[34 500] loss: 122.1511459350586\n",
      "[34 600] loss: 121.110107421875\n",
      "[35 100] loss: 122.0988540649414\n",
      "[35 200] loss: 121.39830017089844\n",
      "[35 300] loss: 122.01988983154297\n",
      "[35 400] loss: 121.63909912109375\n",
      "[35 500] loss: 121.87016296386719\n",
      "[35 600] loss: 122.06119537353516\n",
      "[36 100] loss: 121.9077377319336\n",
      "[36 200] loss: 122.23711395263672\n",
      "[36 300] loss: 121.49942016601562\n",
      "[36 400] loss: 121.1550521850586\n",
      "[36 500] loss: 122.13182067871094\n",
      "[36 600] loss: 121.62301635742188\n",
      "[37 100] loss: 122.13858795166016\n",
      "[37 200] loss: 121.33472442626953\n",
      "[37 300] loss: 121.76984405517578\n",
      "[37 400] loss: 121.5070571899414\n",
      "[37 500] loss: 121.44718170166016\n",
      "[37 600] loss: 121.97647094726562\n",
      "[38 100] loss: 121.66768646240234\n",
      "[38 200] loss: 122.25765228271484\n",
      "[38 300] loss: 121.23751831054688\n",
      "[38 400] loss: 121.60407257080078\n",
      "[38 500] loss: 120.86875915527344\n",
      "[38 600] loss: 121.87584686279297\n",
      "[39 100] loss: 120.3424072265625\n",
      "[39 200] loss: 121.76058197021484\n",
      "[39 300] loss: 121.82411193847656\n",
      "[39 400] loss: 121.55071258544922\n",
      "[39 500] loss: 121.79508209228516\n",
      "[39 600] loss: 121.49971771240234\n",
      "[40 100] loss: 121.32160186767578\n",
      "[40 200] loss: 120.93342590332031\n",
      "[40 300] loss: 122.22543334960938\n",
      "[40 400] loss: 122.26801300048828\n",
      "[40 500] loss: 120.63655090332031\n",
      "[40 600] loss: 121.04985809326172\n",
      "[41 100] loss: 120.89414978027344\n",
      "[41 200] loss: 121.23533630371094\n",
      "[41 300] loss: 120.7613525390625\n",
      "[41 400] loss: 122.35005950927734\n",
      "[41 500] loss: 120.6708984375\n",
      "[41 600] loss: 121.9225845336914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42 100] loss: 120.28485107421875\n",
      "[42 200] loss: 120.97447967529297\n",
      "[42 300] loss: 121.48497772216797\n",
      "[42 400] loss: 121.68321228027344\n",
      "[42 500] loss: 121.20040130615234\n",
      "[42 600] loss: 121.50517272949219\n",
      "[43 100] loss: 120.48006439208984\n",
      "[43 200] loss: 120.7411117553711\n",
      "[43 300] loss: 121.43186950683594\n",
      "[43 400] loss: 121.89122009277344\n",
      "[43 500] loss: 120.43087005615234\n",
      "[43 600] loss: 121.86043548583984\n",
      "[44 100] loss: 121.59639739990234\n",
      "[44 200] loss: 120.68988800048828\n",
      "[44 300] loss: 121.0360336303711\n",
      "[44 400] loss: 121.57837677001953\n",
      "[44 500] loss: 121.11994934082031\n",
      "[44 600] loss: 120.20574951171875\n",
      "[45 100] loss: 121.2071762084961\n",
      "[45 200] loss: 121.11624908447266\n",
      "[45 300] loss: 120.86133575439453\n",
      "[45 400] loss: 121.36619567871094\n",
      "[45 500] loss: 120.23641967773438\n",
      "[45 600] loss: 120.95929718017578\n",
      "[46 100] loss: 121.3506088256836\n",
      "[46 200] loss: 120.95436096191406\n",
      "[46 300] loss: 120.62047576904297\n",
      "[46 400] loss: 121.13243103027344\n",
      "[46 500] loss: 120.84373474121094\n",
      "[46 600] loss: 120.4085922241211\n",
      "[47 100] loss: 120.8368911743164\n",
      "[47 200] loss: 120.7237548828125\n",
      "[47 300] loss: 120.47025299072266\n",
      "[47 400] loss: 120.93754577636719\n",
      "[47 500] loss: 120.6988525390625\n",
      "[47 600] loss: 121.1310806274414\n",
      "[48 100] loss: 120.6347427368164\n",
      "[48 200] loss: 120.31613159179688\n",
      "[48 300] loss: 120.47862243652344\n",
      "[48 400] loss: 121.6745834350586\n",
      "[48 500] loss: 120.97447967529297\n",
      "[48 600] loss: 120.31694030761719\n",
      "[49 100] loss: 120.18924713134766\n",
      "[49 200] loss: 120.19961547851562\n",
      "[49 300] loss: 120.71041107177734\n",
      "[49 400] loss: 120.3757095336914\n",
      "[49 500] loss: 121.35250091552734\n",
      "[49 600] loss: 120.99931335449219\n",
      "[50 100] loss: 120.42704772949219\n",
      "[50 200] loss: 120.74874877929688\n",
      "[50 300] loss: 120.94072723388672\n",
      "[50 400] loss: 120.13632202148438\n",
      "[50 500] loss: 120.54022216796875\n",
      "[50 600] loss: 120.68153381347656\n",
      "[51 100] loss: 120.26251983642578\n",
      "[51 200] loss: 120.8197021484375\n",
      "[51 300] loss: 120.57740783691406\n",
      "[51 400] loss: 120.76107025146484\n",
      "[51 500] loss: 120.45927429199219\n",
      "[51 600] loss: 120.07994842529297\n",
      "[52 100] loss: 119.58977508544922\n",
      "[52 200] loss: 121.49004364013672\n",
      "[52 300] loss: 119.811767578125\n",
      "[52 400] loss: 120.95525360107422\n",
      "[52 500] loss: 120.65886688232422\n",
      "[52 600] loss: 120.15294647216797\n",
      "[53 100] loss: 120.29576873779297\n",
      "[53 200] loss: 119.75437927246094\n",
      "[53 300] loss: 120.4522705078125\n",
      "[53 400] loss: 120.7214584350586\n",
      "[53 500] loss: 120.06791687011719\n",
      "[53 600] loss: 120.72200012207031\n",
      "[54 100] loss: 120.00596618652344\n",
      "[54 200] loss: 120.2286605834961\n",
      "[54 300] loss: 120.37098693847656\n",
      "[54 400] loss: 120.05686950683594\n",
      "[54 500] loss: 119.69068145751953\n",
      "[54 600] loss: 121.1548080444336\n",
      "[55 100] loss: 120.146240234375\n",
      "[55 200] loss: 119.91597747802734\n",
      "[55 300] loss: 120.1976547241211\n",
      "[55 400] loss: 120.6130142211914\n",
      "[55 500] loss: 120.1765365600586\n",
      "[55 600] loss: 120.19609832763672\n",
      "[56 100] loss: 119.7983169555664\n",
      "[56 200] loss: 120.04246520996094\n",
      "[56 300] loss: 120.37919616699219\n",
      "[56 400] loss: 120.21583557128906\n",
      "[56 500] loss: 120.01849365234375\n",
      "[56 600] loss: 120.46512603759766\n",
      "[57 100] loss: 120.1811294555664\n",
      "[57 200] loss: 120.6107406616211\n",
      "[57 300] loss: 120.21099090576172\n",
      "[57 400] loss: 119.9706802368164\n",
      "[57 500] loss: 119.28460693359375\n",
      "[57 600] loss: 120.37494659423828\n",
      "[58 100] loss: 120.2087631225586\n",
      "[58 200] loss: 118.73941802978516\n",
      "[58 300] loss: 120.51602172851562\n",
      "[58 400] loss: 119.81704711914062\n",
      "[58 500] loss: 120.64289093017578\n",
      "[58 600] loss: 120.20623016357422\n",
      "[59 100] loss: 120.30029296875\n",
      "[59 200] loss: 119.45951843261719\n",
      "[59 300] loss: 119.82808685302734\n",
      "[59 400] loss: 119.95774841308594\n",
      "[59 500] loss: 120.00650787353516\n",
      "[59 600] loss: 119.90718841552734\n",
      "[60 100] loss: 120.02494049072266\n",
      "[60 200] loss: 120.1482162475586\n",
      "[60 300] loss: 120.06551361083984\n",
      "[60 400] loss: 119.50345611572266\n",
      "[60 500] loss: 119.39241790771484\n",
      "[60 600] loss: 120.18929290771484\n",
      "[61 100] loss: 119.81694793701172\n",
      "[61 200] loss: 119.5491943359375\n",
      "[61 300] loss: 119.16690063476562\n",
      "[61 400] loss: 120.47443389892578\n",
      "[61 500] loss: 120.20323944091797\n",
      "[61 600] loss: 119.87493133544922\n",
      "[62 100] loss: 119.57150268554688\n",
      "[62 200] loss: 119.95711517333984\n",
      "[62 300] loss: 120.21065521240234\n",
      "[62 400] loss: 120.22129821777344\n",
      "[62 500] loss: 119.03836822509766\n",
      "[62 600] loss: 119.43433380126953\n",
      "[63 100] loss: 119.21722412109375\n",
      "[63 200] loss: 119.51457977294922\n",
      "[63 300] loss: 120.12728881835938\n",
      "[63 400] loss: 119.68585968017578\n",
      "[63 500] loss: 119.892822265625\n",
      "[63 600] loss: 119.81512451171875\n",
      "[64 100] loss: 120.13274383544922\n",
      "[64 200] loss: 119.29340362548828\n",
      "[64 300] loss: 119.16960906982422\n",
      "[64 400] loss: 119.02721405029297\n",
      "[64 500] loss: 119.97222137451172\n",
      "[64 600] loss: 120.33464813232422\n",
      "[65 100] loss: 120.2172622680664\n",
      "[65 200] loss: 119.2294692993164\n",
      "[65 300] loss: 119.58714294433594\n",
      "[65 400] loss: 119.30559539794922\n",
      "[65 500] loss: 119.46540069580078\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(cvae.parameters())\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    for it, (images, labels) in enumerate(trainloader):\n",
    "\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels = nn.functional.one_hot(labels, conditional_size).float()\n",
    "\n",
    "        out = cvae(images, labels)\n",
    "        loss = -models.CVAE.variational_objective(images, *out)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss\n",
    "        if it % n_batches_print == n_batches_print-1:\n",
    "            print(f'[{epoch+1} {it+1}] loss: {running_loss/n_batches_print}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "torch.save(vae.state_dict(), \"./cvae.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (encoder): Encoder(\n",
      "    (mlp): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=784, out_features=500, bias=True)\n",
      "        (1): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (fc_mu): Linear(in_features=500, out_features=2, bias=True)\n",
      "    (fc_logvariance): Linear(in_features=500, out_features=2, bias=True)\n",
      "  )\n",
      "  (stochasticlayer): StochasticLayer()\n",
      "  (decoder): Decoder(\n",
      "    (mlp): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=500, bias=True)\n",
      "        (1): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (last_layer): Sequential(\n",
      "      (0): Linear(in_features=500, out_features=784, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vae = models.VAE(enc_sizes,dec_sizes)\n",
    "\n",
    "if not train:\n",
    "    vae.load_state_dict(torch.load('vae.pt'))\n",
    "\n",
    "vae.to(device)\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "        \n",
    "    optimizer = torch.optim.Adam(vae.parameters())\n",
    "\n",
    "    n_epochs = 200\n",
    "    n_batches_print = 100\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        running_loss = 0\n",
    "\n",
    "        for it, (images, labels) in enumerate(trainloader):\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            mu, logvariance, z, y = vae(images)\n",
    "            loss = -models.VAE.variational_objective(images, mu, logvariance, z, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss\n",
    "            if it % n_batches_print == n_batches_print-1:\n",
    "                print(f'[{epoch+1} {it+1}] loss: {running_loss/n_batches_print}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "    torch.save(vae.state_dict(), \"./vae.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set ELBO = -130.44461059570312\n",
      "Test set ELBO     = -151.35696411132812\n"
     ]
    }
   ],
   "source": [
    "def test_model(vae, dataloader, device):\n",
    "    elbo_avg = 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        mu, logvariance, z, y = vae(x)\n",
    "        elbo_avg += models.VAE.variational_objective(x, mu, logvariance, z, y)\n",
    "    return elbo_avg/len(dataloader)\n",
    "\n",
    "\n",
    "print(f\"Training set ELBO = {test_model(vae, trainloader, device)}\")\n",
    "print(f\"Test set ELBO     = {test_model(vae, testloader, device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    plt.imshow(np.transpose(img.detach().numpy(), (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent space visualization \n",
    "#     encode samples and visualize distribution in latent space\n",
    "\n",
    "\n",
    "def latent_space_visualization(dataset, saveas=None, n_samples_per_class=100, device=torch.device('cuda:0')):\n",
    "\n",
    "    import itertools\n",
    "\n",
    "    viridis = plt.cm.get_cmap('viridis', 10)\n",
    "\n",
    "    for c in range(10):\n",
    "\n",
    "        x = itertools.islice(filter(lambda x: x[1] == c, dataset), n_samples_per_class)\n",
    "        x = torch.stack(list(zip(*x))[0], 0)\n",
    "        x = x.to(device)\n",
    "\n",
    "        mu, _ = vae.encoder(x)\n",
    "        mu = mu.cpu().detach().numpy()\n",
    "\n",
    "        color = np.ones(n_samples_per_class) * c\n",
    "        color = color.astype(np.uint8)\n",
    "\n",
    "        plt.scatter(mu[:,0],mu[:,1],c=np.tile(np.array(viridis(c/10)),(n_samples_per_class,1)),alpha=0.5,label=f'{c}')\n",
    "\n",
    "    plt.title('latent space visualization')\n",
    "    plt.xlim((-6,6))\n",
    "    plt.ylim((-6,6))\n",
    "    plt.xlabel('z1')\n",
    "    plt.ylabel('z2')\n",
    "    plt.legend()\n",
    "\n",
    "    if saveas:\n",
    "        plt.savefig(saveas)\n",
    "    \n",
    "    \n",
    "latent_space_visualization(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(a,b,c):\n",
    "    print(a,b,c)\n",
    "    \n",
    "    \n",
    "l = [2,3]\n",
    "foo(1,*l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:misc_impl] *",
   "language": "python",
   "name": "conda-env-misc_impl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
