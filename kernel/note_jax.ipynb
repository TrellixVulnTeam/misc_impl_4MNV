{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jax\n",
    "\n",
    "- quickstart https://jax.readthedocs.io/en/latest/notebooks/quickstart.html\n",
    "- common gotchas https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html\n",
    "    - works on pure functions only, avoid side-effect, global variables\n",
    "    - on mutation (keep variables pure!), mutate via `index, index_add,index_update`\n",
    "    - out-of-bounds indexing do not raise error, but outputs last value of array \n",
    "    - random number generation via keys, made explicit\n",
    "    - `@jit` is compiled to `ShapedArray` of fixed size, so compiled function can be applied to array of arbitrary values. Use `static_argnums` then can specify trace on concrete values of some arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.372111    0.2642311  -0.18252774 -0.7368198  -0.44030386 -0.15214427\n",
      " -0.6713536  -0.59086424  0.73168874  0.56730247]\n",
      "95.9 ms ± 1.4 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "130 ms ± 1.08 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# quickstart https://jax.readthedocs.io/en/latest/notebooks/quickstart.html\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "x = random.normal(key, (10,))\n",
    "print(x)\n",
    "\n",
    "## multiply big matrices\n",
    "size = 3000\n",
    "x = random.normal(key, (size, size), dtype=jnp.float32)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()  # runs on the GPU\n",
    "# 95.9 ms ± 1.4 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
    "\n",
    "# works on np arrays, slower since needs to put data to GPU\n",
    "import numpy as np\n",
    "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()\n",
    "# 130 ms ± 1.08 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
    "\n",
    "# use `device_put` to put onto GPU first\n",
    "from jax import device_put\n",
    "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
    "x = device_put(x)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()\n",
    "# 96.1 ms ± 2.05 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
    "\n",
    "## Use @jit to compile multiple operations using XLA\n",
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "    return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "x = random.normal(key, (1000000,))\n",
    "%timeit selu(x).block_until_ready()\n",
    "# 2.62 ms ± 122 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
    "selu_jit = jit(selu)\n",
    "%timeit selu_jit(x).block_until_ready()\n",
    "# 598 µs ± 11.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
    "\n",
    "\n",
    "# Use `grad` to take derivatives\n",
    "def sum_logistic(x):\n",
    "    return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
    "x_small = jnp.arange(3.)\n",
    "derivative_fn = grad(sum_logistic)\n",
    "print(derivative_fn(x_small))\n",
    "# [0.25       0.19661197 0.10499357]\n",
    "def first_finite_differences(f, x):\n",
    "    eps = 1e-3\n",
    "    return jnp.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps)\n",
    "                   for v in jnp.eye(len(x))])\n",
    "print(first_finite_differences(sum_logistic, x_small))\n",
    "# [0.24998187 0.1964569  0.10502338]\n",
    "\n",
    "# Use `jax.vjp` for reverse-mode vector-jacobian products\n",
    "# Use `jax.jvp` for forward-mode jacobian-vector products\n",
    "from jax import jacfwd, jacrev\n",
    "def hessian(fun):\n",
    "    return jit(jacfwd(jacrev(fun)))\n",
    "\n",
    "## auto-vectorization with `vmap`\n",
    "mat = random.normal(key, (150, 100))\n",
    "batched_x = random.normal(key, (10, 100))\n",
    "\n",
    "def apply_matrix(v):\n",
    "    return jnp.dot(mat, v)\n",
    "def naively_batched_apply_matrix(v_batched):\n",
    "    return jnp.stack([apply_matrix(v) for v in v_batched])\n",
    "print('Naively batched')\n",
    "%timeit naively_batched_apply_matrix(batched_x).block_until_ready()\n",
    "@jit\n",
    "def batched_apply_matrix(v_batched):\n",
    "    return jnp.dot(v_batched, mat.T)\n",
    "print('Manually batched')\n",
    "%timeit batched_apply_matrix(batched_x).block_until_ready()\n",
    "@jit\n",
    "def vmap_batched_apply_matrix(v_batched):\n",
    "    return vmap(apply_matrix)(v_batched)\n",
    "print('Auto-vectorized with vmap')\n",
    "%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()\n",
    "\n",
    "# Naively batched\n",
    "# 2.92 ms ± 98.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
    "# Manually batched\n",
    "# 112 µs ± 1.29 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
    "# Auto-vectorized with vmap\n",
    "# 105 µs ± 4.27 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Neurips 2019 Jax tutorial\n",
    "# https://slideslive.com/38923687/jax-accelerated-machinelearning-research-via-composable-function-transformations-in-python\n",
    "\n",
    "\n",
    "x = jnp.array([1,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MNIST\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import numpy.random as npr\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, grad, random\n",
    "from jax.experimental import optimizers\n",
    "from jax.experimental import stax\n",
    "from jax.experimental.stax import Dense, Relu, LogSoftmax\n",
    "import datasets\n",
    "\n",
    "\n",
    "def loss(params, batch):\n",
    "    inputs, targets = batch\n",
    "    preds = predict(params, inputs)\n",
    "    return -jnp.mean(jnp.sum(preds * targets, axis=1))\n",
    "\n",
    "def accuracy(params, batch):\n",
    "    inputs, targets = batch\n",
    "    target_class = jnp.argmax(targets, axis=1)\n",
    "    predicted_class = jnp.argmax(predict(params, inputs), axis=1)\n",
    "    return jnp.mean(predicted_class == target_class)\n",
    "\n",
    "init_random_params, predict = stax.serial(\n",
    "    Dense(1024), Relu,\n",
    "    Dense(1024), Relu,\n",
    "    Dense(10), LogSoftmax)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    rng = random.PRNGKey(0)\n",
    "\n",
    "    step_size = 0.001\n",
    "    num_epochs = 10\n",
    "    batch_size = 128\n",
    "    momentum_mass = 0.9\n",
    "\n",
    "    train_images, train_labels, test_images, test_labels = datasets.mnist()\n",
    "    num_train = train_images.shape[0]\n",
    "    num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "    num_batches = num_complete_batches + bool(leftover)\n",
    "\n",
    "    def data_stream():\n",
    "        rng = npr.RandomState(0)\n",
    "    while True:\n",
    "        perm = rng.permutation(num_train)\n",
    "        for i in range(num_batches):\n",
    "            batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
    "            yield train_images[batch_idx], train_labels[batch_idx]\n",
    "    batches = data_stream()\n",
    "\n",
    "    opt_init, opt_update, get_params = optimizers.momentum(step_size, mass=momentum_mass)\n",
    "\n",
    "    @jit\n",
    "    def update(i, opt_state, batch):\n",
    "        params = get_params(opt_state)\n",
    "        return opt_update(i, grad(loss)(params, batch), opt_state)\n",
    "\n",
    "    _, init_params = init_random_params(rng, (-1, 28 * 28))\n",
    "    opt_state = opt_init(init_params)\n",
    "    itercount = itertools.count()\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    for _ in range(num_batches):\n",
    "        opt_state = update(next(itercount), opt_state, next(batches))\n",
    "    epoch_time = time.time() - start_time\n",
    "\n",
    "    params = get_params(opt_state)\n",
    "    train_acc = accuracy(params, (train_images, train_labels))\n",
    "    test_acc = accuracy(params, (test_images, test_labels))\n",
    "    print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "    print(\"Training set accuracy {}\".format(train_acc))\n",
    "    print(\"Test set accuracy {}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:misc_impl] *",
   "language": "python",
   "name": "conda-env-misc_impl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
