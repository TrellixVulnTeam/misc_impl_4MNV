{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy case, \n",
    "\n",
    "$X\\sim N(\\mu,\\Sigma)$, $f(X)=[1,0]^T X+b$, $g(X)=BX+b$. Solve the following optimization problem \n",
    "\n",
    "$$ \\min_{B,a,b} \\text{HSIC}(f(X),g(X)) - (B - [1,0])^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.getipython import get_ipython\n",
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "get_ipython().run_line_magic('autoreload', '2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from jaxkern import (\n",
    "    rbf_kernel, linear_kernel, estimate_sigma_median, hsic, mmd, squared_l2_norm)\n",
    "\n",
    "import jax\n",
    "from jax.experimental import optimizers\n",
    "from jax import grad, random\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_kde(X):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(6,6)\n",
    "    lim = np.min(X)-1, np.max(X)+1\n",
    "    XX,YY = np.meshgrid(\n",
    "        np.linspace(lim[0], lim[1], 100),\n",
    "        np.linspace(lim[0], lim[1], 100))\n",
    "    XY = np.vstack([XX.ravel(), YY.ravel()])\n",
    "    kernel = stats.gaussian_kde(X.T)\n",
    "    Z = kernel(XY).reshape(XX.shape)\n",
    "    # ax.scatter(X[:,0], X[:,1], c='k', s=3)\n",
    "    ax.imshow(Z, cmap='Blues',\n",
    "              extent=[lim[0], lim[1], lim[0], lim[1]])\n",
    "    ax.set_xlim([lim[0], lim[1]])\n",
    "    ax.set_ylim([lim[0], lim[1]])\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "d = 2\n",
    "lr = .2\n",
    "num_steps = 1000\n",
    "batch_size = 128\n",
    "\n",
    "npr.seed(0)\n",
    "mu  = np.array([1,1])\n",
    "cov = np.array([[1,0], [0,1]])\n",
    "Xdist = stats.multivariate_normal(mu, cov)\n",
    "X = Xdist.rvs(size=(n,))\n",
    "X = jax.device_put(X)\n",
    "\n",
    "sigma = estimate_sigma_median(X)\n",
    "gamma = 1/(2*(sigma**2))\n",
    "print(f'sigma={sigma}')\n",
    "print(f'gamma={gamma}')\n",
    "\n",
    "def kernel(X,Y):\n",
    "    return rbf_kernel(X,Y,gamma=gamma)\n",
    "\n",
    "a1 = jnp.array([[.5,.5]], dtype=jnp.float32)\n",
    "params = {\n",
    "    'a2': a1.copy(),\n",
    "}\n",
    "\n",
    "def f(params, X):\n",
    "    return jnp.dot(a1,X)\n",
    "def g(params, X):\n",
    "    return jnp.dot(params['a2'],X)\n",
    "f = jax.vmap(f, (None, 0), 0)\n",
    "g = jax.vmap(g, (None, 0), 0)\n",
    "\n",
    "def loss_fn(params, X):\n",
    "    fX = f(params, X)\n",
    "    gX = g(params, X)\n",
    "    loss = hsic(fX, gX, kernel, kernel) + squared_l2_norm(jnp.vstack((a1,params['a2']))@mu.T - mu.T)\n",
    "    return loss\n",
    "loss_fn = jax.jit(loss_fn)\n",
    "grad_fn = jax.jit(grad(loss_fn))\n",
    "\n",
    "\n",
    "num_complete_batches, leftover = divmod(n, batch_size)\n",
    "num_batches = num_complete_batches + bool(leftover)\n",
    "def data_stream():\n",
    "    rng = npr.RandomState(0)\n",
    "    while True:\n",
    "        perm = rng.permutation(n)\n",
    "        for i in range(num_batches):\n",
    "            batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
    "            yield X[batch_idx]\n",
    "batches = data_stream()\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.sgd(lr)\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "import itertools\n",
    "itercount = itertools.count()\n",
    "\n",
    "fig, axs = plt.subplots(())\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for i in range(num_steps):\n",
    "    start_time = time.time()\n",
    "    for j in range(num_batches):\n",
    "        batch = next(batches)\n",
    "        params = get_params(opt_state)\n",
    "        fX = f(params, batch)\n",
    "        gX = g(params, batch)\n",
    "        fgX = jnp.hstack([fX, gX])\n",
    "        l = loss_fn(params, batch) \n",
    "        params_grad = grad_fn(params, batch)\n",
    "        opt_state = opt_update(next(itercount), params_grad, opt_state)\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    if i%50 == 0:\n",
    "        print(f'[{i:3}] time={epoch_time:4.4f}\\t loss={l:5.8f}\\t'\n",
    "              f'a2={params[\"a2\"]}')\n",
    "        \n",
    "        A = jnp.vstack((a1, params['a2']))\n",
    "        ycov = A@cov@A.T\n",
    "        print(ycov)\n",
    "        \n",
    "        plt.scatter(fgX[:,0], fgX[:,1], label=i)\n",
    "        plt.xlim((-5,5))\n",
    "        plt.ylim((-5,5))\n",
    "    \n",
    "plt.legend()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = get_params(opt_state)\n",
    "print(params)\n",
    "\n",
    "fX = f(params, X)\n",
    "gX = g(params, X)\n",
    "fgX = jnp.hstack([fX, gX])\n",
    "\n",
    "\n",
    "\n",
    "l = loss_fn(params, X) \n",
    "\n",
    "params_grad = grad_fn(params, X)\n",
    "l, params_grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:misc_impl] *",
   "language": "python",
   "name": "conda-env-misc_impl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
