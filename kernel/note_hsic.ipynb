{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy case, \n",
    "\n",
    "$X\\sim N(\\mu,\\Sigma)$, $f(X)=[1,0]^T X+b$, $g(X)=BX+b$. Solve the following optimization problem \n",
    "\n",
    "$$ \\min_{B,a,b} \\text{HSIC}(f(X),g(X)) - B^TB $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.getipython import get_ipython\n",
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "get_ipython().run_line_magic('autoreload', '2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from kernel import gauss_kernel, linear_kernel\n",
    "from mmd import mmd\n",
    "from hsic import hsic\n",
    "\n",
    "import jax\n",
    "from jax.experimental import optimizers\n",
    "from jax import grad, random\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_kde(X):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(6,6)\n",
    "    lim = np.min(X)-1, np.max(X)+1\n",
    "    XX,YY = np.meshgrid(\n",
    "        np.linspace(lim[0], lim[1], 100),\n",
    "        np.linspace(lim[0], lim[1], 100))\n",
    "    XY = np.vstack([XX.ravel(), YY.ravel()])\n",
    "    kernel = stats.gaussian_kde(X.T)\n",
    "    Z = kernel(XY).reshape(XX.shape)\n",
    "    # ax.scatter(X[:,0], X[:,1], c='k', s=3)\n",
    "    ax.imshow(Z, cmap='Blues',\n",
    "              extent=[lim[0], lim[1], lim[0], lim[1]])\n",
    "    ax.set_xlim([lim[0], lim[1]])\n",
    "    ax.set_ylim([lim[0], lim[1]])\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Taken from https://github.com/IPL-UV/jaxkern\n",
    "\n",
    "def sqeuclidean_distance(x, y):\n",
    "    return jnp.sum((x - y) ** 2)\n",
    "\n",
    "def euclidean_distance(x, y):\n",
    "    return jnp.sqrt(sqeuclidean_distance(x, y))\n",
    "\n",
    "def distmat(func, x, y):\n",
    "    return jax.vmap(lambda x1: jax.vmap(lambda y1: func(x1, y1))(y))(x)\n",
    "\n",
    "def cdist_sqeuclidean(x, y):\n",
    "    \"\"\" Squared euclidean distance matrix \"\"\"\n",
    "    return distmat(sqeuclidean_distance, x, y)\n",
    "\n",
    "def cdist_euclidean(x, y):\n",
    "    \"\"\" Squared euclidean distance matrix \"\"\"\n",
    "    return distmat(euclidean_distance, x, y)\n",
    "\n",
    "def rbf_kernel(X, Y, gamma=1.):\n",
    "    \"\"\"Radial Basis Function Kernel\n",
    "        \n",
    "            k(x,y)=exp(-\\gamma*||x-y||**2)\n",
    "                where \\gamma   = 1/(2*sigma^2)\n",
    "                      \\sigma^2 = 1/(2*\\gamma)\n",
    " \n",
    "        X, Y    (n, d)\n",
    "        Returns kernel matrix of size (n, n)\n",
    "    \"\"\"\n",
    "    return jnp.exp(-gamma*cdist_sqeuclidean(X, Y))\n",
    "\n",
    "def linear_kernel(X, Y):\n",
    "    return jnp.dot(X, Y.T)\n",
    "\n",
    "def estimate_sigma_median(X):\n",
    "    \"\"\"Estimate sigma using the median heuristic\n",
    "            bandwidth = median(l2dist.([X,Y]))\n",
    "                with \\sigma = \\sqrt(bandwidth/2)\n",
    "        \n",
    "        X, Y    (n, d)\n",
    "    \"\"\"\n",
    "    D = cdist_euclidean(X, X)\n",
    "    D = D[jnp.nonzero(D)]\n",
    "    bandwidth = jnp.median(D)\n",
    "    sigma = jnp.sqrt(bandwidth/2)\n",
    "    return sigma\n",
    "\n",
    "def hsic(X, Y, k, l):\n",
    "    \"\"\" Computes empirical HSIC = tr(KHLH)\n",
    "            where H is the centering matrix\n",
    "    \"\"\"\n",
    "    K = k(X, Y)\n",
    "    L = l(X, Y)\n",
    "    m = len(K)\n",
    "    H = jnp.eye(m) - 1/m\n",
    "    statistic = jnp.trace(K.dot(H).dot(L).dot(H)) / (m**2)\n",
    "    return statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "d = 2\n",
    "lr = 1\n",
    "num_steps = 40\n",
    "batch_size = 128\n",
    "\n",
    "npr.seed(0)\n",
    "mu  = np.array([0,0])\n",
    "cov = np.array([[1,0], [0,1]])\n",
    "Xdist = stats.multivariate_normal(mu, cov)\n",
    "X = Xdist.rvs(size=(n,))\n",
    "X = jax.device_put(X)\n",
    "\n",
    "sigma = estimate_sigma_median(X)\n",
    "gamma = 1/(2*(sigma**2))\n",
    "print(f'sigma={sigma}')\n",
    "print(f'gamma={gamma}')\n",
    "\n",
    "def kernel(X,Y):\n",
    "    return rbf_kernel(X,Y,gamma=gamma)\n",
    "\n",
    "A = jnp.array([[1,0]], dtype=jnp.float32)\n",
    "params = {\n",
    "    'B': jnp.array([[1,0]], dtype=jnp.float32),\n",
    "#     'a': random.normal(key, (1,)),\n",
    "#     'b': random.normal(key, (1,)),\n",
    "}\n",
    "\n",
    "def f(params, X):\n",
    "    return jnp.dot(A,X)\n",
    "def g(params, X):\n",
    "    return jnp.dot(params['B'],X)\n",
    "f = jax.vmap(f, (None, 0), 0)\n",
    "g = jax.vmap(g, (None, 0), 0)\n",
    "\n",
    "def loss_fn(params, X):\n",
    "    fX = f(params, X)\n",
    "    gX = g(params, X)\n",
    "#     print(hsic(fX, gX, kernel, kernel), max(-(params['B']@params['B'].T)[0,0], -1))\n",
    "    loss = hsic(fX, gX, kernel, kernel) + max(-(params['B']@params['B'].T)[0,0], -1)\n",
    "    return loss\n",
    "\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.sgd(lr)\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for i in range(num_steps):\n",
    "    start_time = time.time()\n",
    "    params = get_params(opt_state)\n",
    "    \n",
    "    fX = f(params, X)\n",
    "    gX = g(params, X)\n",
    "    fgX = jnp.hstack([fX, gX])\n",
    "    plt.scatter(fgX[:,0], fgX[:,1], label=i)\n",
    "    plt.xlim((-5,5))\n",
    "    plt.ylim((-5,5))\n",
    "    \n",
    "    l = loss_fn(params, X)\n",
    "    grd = grad(loss_fn)(get_params(opt_state), X)\n",
    "    opt_state = opt_update(i, grd, opt_state)\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if i%5 == 0:\n",
    "        print(f'[{i:3}] time={epoch_time:4.4f}\\t loss={l:5.8f}\\t A^TB={jnp.dot(A,params[\"B\"].T)} B={params[\"B\"]}')\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "plt.legend()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params['B'])\n",
    "# print(params['a'])\n",
    "# print(params['b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-(params['B']@params['B'].T)[0,0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:misc_impl] *",
   "language": "python",
   "name": "conda-env-misc_impl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
