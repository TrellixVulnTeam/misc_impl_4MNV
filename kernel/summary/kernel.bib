
@article{grettonKernelTwoSampleTest2012,
	title = {A Kernel Two-Sample Test},
	volume = {13},
	url = {http://jmlr.org/papers/v13/gretton12a.html},
	abstract = {We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions.  Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space ({RKHS}), and is called the maximum mean discrepancy ({MMD}).  We present two distribution-free tests based on large deviation bounds for the {MMD}, and a third test based on the asymptotic distribution of this statistic.  The {MMD} can be computed in quadratic time, although efficient linear time approximations are available.  Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an {RKHS}.  We apply our two-sample tests  to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly.  Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
	pages = {723--773},
	number = {25},
	journaltitle = {Journal of Machine Learning Research},
	author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Schölkopf, Bernhard and Smola, Alexander},
	urldate = {2020-06-21},
	date = {2012},
	keywords = {good, tbr},
	file = {Gretton et al_2012_A Kernel Two-Sample Test.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gretton et al_2012_A Kernel Two-Sample Test.pdf:application/pdf}
}

@article{sutherlandGenerativeModelsModel2019,
	title = {Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy},
	url = {http://arxiv.org/abs/1611.04488},
	abstract = {We propose a method to optimize the representation and distinguishability of samples from two probability distributions, by maximizing the estimated power of a statistical test based on the maximum mean discrepancy ({MMD}). This optimized {MMD} is applied to the setting of unsupervised learning by generative adversarial networks ({GAN}), in which a model attempts to generate realistic samples, and a discriminator attempts to tell these apart from data samples. In this context, the {MMD} may be used in two roles: first, as a discriminator, either directly on the samples, or on features of the samples. Second, the {MMD} can be used to evaluate the performance of a generative model, by testing the model's samples against a reference data set. In the latter role, the optimized {MMD} is particularly helpful, as it gives an interpretable indication of how the model and data distributions differ, even in cases where individual model samples are not easily distinguished either by eye or by classifier.},
	journaltitle = {{arXiv}:1611.04488 [cs, stat]},
	author = {Sutherland, Dougal J. and Tung, Hsiao-Yu and Strathmann, Heiko and De, Soumyajit and Ramdas, Aaditya and Smola, Alex and Gretton, Arthur},
	urldate = {2020-06-22},
	date = {2019-06-06},
	eprinttype = {arxiv},
	eprint = {1611.04488},
	keywords = {read},
	file = {Sutherland et al_2019_Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy.pdf:/Users/wpq/Dropbox (MIT)/zotero/Sutherland et al_2019_Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy.pdf:application/pdf}
}

@incollection{jaakkolaExploitingGenerativeModels1999,
	title = {Exploiting Generative Models in Discriminative Classifiers},
	url = {http://papers.nips.cc/paper/1520-exploiting-generative-models-in-discriminative-classifiers.pdf},
	pages = {487--493},
	booktitle = {Advances in Neural Information Processing Systems 11},
	publisher = {{MIT} Press},
	author = {Jaakkola, Tommi and Haussler, David},
	editor = {Kearns, M. J. and Solla, S. A. and Cohn, D. A.},
	urldate = {2020-08-25},
	date = {1999},
	keywords = {tbr},
	file = {Jaakkola_Haussler_1999_Exploiting Generative Models in Discriminative Classifiers.pdf:/Users/wpq/Dropbox (MIT)/zotero/Jaakkola_Haussler_1999_Exploiting Generative Models in Discriminative Classifiers.pdf:application/pdf}
}

@article{sriperumbudurHilbertSpaceEmbeddings2010,
	title = {Hilbert Space Embeddings and Metrics on Probability Measures},
	abstract = {A Hilbert space embedding for probability measures has recently been proposed, with applications including dimensionality reduction, homogeneity testing, and independence testing. This embedding represents any probability measure as a mean element in a reproducing kernel Hilbert space ({RKHS}). A pseudometric on the space of probability measures can be deﬁned as the distance between distribution embeddings: we denote this as γk, indexed by the kernel function k that deﬁnes the inner product in the {RKHS}.},
	pages = {45},
	author = {Sriperumbudur, Bharath K and Gretton, Arthur and Fukumizu, Kenji and Scholkopf, Bernhard and Lanckriet, Gert R G},
	date = {2010},
	langid = {english},
	keywords = {tbr},
	file = {Sriperumbudur et al_2010_Hilbert Space Embeddings and Metrics on Probability Measures.pdf:/Users/wpq/Dropbox (MIT)/zotero/Sriperumbudur et al_2010_Hilbert Space Embeddings and Metrics on Probability Measures.pdf:application/pdf}
}

@inproceedings{leslieSPECTRUMKERNELSTRING2001,
	location = {Kauai, Hawaii, {USA}},
	title = {{THE} {SPECTRUM} {KERNEL}: A {STRING} {KERNEL} {FOR} {SVM} {PROTEIN} {CLASSIFICATION}},
	isbn = {978-981-02-4777-5 978-981-279-962-3},
	url = {http://www.worldscientific.com/doi/abs/10.1142/9789812799623_0053},
	doi = {10.1142/9789812799623_0053},
	shorttitle = {{THE} {SPECTRUM} {KERNEL}},
	eventtitle = {Proceedings of the Pacific Symposium},
	pages = {564--575},
	booktitle = {Biocomputing 2002},
	publisher = {{WORLD} {SCIENTIFIC}},
	author = {Leslie, Christina and Eskin, Eleazar and Noble, William Stafford},
	urldate = {2020-12-23},
	date = {2001-12},
	langid = {english},
	keywords = {tbr},
	file = {Leslie et al_2001_THE SPECTRUM KERNEL - A STRING KERNEL FOR SVM PROTEIN CLASSIFICATION.pdf:/Users/wpq/Dropbox (MIT)/zotero/Leslie et al_2001_THE SPECTRUM KERNEL - A STRING KERNEL FOR SVM PROTEIN CLASSIFICATION.pdf:application/pdf}
}

@inproceedings{choLargeMarginClassificationHyperbolic2019,
	title = {Large-Margin Classification in Hyperbolic Space},
	url = {http://proceedings.mlr.press/v89/cho19a.html},
	abstract = {Representing data in hyperbolic space can effectively capture latent hierarchical relationships. To enable accurate classification of points in hyperbolic space while respecting their hyperbolic ge...},
	eventtitle = {The 22nd International Conference on Artificial Intelligence and Statistics},
	pages = {1832--1840},
	booktitle = {The 22nd International Conference on Artificial Intelligence and Statistics},
	publisher = {{PMLR}},
	author = {Cho, Hyunghoon and {DeMeo}, Benjamin and Peng, Jian and Berger, Bonnie},
	urldate = {2020-12-24},
	date = {2019-04-11},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	keywords = {tbr},
	file = {Cho et al_2019_Large-Margin Classification in Hyperbolic Space.pdf:/Users/wpq/Dropbox (MIT)/zotero/Cho et al_2019_Large-Margin Classification in Hyperbolic Space.pdf:application/pdf}
}

@article{ngCs229kernel2019,
	title = {cs229-kernel},
	url = {http://cs229.stanford.edu/summer2020/cs229-notes3.pdf},
	author = {Ng, Andrew},
	date = {2019},
	keywords = {read},
	file = {Ng_2019_cs229-kernel.pdf:/Users/wpq/Dropbox (MIT)/zotero/Ng_2019_cs229-kernel.pdf:application/pdf}
}

@article{rahimiRandomFeaturesLargeScale2007,
	title = {Random Features for Large-Scale Kernel Machines},
	abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. Our randomized features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user speciﬁed shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classiﬁcation and regression tasks linear machine learning algorithms that use these features outperform state-of-the-art large-scale kernel machines.},
	pages = {8},
	author = {Rahimi, Ali and Recht, Ben},
	date = {2007},
	langid = {english},
	keywords = {read},
	file = {Rahimi_Recht_2007_Random Features for Large-Scale Kernel Machines.pdf:/Users/wpq/Dropbox (MIT)/zotero/Rahimi_Recht_2007_Random Features for Large-Scale Kernel Machines.pdf:application/pdf}
}

@article{vertPrimerKernelMethods2004,
	title = {A primer on kernel methods},
	pages = {42},
	author = {Vert, Jean-Philippe and Tsuda, Koji and Scholkopf, Bernhard},
	date = {2004},
	langid = {english},
	keywords = {read},
	file = {Vert et al_2004_A primer on kernel methods.pdf:/Users/wpq/Dropbox (MIT)/zotero/Vert et al_2004_A primer on kernel methods.pdf:application/pdf}
}

@article{boserTrainingAlgorithmOptimal1992,
	title = {A Training Algorithm for Optimal Margin Classifiers},
	author = {Boser, Bernhard and Guyon, Isabelle and Vapnik, Vladimir},
	date = {1992},
	keywords = {tbr},
	file = {Boser et al_1992_A Training Algorithm for Optimal Margin Classifiers.pdf:/Users/wpq/Dropbox (MIT)/zotero/Boser et al_1992_A Training Algorithm for Optimal Margin Classifiers.pdf:application/pdf}
}

@article{evgeniouRegularizationNetworksSupport2000,
	title = {Regularization Networks and Support Vector Machines},
	abstract = {Regularization Networks and Support Vector Machines are techniques for solving certain problems of learning from examples – in particular the regression problem of approximating a multivariate function from sparse data. Radial Basis Functions, for example, are a special case of both regularization and Support Vector Machines. We review both formulations in the context of Vapnik’s theory of statistical learning which provides a general foundation for the learning problem, combining functional analysis and statistics. The emphasis is on regression: classiﬁcation is treated as a special case.},
	pages = {53},
	author = {Evgeniou, Theodoros and Pontil, Massimiliano and Poggio, Tomaso},
	date = {2000},
	langid = {english},
	keywords = {tbr},
	file = {Evgeniou et al_2000_Regularization Networks and Support Vector Machines.pdf:/Users/wpq/Dropbox (MIT)/zotero/Evgeniou et al_2000_Regularization Networks and Support Vector Machines.pdf:application/pdf}
}

@article{wahbaIntroductionReproducingKernel2003,
	title = {An introduction to reproducing kernel hilbert spaces and why they are so useful},
	volume = {36},
	issn = {1474-6670},
	url = {http://www.sciencedirect.com/science/article/pii/S1474667017348152},
	doi = {10.1016/S1474-6670(17)34815-2},
	series = {13th {IFAC} Symposium on System Identification ({SYSID} 2003), Rotterdam, The Netherlands, 27-29 August, 2003},
	abstract = {We review some of the basic facts about reproducing kernel Hilbert spaces ({RKHS}), and the solution of optimization problems in {RKHS}, These facts provide some clues to how useful {RKHS}-based methods can be in curve fitting, function estimation, model description, model fitting and ill-posed inverse problems. A number of references are made to mostly older works of the author, colleagues and former students - not an attempt at a balanced review of the literature. The growth of the further development and application of {RKHS} based methods is demonstrated in the other papers in the invited Session {INV}-2 of this 13th {IFAC} Symposium on System Identification ({SYSID}-2003), and elsewhere in {SISID}-2003. A recent advanced google search for "reproducing kernel" turned up over 4400 entries.},
	pages = {525--528},
	number = {16},
	journaltitle = {{IFAC} Proceedings Volumes},
	shortjournal = {{IFAC} Proceedings Volumes},
	author = {Wahba, Grace},
	urldate = {2020-12-24},
	date = {2003-09-01},
	langid = {english},
	keywords = {tbr},
	file = {Wahba_2003_An introduction to reproducing kernel hilbert spaces and why they are so useful.pdf:/Users/wpq/Dropbox (MIT)/zotero/Wahba_2003_An introduction to reproducing kernel hilbert spaces and why they are so useful.pdf:application/pdf}
}

@book{rifkinEverythingOldNew2002,
	title = {Everything Old Is New Again: A Fresh Look at Historical Approaches {IN} {MACHINE} {LEARNING}},
	shorttitle = {Everything Old Is New Again},
	author = {Rifkin, Ryan Michael},
	date = {2002},
	keywords = {read},
	file = {Rifkin_2002_Everything Old Is New Again - A Fresh Look at Historical Approaches IN MACHINE LEARNING.pdf:/Users/wpq/Dropbox (MIT)/zotero/Rifkin_2002_Everything Old Is New Again - A Fresh Look at Historical Approaches IN MACHINE LEARNING.pdf:application/pdf}
}

@article{songLearningHilbertSpace2008,
	title = {Learning via Hilbert Space Embedding of Distributions},
	author = {Song, Le},
	date = {2008},
	keywords = {good, read},
	file = {Song_2008_Learning via Hilbert Space Embedding of Distributions.pdf:/Users/wpq/Dropbox (MIT)/zotero/Song_2008_Learning via Hilbert Space Embedding of Distributions.pdf:application/pdf}
}

@article{sunDifferentiableCompositionalKernel2018,
	title = {Differentiable Compositional Kernel Learning for Gaussian Processes},
	url = {http://arxiv.org/abs/1806.04326},
	abstract = {The generalization properties of Gaussian processes depend heavily on the choice of kernel, and this choice remains a dark art. We present the Neural Kernel Network ({NKN}), a flexible family of kernels represented by a neural network. The {NKN} architecture is based on the composition rules for kernels, so that each unit of the network corresponds to a valid kernel. It can compactly approximate compositional kernel structures such as those used by the Automatic Statistician (Lloyd et al., 2014), but because the architecture is differentiable, it is end-to-end trainable with gradient-based optimization. We show that the {NKN} is universal for the class of stationary kernels. Empirically we demonstrate pattern discovery and extrapolation abilities of {NKN} on several tasks that depend crucially on identifying the underlying structure, including time series and texture extrapolation, as well as Bayesian optimization.},
	journaltitle = {{arXiv}:1806.04326 [cs, stat]},
	author = {Sun, Shengyang and Zhang, Guodong and Wang, Chaoqi and Zeng, Wenyuan and Li, Jiaman and Grosse, Roger},
	urldate = {2020-12-25},
	date = {2018-08-05},
	eprinttype = {arxiv},
	eprint = {1806.04326},
	keywords = {tbr},
	file = {Sun et al_2018_Differentiable Compositional Kernel Learning for Gaussian Processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Sun et al_2018_Differentiable Compositional Kernel Learning for Gaussian Processes.pdf:application/pdf}
}

@article{sutherlandScalableFlexibleActive2016,
	title = {Scalable, Flexible and Active Learning on Distributions},
	author = {Sutherland, Dougal},
	date = {2016},
	keywords = {good, tbr},
	file = {Sutherland_2016_Scalable, Flexible and Active Learning on Distributions.pdf:/Users/wpq/Dropbox (MIT)/zotero/Sutherland_2016_Scalable, Flexible and Active Learning on Distributions.pdf:application/pdf}
}

@article{grettonIntroductionRKHSSimple2019,
	title = {Introduction to {RKHS}, and some simple kernel algorithms},
	url = {http://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf},
	pages = {33},
	author = {Gretton, Arthur},
	date = {2019},
	langid = {english},
	keywords = {read},
	file = {Gretton_2019_Introduction to RKHS, and some simple kernel algorithms.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gretton_2019_Introduction to RKHS, and some simple kernel algorithms.pdf:application/pdf}
}

@article{muandetKernelMeanEmbedding2017,
	title = {Kernel Mean Embedding of Distributions: A Review and Beyond},
	volume = {10},
	issn = {1935-8237, 1935-8245},
	url = {http://arxiv.org/abs/1605.09522},
	doi = {10.1561/2200000060},
	shorttitle = {Kernel Mean Embedding of Distributions},
	abstract = {A Hilbert space embedding of a distribution---in short, a kernel mean embedding---has recently emerged as a powerful tool for machine learning and inference. The basic idea behind this framework is to map distributions into a reproducing kernel Hilbert space ({RKHS}) in which the whole arsenal of kernel methods can be extended to probability measures. It can be viewed as a generalization of the original "feature map" common to support vector machines ({SVMs}) and other kernel methods. While initially closely associated with the latter, it has meanwhile found application in fields ranging from kernel machines and probabilistic modeling to statistical inference, causal discovery, and deep learning. The goal of this survey is to give a comprehensive review of existing work and recent advances in this research area, and to discuss the most challenging issues and open problems that could lead to new research directions. The survey begins with a brief introduction to the {RKHS} and positive definite kernels which forms the backbone of this survey, followed by a thorough discussion of the Hilbert space embedding of marginal distributions, theoretical guarantees, and a review of its applications. The embedding of distributions enables us to apply {RKHS} methods to probability measures which prompts a wide range of applications such as kernel two-sample testing, independent testing, and learning on distributional data. Next, we discuss the Hilbert space embedding for conditional distributions, give theoretical insights, and review some applications. The conditional mean embedding enables us to perform sum, product, and Bayes' rules---which are ubiquitous in graphical model, probabilistic inference, and reinforcement learning---in a non-parametric way. We then discuss relationships between this framework and other related areas. Lastly, we give some suggestions on future research directions.},
	pages = {1--141},
	number = {1},
	journaltitle = {Foundations and Trends® in Machine Learning},
	shortjournal = {{FNT} in Machine Learning},
	author = {Muandet, Krikamol and Fukumizu, Kenji and Sriperumbudur, Bharath and Schölkopf, Bernhard},
	urldate = {2020-12-25},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1605.09522},
	keywords = {good, read},
	file = {Muandet et al_2017_Kernel Mean Embedding of Distributions - A Review and Beyond.pdf:/Users/wpq/Dropbox (MIT)/zotero/Muandet et al_2017_Kernel Mean Embedding of Distributions - A Review and Beyond.pdf:application/pdf}
}

@incollection{scholkopfKernelPrincipalComponent1997,
	location = {Berlin, Heidelberg},
	title = {Kernel principal component analysis},
	volume = {1327},
	isbn = {978-3-540-63631-1 978-3-540-69620-9},
	url = {http://link.springer.com/10.1007/BFb0020217},
	abstract = {A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can e ciently compute principal components in high dimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.},
	pages = {583--588},
	booktitle = {Artificial Neural Networks — {ICANN}'97},
	publisher = {Springer Berlin Heidelberg},
	author = {Schölkopf, Bernhard and Smola, Alexander and Müller, Klaus-Robert},
	editor = {Gerstner, Wulfram and Germond, Alain and Hasler, Martin and Nicoud, Jean-Daniel},
	editorb = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan},
	editorbtype = {redactor},
	urldate = {2020-12-26},
	date = {1997},
	langid = {english},
	doi = {10.1007/BFb0020217},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {tbr},
	file = {Scholkopf et al_1997_Kernel principal component analysis.pdf:/Users/wpq/Dropbox (MIT)/zotero/Scholkopf et al_1997_Kernel principal component analysis.pdf:application/pdf}
}

@article{liuLearningDeepKernels2020,
	title = {Learning Deep Kernels for Non-Parametric Two-Sample Tests},
	url = {http://arxiv.org/abs/2002.09116},
	abstract = {We propose a class of kernel-based two-sample tests, which aim to determine whether two sets of samples are drawn from the same distribution. Our tests are constructed from kernels parameterized by deep neural nets, trained to maximize test power. These tests adapt to variations in distribution smoothness and shape over space, and are especially suited to high dimensions and complex data. By contrast, the simpler kernels used in prior kernel testing work are spatially homogeneous, and adaptive only in lengthscale. We explain how this scheme includes popular classifier-based two-sample tests as a special case, but improves on them in general. We provide the first proof of consistency for the proposed adaptation method, which applies both to kernels on deep features and to simpler radial basis kernels or multiple kernel learning. In experiments, we establish the superior performance of our deep kernels in hypothesis testing on benchmark and real-world data. The code of our deep-kernel-based two sample tests is available at https://github.com/fengliu90/{DK}-for-{TST}.},
	journaltitle = {{arXiv}:2002.09116 [cs, stat]},
	author = {Liu, Feng and Xu, Wenkai and Lu, Jie and Zhang, Guangquan and Gretton, Arthur and Sutherland, D. J.},
	urldate = {2020-12-26},
	date = {2020-07-15},
	eprinttype = {arxiv},
	eprint = {2002.09116},
	keywords = {good, tbr},
	file = {Liu et al_2020_Learning Deep Kernels for Non-Parametric Two-Sample Tests.pdf:/Users/wpq/Dropbox (MIT)/zotero/Liu et al_2020_Learning Deep Kernels for Non-Parametric Two-Sample Tests.pdf:application/pdf}
}

@article{smolaHilbertSpaceEmbedding2007,
	title = {A Hilbert Space Embedding for Distributions},
	abstract = {We describe a technique for comparing distributions without the need for density estimation as an intermediate step. Our approach relies on mapping the distributions into a reproducing kernel Hilbert space. Applications of this technique can be found in two-sample tests, which are used for determining whether two sets of observations arise from the same distribution, covariate shift correction, local learning, measures of independence, and density estimation.},
	pages = {20},
	author = {Smola, Alex and Gretton, Arthur and Song, Le and Scholkopf, Bernhard},
	date = {2007},
	langid = {english},
	keywords = {tbr},
	file = {Smola et al_2007_A Hilbert Space Embedding for Distributions.pdf:/Users/wpq/Dropbox (MIT)/zotero/Smola et al_2007_A Hilbert Space Embedding for Distributions.pdf:application/pdf}
}

@article{grettonKernelMethodsMeasuring2005,
	title = {Kernel Methods for Measuring Independence},
	abstract = {We introduce two new functionals, the constrained covariance and the kernel mutual information, to measure the degree of independence of random variables. These quantities are both based on the covariance between functions of the random variables in reproducing kernel Hilbert spaces ({RKHSs}). We prove that when the {RKHSs} are universal, both functionals are zero if and only if the random variables are pairwise independent. We also show that the kernel mutual information is an upper bound near independence on the Parzen window estimate of the mutual information. Analogous results apply for two correlation-based dependence functionals introduced earlier: we show the kernel canonical correlation and the kernel generalised variance to be independence measures for universal kernels, and prove the latter to be an upper bound on the mutual information near independence. The performance of the kernel dependence functionals in measuring independence is veriﬁed in the context of independent component analysis.},
	pages = {55},
	author = {Gretton, Arthur and Herbrich, Ralf and Smola, Alexander and Bousquet, Olivier and Schölkopf, Bernhard},
	date = {2005},
	langid = {english},
	keywords = {tbr},
	file = {Gretton et al_2005_Kernel Methods for Measuring Independence.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gretton et al_2005_Kernel Methods for Measuring Independence.pdf:application/pdf}
}

@article{wilsonStochasticVariationalDeep2016,
	title = {Stochastic Variational Deep Kernel Learning},
	url = {https://arxiv.org/abs/1611.00336v2},
	abstract = {Deep kernel learning combines the non-parametric flexibility of kernel
methods with the inductive biases of deep learning architectures. We propose a
novel deep kernel learning model and stochastic variational inference procedure
which generalizes deep kernel learning approaches to enable classification,
multi-task learning, additive covariance structures, and stochastic gradient
training. Specifically, we apply additive base kernels to subsets of output
features from deep neural architectures, and jointly learn the parameters of
the base kernels and deep network through a Gaussian process marginal
likelihood objective. Within this framework, we derive an efficient form of
stochastic variational inference which leverages local kernel interpolation,
inducing points, and structure exploiting algebra. We show improved performance
over stand alone deep networks, {SVMs}, and state of the art scalable Gaussian
processes on several classification benchmarks, including an airline delay
dataset containing 6 million training points, {CIFAR}, and {ImageNet}.},
	author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
	urldate = {2020-12-28},
	date = {2016-11-01},
	langid = {english},
	keywords = {tbr},
	file = {Wilson et al_2016_Stochastic Variational Deep Kernel Learning.pdf:/Users/wpq/Dropbox (MIT)/zotero/Wilson et al_2016_Stochastic Variational Deep Kernel Learning.pdf:application/pdf}
}

@article{salakhutdinovDeepKernelLearning2016,
	title = {Deep Kernel Learning},
	abstract = {We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric ﬂexibility of kernel methods. Speciﬁcally, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation. These closed-form kernels can be used as drop-in replacements for standard kernels, with beneﬁts in expressive power and scalability. We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process. Inference and learning cost O(n) for n training points, and predictions cost O(1) per test point. On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with ﬂexible kernel learning models, and stand-alone deep architectures.},
	pages = {19},
	author = {Salakhutdinov, Ruslan and Hu, Zhiting and Xing, Eric P},
	date = {2016},
	langid = {english},
	keywords = {tbr},
	file = {Salakhutdinov et al_2016_Deep Kernel Learning.pdf:/Users/wpq/Dropbox (MIT)/zotero/Salakhutdinov et al_2016_Deep Kernel Learning.pdf:application/pdf}
}

@article{wilsonCovarianceKernelsFast2014,
	title = {Covariance Kernels for Fast Automatic Pattern Discovery and Extrapolation with Gaussian Processes},
	abstract = {Truly intelligent systems are capable of pattern discovery and extrapolation without human intervention. Bayesian nonparametric models, which can uniquely represent expressive prior information and detailed inductive biases, provide a distinct opportunity to develop intelligent systems, with applications in essentially any learning and prediction task.},
	pages = {226},
	author = {Wilson, Andrew Gordon},
	date = {2014},
	langid = {english},
	keywords = {tbr},
	file = {Wilson_2014_Covariance Kernels for Fast Automatic Pattern Discovery and Extrapolation with Gaussian Processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Wilson_2014_Covariance Kernels for Fast Automatic Pattern Discovery and Extrapolation with Gaussian Processes.pdf:application/pdf}
}

@article{binkowskiDemystifyingMMDGANs2018,
	title = {Demystifying {MMD} {GANs}},
	url = {http://arxiv.org/abs/1801.01401},
	abstract = {We investigate the training and performance of generative adversarial networks using the Maximum Mean Discrepancy ({MMD}) as critic, termed {MMD} {GANs}. As our main theoretical contribution, we clarify the situation with bias in {GAN} loss functions raised by recent work: we show that gradient estimators used in the optimization process for both {MMD} {GANs} and Wasserstein {GANs} are unbiased, but learning a discriminator based on samples leads to biased gradients for the generator parameters. We also discuss the issue of kernel choice for the {MMD} critic, and characterize the kernel corresponding to the energy distance used for the Cramer {GAN} critic. Being an integral probability metric, the {MMD} benefits from training strategies recently developed for Wasserstein {GANs}. In experiments, the {MMD} {GAN} is able to employ a smaller critic network than the Wasserstein {GAN}, resulting in a simpler and faster-training algorithm with matching performance. We also propose an improved measure of {GAN} convergence, the Kernel Inception Distance, and show how to use it to dynamically adapt learning rates during {GAN} training.},
	journaltitle = {{arXiv}:1801.01401 [cs, stat]},
	author = {Bińkowski, Mikołaj and Sutherland, Dougal J. and Arbel, Michael and Gretton, Arthur},
	urldate = {2020-12-28},
	date = {2018-03-21},
	eprinttype = {arxiv},
	eprint = {1801.01401},
	keywords = {tbr},
	file = {Binkowski et al_2018_Demystifying MMD GANs.pdf:/Users/wpq/Dropbox (MIT)/zotero/Binkowski et al_2018_Demystifying MMD GANs.pdf:application/pdf}
}

@article{grettonNotesMeanEmbeddings2019,
	title = {Notes on mean embeddings and covariance operators},
	url = {http://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture5_covarianceOperator.pdf},
	pages = {15},
	author = {Gretton, Arthur},
	date = {2019},
	langid = {english},
	keywords = {tbr},
	file = {Gretton_2019_Notes on mean embeddings and covariance operators.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gretton_2019_Notes on mean embeddings and covariance operators.pdf:application/pdf}
}

@article{grettonWhatRKHS2012,
	title = {What is an {RKHS}?},
	url = {http://www.stats.ox.ac.uk/~sejdinov/teaching/atml14/Theory_2014.pdf},
	author = {Gretton, Arthur},
	date = {2012},
	keywords = {good, read},
	file = {Gretton_2012_What is an RKHS.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gretton_2012_What is an RKHS.pdf:application/pdf}
}

@article{yangCarteLearningFast2015,
	title = {A la Carte — Learning Fast Kernels},
	abstract = {Kernel methods have great promise for learning rich statistical representations of large modern datasets. However, compared to neural networks, kernel methods have been perceived as lacking in scalability and ﬂexibility. We introduce a family of fast, ﬂexible, lightly parametrized and general purpose kernel learning methods, derived from Fastfood basis function expansions. We provide mechanisms to learn the properties of groups of spectral frequencies in these expansions, which require only O(m log d) time and O(m) memory, for m basis functions and d input dimensions. We show that the proposed methods can learn a wide class of kernels, outperforming the alternatives in accuracy, speed, and memory consumption.},
	pages = {9},
	author = {Yang, Zichao and Smola, Alexander J and Song, Le and Wilson, Andrew Gordon},
	date = {2015},
	langid = {english},
	keywords = {tbr},
	file = {Yang et al_2015_A la Carte — Learning Fast Kernels.pdf:/Users/wpq/Dropbox (MIT)/zotero/Yang et al_2015_A la Carte — Learning Fast Kernels.pdf:application/pdf}
}

@book{lampertKernelMethodsComputer2009,
	title = {Kernel Methods in Computer Vision},
	author = {Lampert, Christoph H.},
	date = {2009},
	keywords = {read},
	file = {Lampert_2009_Kernel Methods in Computer Vision.pdf:/Users/wpq/Dropbox (MIT)/zotero/Lampert_2009_Kernel Methods in Computer Vision.pdf:application/pdf}
}

@article{songFeatureSelectionDependence2012a,
	title = {Feature Selection via Dependence Maximization},
	abstract = {We introduce a framework for feature selection based on dependence maximization between the selected features and the labels of an estimation problem, using the Hilbert-Schmidt Independence Criterion. The key idea is that good features should be highly dependent on the labels. Our approach leads to a greedy procedure for feature selection. We show that a number of existing feature selectors are special cases of this framework. Experiments on both artiﬁcial and real-world data show that our feature selector works well in practice.},
	pages = {42},
	author = {Song, Le},
	date = {2012},
	langid = {english},
	keywords = {good, tbr},
	file = {Song_2012_Feature Selection via Dependence Maximization.pdf:/Users/wpq/Dropbox (MIT)/zotero/Song_2012_Feature Selection via Dependence Maximization.pdf:application/pdf}
}

@article{liKernelDependenceRegularizers2019,
	title = {Kernel Dependence Regularizers and Gaussian Processes with Applications to Algorithmic Fairness},
	url = {http://arxiv.org/abs/1911.04322},
	abstract = {Current adoption of machine learning in industrial, societal and economical activities has raised concerns about the fairness, equity and ethics of automated decisions. Predictive models are often developed using biased datasets and thus retain or even exacerbate biases in their decisions and recommendations. Removing the sensitive covariates, such as gender or race, is insufficient to remedy this issue since the biases may be retained due to other related covariates. We present a regularization approach to this problem that trades off predictive accuracy of the learned models (with respect to biased labels) for the fairness in terms of statistical parity, i.e. independence of the decisions from the sensitive covariates. In particular, we consider a general framework of regularized empirical risk minimization over reproducing kernel Hilbert spaces and impose an additional regularizer of dependence between predictors and sensitive covariates using kernel-based measures of dependence, namely the Hilbert-Schmidt Independence Criterion ({HSIC}) and its normalized version. This approach leads to a closed-form solution in the case of squared loss, i.e. ridge regression. Moreover, we show that the dependence regularizer has an interpretation as modifying the corresponding Gaussian process ({GP}) prior. As a consequence, a {GP} model with a prior that encourages fairness to sensitive variables can be derived, allowing principled hyperparameter selection and studying of the relative relevance of covariates under fairness constraints. Experimental results in synthetic examples and in real problems of income and crime prediction illustrate the potential of the approach to improve fairness of automated decisions.},
	journaltitle = {{arXiv}:1911.04322 [cs, stat]},
	author = {Li, Zhu and Perez-Suay, Adrian and Camps-Valls, Gustau and Sejdinovic, Dino},
	urldate = {2020-12-30},
	date = {2019-11-11},
	eprinttype = {arxiv},
	eprint = {1911.04322},
	keywords = {good, tbr},
	file = {Li et al_2019_Kernel Dependence Regularizers and Gaussian Processes with Applications to Algorithmic Fairness.pdf:/Users/wpq/Dropbox (MIT)/zotero/Li et al_2019_Kernel Dependence Regularizers and Gaussian Processes with Applications to Algorithmic Fairness.pdf:application/pdf}
}

@article{muandetLearningDistributionsSupport2013,
	title = {Learning from Distributions via Support Measure Machines},
	url = {http://arxiv.org/abs/1202.6504},
	abstract = {This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space ({RKHS}), we are able to apply many standard kernel-based learning techniques in straightforward fashion. To accomplish this, we construct a generalization of the support vector machine ({SVM}) called a support measure machine ({SMM}). Our analyses of {SMMs} provides several insights into their relationship to traditional {SVMs}. Based on such insights, we propose a flexible {SVM} (Flex-{SVM}) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework.},
	journaltitle = {{arXiv}:1202.6504 [cs, stat]},
	author = {Muandet, Krikamol and Fukumizu, Kenji and Dinuzzo, Francesco and Schölkopf, Bernhard},
	urldate = {2020-12-30},
	date = {2013-01-12},
	eprinttype = {arxiv},
	eprint = {1202.6504},
	keywords = {read},
	file = {Muandet et al_2013_Learning from Distributions via Support Measure Machines.pdf:/Users/wpq/Dropbox (MIT)/zotero/Muandet et al_2013_Learning from Distributions via Support Measure Machines.pdf:application/pdf}
}

@inproceedings{grettonMeasuringStatisticalDependence2005,
	location = {Berlin, Heidelberg},
	title = {Measuring statistical dependence with hilbert-schmidt norms},
	isbn = {978-3-540-29242-5},
	url = {https://doi.org/10.1007/11564089_7},
	doi = {10.1007/11564089_7},
	series = {{ALT}'05},
	abstract = {We propose an independence criterion based on the eigenspectrum of covariance operators in reproducing kernel Hilbert spaces ({RKHSs}), consisting of an empirical estimate of the Hilbert-Schmidt norm of the cross-covariance operator (we term this a Hilbert-Schmidt Independence Criterion, or {HSIC}). This approach has several advantages, compared with previous kernel-based independence criteria. First, the empirical estimate is simpler than any other kernel dependence test, and requires no user-defined regularisation. Second, there is a clearly defined population quantity which the empirical estimate approaches in the large sample limit, with exponential convergence guaranteed between the two: this ensures that independence tests based on {HSIC} do not suffer from slow learning rates. Finally, we show in the context of independent component analysis ({ICA}) that the performance of {HSIC} is competitive with that of previously published kernel-based criteria, and of other recently published {ICA} methods.},
	pages = {63--77},
	booktitle = {Proceedings of the 16th international conference on Algorithmic Learning Theory},
	publisher = {Springer-Verlag},
	author = {Gretton, Arthur and Bousquet, Olivier and Smola, Alex and Schölkopf, Bernhard},
	urldate = {2021-01-01},
	date = {2005-10-08},
	keywords = {tbr},
	file = {Gretton et al_2005_Measuring statistical dependence with hilbert-schmidt norms.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gretton et al_2005_Measuring statistical dependence with hilbert-schmidt norms.pdf:application/pdf}
}

@report{fukumizuDimensionalityReductionSupervised2003,
	location = {Fort Belvoir, {VA}},
	title = {Dimensionality Reduction for Supervised Learning With Reproducing Kernel Hilbert Spaces:},
	url = {http://www.dtic.mil/docs/citations/ADA446572},
	shorttitle = {Dimensionality Reduction for Supervised Learning With Reproducing Kernel Hilbert Spaces},
	abstract = {We propose a novel method of dimensionality reduction for supervised learning problems. Given a regression or classiﬁcation problem in which we wish to predict a response variable Y from an explanatory variable X, we treat the problem of dimensionality reduction as that of ﬁnding a low-dimensional “eﬀective subspace” for X which retains the statistical relationship between X and Y . We show that this problem can be formulated in terms of conditional independence. To turn this formulation into an optimization problem we establish a general nonparametric characterization of conditional independence using covariance operators on reproducing kernel Hilbert spaces. This characterization allows us to derive a contrast function for estimation of the eﬀective subspace. Unlike many conventional methods for dimensionality reduction in supervised learning, the proposed method requires neither assumptions on the marginal distribution of X, nor a parametric model of the conditional distribution of Y . We present experiments that compare the performance of the method with conventional methods.},
	institution = {Defense Technical Information Center},
	author = {Fukumizu, Kenji and Bach, Francis R. and Jordan, Michael I.},
	urldate = {2021-01-01},
	date = {2003-05-25},
	langid = {english},
	doi = {10.21236/ADA446572},
	keywords = {tbr},
	file = {Fukumizu et al_2003_Dimensionality Reduction for Supervised Learning With Reproducing Kernel Hilbert Spaces.pdf:/Users/wpq/Dropbox (MIT)/zotero/Fukumizu et al_2003_Dimensionality Reduction for Supervised Learning With Reproducing Kernel Hilbert Spaces.pdf:application/pdf}
}

@article{bakerJointMeasuresCrossCovariance1973,
	title = {Joint Measures and Cross-Covariance Operators},
	abstract = {Let H. (resp., H ) be a real and separable Hubert space with Borel O'field T (resp., rj, and let (H. X //-, T, X T.) be the product measurable space generated by the measurable rectangles. This paper develops relations between probability measures on (H. x H , V x T.), i.e., joint measures, and the projections of such measures on (H., T.) and (H , Y ). In particular, the class of all joint Gaussian measures having two specified Gaussian measures as projections is characterized, and conditions are obtained for two joint Gaussian measures to be mutually absolutely continuous. The cross-covariance operator of a joint measure plays a major role in these results and these operators are characterized.},
	pages = {17},
	author = {Baker, Charlesr},
	date = {1973},
	langid = {english},
	keywords = {tbr},
	file = {Baker_1973_Joint Measures and Cross-Covariance Operators.pdf:/Users/wpq/Dropbox (MIT)/zotero/Baker_1973_Joint Measures and Cross-Covariance Operators.pdf:application/pdf}
}

@article{liangCS229TSTAT231Statistical2016,
	title = {{CS}229T/{STAT}231: Statistical Learning Theory (Winter 2016)},
	pages = {217},
	author = {Liang, Percy},
	date = {2016},
	langid = {english},
	keywords = {tbr},
	file = {Liang_2016_CS229T-STAT231 - Statistical Learning Theory (Winter 2016).pdf:/Users/wpq/Dropbox (MIT)/zotero/Liang_2016_CS229T-STAT231 - Statistical Learning Theory (Winter 2016).pdf:application/pdf}
}

@article{liImplicitKernelLearning2019,
	title = {Implicit Kernel Learning},
	url = {http://arxiv.org/abs/1902.10214},
	abstract = {Kernels are powerful and versatile tools in machine learning and statistics. Although the notion of universal kernels and characteristic kernels has been studied, kernel selection still greatly influences the empirical performance. While learning the kernel in a data driven way has been investigated, in this paper we explore learning the spectral distribution of kernel via implicit generative models parametrized by deep neural networks. We called our method Implicit Kernel Learning ({IKL}). The proposed framework is simple to train and inference is performed via sampling random Fourier features. We investigate two applications of the proposed {IKL} as examples, including generative adversarial networks with {MMD} ({MMD} {GAN}) and standard supervised learning. Empirically, {MMD} {GAN} with {IKL} outperforms vanilla predefined kernels on both image and text generation benchmarks; using {IKL} with Random Kitchen Sinks also leads to substantial improvement over existing state-of-the-art kernel learning algorithms on popular supervised learning benchmarks. Theory and conditions for using {IKL} in both applications are also studied as well as connections to previous state-of-the-art methods.},
	journaltitle = {{arXiv}:1902.10214 [cs, stat]},
	author = {Li, Chun-Liang and Chang, Wei-Cheng and Mroueh, Youssef and Yang, Yiming and Póczos, Barnabás},
	urldate = {2021-01-04},
	date = {2019-02-26},
	eprinttype = {arxiv},
	eprint = {1902.10214},
	keywords = {tbr},
	file = {Li et al_2019_Implicit Kernel Learning.pdf:/Users/wpq/Dropbox (MIT)/zotero/Li et al_2019_Implicit Kernel Learning.pdf:application/pdf}
}

@article{wilsonGaussianProcessKernels2013,
	title = {Gaussian Process Kernels for Pattern Discovery and Extrapolation},
	abstract = {Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation. We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation. These kernels are derived by modelling a spectral density – the Fourier transform of a kernel – with a Gaussian mixture. The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic. We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric {CO}2 trends and airline passenger data. We also show that it is possible to reconstruct several popular standard covariances within our framework.},
	pages = {9},
	author = {Wilson, Andrew Gordon and Adams, Ryan Prescott},
	date = {2013},
	langid = {english},
	keywords = {tbr},
	file = {Wilson and Adams - Gaussian Process Kernels for Pattern Discovery and.pdf:/Users/wpq/Zotero/storage/KF7ABATZ/Wilson and Adams - Gaussian Process Kernels for Pattern Discovery and.pdf:application/pdf}
}

@inproceedings{grettonOptimalKernelChoice2012,
	title = {Optimal kernel choice for large-scale two-sample tests},
	volume = {25},
	abstract = {Given samples from distributions p and q, a two-sample test determines whether to reject the null hypothesis that p = q, based on the value of a test statistic measuring the distance between the samples. One choice of test statistic is the maximum mean discrepancy ({MMD}), which is a distance between embeddings of the probability distributions in a reproducing kernel Hilbert space. The kernel used in obtaining these embeddings is critical in ensuring the test has high power, and correctly distinguishes unlike distributions with high probability. A means of parameter selection for the two-sample test based on the {MMD} is proposed. For a given test level (an upper bound on the probability of making a Type I error), the kernel is chosen so as to maximize the test power, and minimize the probability of making a Type {II} error. The test statistic, test threshold, and optimization over the kernel parameters are obtained with cost linear in the sample size. These properties make the kernel selection and test procedures suited to data streams, where the observations cannot all be stored in memory. In experiments, the new kernel selection approach yields a more powerful test than earlier kernel selection heuristics.},
	eventtitle = {Advances in Neural Information Processing Systems},
	author = {Gretton, Arthur and Sriperumbudur, Bharath and Sejdinovic, Dino and Strathmann, Heiko and Balakrishnan, Sivaraman and Pontil, Massimiliano and Kenji, Fukumizu},
	date = {2012-01-01},
	keywords = {tbr},
	file = {Gretton et al_2012_Optimal kernel choice for large-scale two-sample tests.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gretton et al_2012_Optimal kernel choice for large-scale two-sample tests.pdf:application/pdf}
}