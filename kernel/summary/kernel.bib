
@incollection{jaakkolaExploitingGenerativeModels1999,
	title = {Exploiting Generative Models in Discriminative Classifiers},
	url = {http://papers.nips.cc/paper/1520-exploiting-generative-models-in-discriminative-classifiers.pdf},
	pages = {487--493},
	booktitle = {Advances in Neural Information Processing Systems 11},
	publisher = {{MIT} Press},
	author = {Jaakkola, Tommi and Haussler, David},
	editor = {Kearns, M. J. and Solla, S. A. and Cohn, D. A.},
	urldate = {2020-08-25},
	date = {1999},
	keywords = {tbr},
	file = {Jaakkola_Haussler_1999_Exploiting Generative Models in Discriminative Classifiers.pdf:/Users/wpq/Dropbox (MIT)/zotero/Jaakkola_Haussler_1999_Exploiting Generative Models in Discriminative Classifiers.pdf:application/pdf}
}

@article{sriperumbudurHilbertSpaceEmbeddings2010,
	title = {Hilbert Space Embeddings and Metrics on Probability Measures},
	abstract = {A Hilbert space embedding for probability measures has recently been proposed, with applications including dimensionality reduction, homogeneity testing, and independence testing. This embedding represents any probability measure as a mean element in a reproducing kernel Hilbert space ({RKHS}). A pseudometric on the space of probability measures can be deﬁned as the distance between distribution embeddings: we denote this as γk, indexed by the kernel function k that deﬁnes the inner product in the {RKHS}.},
	pages = {45},
	author = {Sriperumbudur, Bharath K and Gretton, Arthur and Fukumizu, Kenji and Scholkopf, Bernhard and Lanckriet, Gert R G},
	date = {2010},
	langid = {english},
	keywords = {tbr},
	file = {Sriperumbudur et al_2010_Hilbert Space Embeddings and Metrics on Probability Measures.pdf:/Users/wpq/Dropbox (MIT)/zotero/Sriperumbudur et al_2010_Hilbert Space Embeddings and Metrics on Probability Measures.pdf:application/pdf}
}

@inproceedings{leslieSPECTRUMKERNELSTRING2001,
	location = {Kauai, Hawaii, {USA}},
	title = {{THE} {SPECTRUM} {KERNEL}: A {STRING} {KERNEL} {FOR} {SVM} {PROTEIN} {CLASSIFICATION}},
	isbn = {978-981-02-4777-5 978-981-279-962-3},
	url = {http://www.worldscientific.com/doi/abs/10.1142/9789812799623_0053},
	doi = {10.1142/9789812799623_0053},
	shorttitle = {{THE} {SPECTRUM} {KERNEL}},
	eventtitle = {Proceedings of the Pacific Symposium},
	pages = {564--575},
	booktitle = {Biocomputing 2002},
	publisher = {{WORLD} {SCIENTIFIC}},
	author = {Leslie, Christina and Eskin, Eleazar and Noble, William Stafford},
	urldate = {2020-12-23},
	date = {2001-12},
	langid = {english},
	keywords = {tbr},
	file = {Leslie et al_2001_THE SPECTRUM KERNEL - A STRING KERNEL FOR SVM PROTEIN CLASSIFICATION.pdf:/Users/wpq/Dropbox (MIT)/zotero/Leslie et al_2001_THE SPECTRUM KERNEL - A STRING KERNEL FOR SVM PROTEIN CLASSIFICATION.pdf:application/pdf}
}

@inproceedings{choLargeMarginClassificationHyperbolic2019,
	title = {Large-Margin Classification in Hyperbolic Space},
	url = {http://proceedings.mlr.press/v89/cho19a.html},
	abstract = {Representing data in hyperbolic space can effectively capture latent hierarchical relationships. To enable accurate classification of points in hyperbolic space while respecting their hyperbolic ge...},
	eventtitle = {The 22nd International Conference on Artificial Intelligence and Statistics},
	pages = {1832--1840},
	booktitle = {The 22nd International Conference on Artificial Intelligence and Statistics},
	publisher = {{PMLR}},
	author = {Cho, Hyunghoon and {DeMeo}, Benjamin and Peng, Jian and Berger, Bonnie},
	urldate = {2020-12-24},
	date = {2019-04-11},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	keywords = {tbr},
	file = {Cho et al_2019_Large-Margin Classification in Hyperbolic Space.pdf:/Users/wpq/Dropbox (MIT)/zotero/Cho et al_2019_Large-Margin Classification in Hyperbolic Space.pdf:application/pdf}
}

@article{ngCs229kernel2019,
	title = {cs229-kernel},
	url = {http://cs229.stanford.edu/summer2020/cs229-notes3.pdf},
	author = {Ng, Andrew},
	date = {2019},
	keywords = {read},
	file = {2019_cs229-kernel.pdf:/Users/wpq/Dropbox (MIT)/zotero/2019_cs229-kernel.pdf:application/pdf}
}

@article{rahimiRandomFeaturesLargeScale2007,
	title = {Random Features for Large-Scale Kernel Machines},
	abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. Our randomized features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user speciﬁed shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classiﬁcation and regression tasks linear machine learning algorithms that use these features outperform state-of-the-art large-scale kernel machines.},
	pages = {8},
	author = {Rahimi, Ali and Recht, Ben},
	date = {2007},
	langid = {english},
	keywords = {tbr},
	file = {Rahimi_Recht_2007_Random Features for Large-Scale Kernel Machines.pdf:/Users/wpq/Dropbox (MIT)/zotero/Rahimi_Recht_2007_Random Features for Large-Scale Kernel Machines.pdf:application/pdf}
}

@article{vertPrimerKernelMethods2004,
	title = {A primer on kernel methods},
	pages = {42},
	author = {Vert, Jean-Philippe and Tsuda, Koji and Scholkopf, Bernhard},
	date = {2004},
	langid = {english},
	keywords = {read},
	file = {Vert et al_2004_A primer on kernel methods.pdf:/Users/wpq/Dropbox (MIT)/zotero/Vert et al_2004_A primer on kernel methods.pdf:application/pdf}
}

@article{boserTrainingAlgorithmOptimal1992,
	title = {A Training Algorithm for Optimal Margin Classifiers},
	author = {Boser, Bernhard and Guyon, Isabelle and Vapnik, Vladimir},
	date = {1992},
	keywords = {tbr},
	file = {Boser et al_1992_A Training Algorithm for Optimal Margin Classifiers.pdf:/Users/wpq/Dropbox (MIT)/zotero/Boser et al_1992_A Training Algorithm for Optimal Margin Classifiers.pdf:application/pdf}
}

@article{evgeniouRegularizationNetworksSupport2000,
	title = {Regularization Networks and Support Vector Machines},
	abstract = {Regularization Networks and Support Vector Machines are techniques for solving certain problems of learning from examples – in particular the regression problem of approximating a multivariate function from sparse data. Radial Basis Functions, for example, are a special case of both regularization and Support Vector Machines. We review both formulations in the context of Vapnik’s theory of statistical learning which provides a general foundation for the learning problem, combining functional analysis and statistics. The emphasis is on regression: classiﬁcation is treated as a special case.},
	pages = {53},
	author = {Evgeniou, Theodoros and Pontil, Massimiliano and Poggio, Tomaso},
	date = {2000},
	langid = {english},
	keywords = {tbr},
	file = {Evgeniou et al_2000_Regularization Networks and Support Vector Machines.pdf:/Users/wpq/Dropbox (MIT)/zotero/Evgeniou et al_2000_Regularization Networks and Support Vector Machines.pdf:application/pdf}
}

@article{wahbaIntroductionReproducingKernel2003,
	title = {An introduction to reproducing kernel hilbert spaces and why they are so useful},
	volume = {36},
	issn = {1474-6670},
	url = {http://www.sciencedirect.com/science/article/pii/S1474667017348152},
	doi = {10.1016/S1474-6670(17)34815-2},
	series = {13th {IFAC} Symposium on System Identification ({SYSID} 2003), Rotterdam, The Netherlands, 27-29 August, 2003},
	abstract = {We review some of the basic facts about reproducing kernel Hilbert spaces ({RKHS}), and the solution of optimization problems in {RKHS}, These facts provide some clues to how useful {RKHS}-based methods can be in curve fitting, function estimation, model description, model fitting and ill-posed inverse problems. A number of references are made to mostly older works of the author, colleagues and former students - not an attempt at a balanced review of the literature. The growth of the further development and application of {RKHS} based methods is demonstrated in the other papers in the invited Session {INV}-2 of this 13th {IFAC} Symposium on System Identification ({SYSID}-2003), and elsewhere in {SISID}-2003. A recent advanced google search for "reproducing kernel" turned up over 4400 entries.},
	pages = {525--528},
	number = {16},
	journaltitle = {{IFAC} Proceedings Volumes},
	shortjournal = {{IFAC} Proceedings Volumes},
	author = {Wahba, Grace},
	urldate = {2020-12-24},
	date = {2003-09-01},
	langid = {english},
	keywords = {tbr},
	file = {Wahba_2003_An introduction to reproducing kernel hilbert spaces and why they are so useful.pdf:/Users/wpq/Dropbox (MIT)/zotero/Wahba_2003_An introduction to reproducing kernel hilbert spaces and why they are so useful.pdf:application/pdf}
}

@book{rifkinEverythingOldNew2002,
	title = {Everything Old Is New Again: A Fresh Look at Historical Approaches {IN} {MACHINE} {LEARNING}},
	shorttitle = {Everything Old Is New Again},
	author = {Rifkin, Ryan Michael},
	date = {2002},
	keywords = {read},
	file = {Rifkin_2002_Everything Old Is New Again - A Fresh Look at Historical Approaches IN MACHINE LEARNING.pdf:/Users/wpq/Dropbox (MIT)/zotero/Rifkin_2002_Everything Old Is New Again - A Fresh Look at Historical Approaches IN MACHINE LEARNING.pdf:application/pdf}
}

@article{songLearningHilbertSpace2008,
	title = {Learning via Hilbert Space Embedding of Distributions},
	author = {Song, Le},
	date = {2008},
	keywords = {tbr},
	file = {Song_2008_Learning via Hilbert Space Embedding of Distributions.pdf:/Users/wpq/Dropbox (MIT)/zotero/Song_2008_Learning via Hilbert Space Embedding of Distributions.pdf:application/pdf}
}

@article{sunDifferentiableCompositionalKernel2018,
	title = {Differentiable Compositional Kernel Learning for Gaussian Processes},
	url = {http://arxiv.org/abs/1806.04326},
	abstract = {The generalization properties of Gaussian processes depend heavily on the choice of kernel, and this choice remains a dark art. We present the Neural Kernel Network ({NKN}), a flexible family of kernels represented by a neural network. The {NKN} architecture is based on the composition rules for kernels, so that each unit of the network corresponds to a valid kernel. It can compactly approximate compositional kernel structures such as those used by the Automatic Statistician (Lloyd et al., 2014), but because the architecture is differentiable, it is end-to-end trainable with gradient-based optimization. We show that the {NKN} is universal for the class of stationary kernels. Empirically we demonstrate pattern discovery and extrapolation abilities of {NKN} on several tasks that depend crucially on identifying the underlying structure, including time series and texture extrapolation, as well as Bayesian optimization.},
	journaltitle = {{arXiv}:1806.04326 [cs, stat]},
	author = {Sun, Shengyang and Zhang, Guodong and Wang, Chaoqi and Zeng, Wenyuan and Li, Jiaman and Grosse, Roger},
	urldate = {2020-12-25},
	date = {2018-08-05},
	eprinttype = {arxiv},
	eprint = {1806.04326},
	keywords = {tbr},
	file = {Sun et al_2018_Differentiable Compositional Kernel Learning for Gaussian Processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Sun et al_2018_Differentiable Compositional Kernel Learning for Gaussian Processes.pdf:application/pdf}
}

@article{sutherlandScalableFlexibleActive2016,
	title = {Scalable, Flexible and Active Learning on Distributions},
	author = {Sutherland, Dougal},
	date = {2016},
	keywords = {tbr},
	file = {Sutherland_2016_Scalable, Flexible and Active Learning on Distributions.pdf:/Users/wpq/Dropbox (MIT)/zotero/Sutherland_2016_Scalable, Flexible and Active Learning on Distributions.pdf:application/pdf}
}

@article{grettonIntroductionRKHSSimple2019,
	title = {Introduction to {RKHS}, and some simple kernel algorithms},
	url = {http://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf},
	pages = {33},
	author = {Gretton, Arthur},
	date = {2019},
	langid = {english},
	keywords = {read},
	file = {Gretton_2019_Introduction to RKHS, and some simple kernel algorithms.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gretton_2019_Introduction to RKHS, and some simple kernel algorithms.pdf:application/pdf}
}

@article{muandetKernelMeanEmbedding2017,
	title = {Kernel Mean Embedding of Distributions: A Review and Beyond},
	volume = {10},
	issn = {1935-8237, 1935-8245},
	url = {http://arxiv.org/abs/1605.09522},
	doi = {10.1561/2200000060},
	shorttitle = {Kernel Mean Embedding of Distributions},
	abstract = {A Hilbert space embedding of a distribution---in short, a kernel mean embedding---has recently emerged as a powerful tool for machine learning and inference. The basic idea behind this framework is to map distributions into a reproducing kernel Hilbert space ({RKHS}) in which the whole arsenal of kernel methods can be extended to probability measures. It can be viewed as a generalization of the original "feature map" common to support vector machines ({SVMs}) and other kernel methods. While initially closely associated with the latter, it has meanwhile found application in fields ranging from kernel machines and probabilistic modeling to statistical inference, causal discovery, and deep learning. The goal of this survey is to give a comprehensive review of existing work and recent advances in this research area, and to discuss the most challenging issues and open problems that could lead to new research directions. The survey begins with a brief introduction to the {RKHS} and positive definite kernels which forms the backbone of this survey, followed by a thorough discussion of the Hilbert space embedding of marginal distributions, theoretical guarantees, and a review of its applications. The embedding of distributions enables us to apply {RKHS} methods to probability measures which prompts a wide range of applications such as kernel two-sample testing, independent testing, and learning on distributional data. Next, we discuss the Hilbert space embedding for conditional distributions, give theoretical insights, and review some applications. The conditional mean embedding enables us to perform sum, product, and Bayes' rules---which are ubiquitous in graphical model, probabilistic inference, and reinforcement learning---in a non-parametric way. We then discuss relationships between this framework and other related areas. Lastly, we give some suggestions on future research directions.},
	pages = {1--141},
	number = {1},
	journaltitle = {Foundations and Trends® in Machine Learning},
	shortjournal = {{FNT} in Machine Learning},
	author = {Muandet, Krikamol and Fukumizu, Kenji and Sriperumbudur, Bharath and Schölkopf, Bernhard},
	urldate = {2020-12-25},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1605.09522},
	keywords = {tbr},
	file = {Muandet et al_2017_Kernel Mean Embedding of Distributions - A Review and Beyond.pdf:/Users/wpq/Dropbox (MIT)/zotero/Muandet et al_2017_Kernel Mean Embedding of Distributions - A Review and Beyond.pdf:application/pdf}
}

@incollection{scholkopfKernelPrincipalComponent1997,
	location = {Berlin, Heidelberg},
	title = {Kernel principal component analysis},
	volume = {1327},
	isbn = {978-3-540-63631-1 978-3-540-69620-9},
	url = {http://link.springer.com/10.1007/BFb0020217},
	abstract = {A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can e ciently compute principal components in high dimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.},
	pages = {583--588},
	booktitle = {Artificial Neural Networks — {ICANN}'97},
	publisher = {Springer Berlin Heidelberg},
	author = {Schölkopf, Bernhard and Smola, Alexander and Müller, Klaus-Robert},
	editor = {Gerstner, Wulfram and Germond, Alain and Hasler, Martin and Nicoud, Jean-Daniel},
	editorb = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan},
	editorbtype = {redactor},
	urldate = {2020-12-26},
	date = {1997},
	langid = {english},
	doi = {10.1007/BFb0020217},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {tbr},
	file = {Scholkopf et al_1997_Kernel principal component analysis.pdf:/Users/wpq/Dropbox (MIT)/zotero/Scholkopf et al_1997_Kernel principal component analysis.pdf:application/pdf}
}

@article{liuLearningDeepKernels2020,
	title = {Learning Deep Kernels for Non-Parametric Two-Sample Tests},
	url = {http://arxiv.org/abs/2002.09116},
	abstract = {We propose a class of kernel-based two-sample tests, which aim to determine whether two sets of samples are drawn from the same distribution. Our tests are constructed from kernels parameterized by deep neural nets, trained to maximize test power. These tests adapt to variations in distribution smoothness and shape over space, and are especially suited to high dimensions and complex data. By contrast, the simpler kernels used in prior kernel testing work are spatially homogeneous, and adaptive only in lengthscale. We explain how this scheme includes popular classifier-based two-sample tests as a special case, but improves on them in general. We provide the first proof of consistency for the proposed adaptation method, which applies both to kernels on deep features and to simpler radial basis kernels or multiple kernel learning. In experiments, we establish the superior performance of our deep kernels in hypothesis testing on benchmark and real-world data. The code of our deep-kernel-based two sample tests is available at https://github.com/fengliu90/{DK}-for-{TST}.},
	journaltitle = {{arXiv}:2002.09116 [cs, stat]},
	author = {Liu, Feng and Xu, Wenkai and Lu, Jie and Zhang, Guangquan and Gretton, Arthur and Sutherland, D. J.},
	urldate = {2020-12-26},
	date = {2020-07-15},
	eprinttype = {arxiv},
	eprint = {2002.09116},
	keywords = {good, tbr},
	file = {Liu et al_2020_Learning Deep Kernels for Non-Parametric Two-Sample Tests.pdf:/Users/wpq/Dropbox (MIT)/zotero/Liu et al_2020_Learning Deep Kernels for Non-Parametric Two-Sample Tests.pdf:application/pdf}
}

@article{smolaHilbertSpaceEmbedding2007,
	title = {A Hilbert Space Embedding for Distributions},
	abstract = {We describe a technique for comparing distributions without the need for density estimation as an intermediate step. Our approach relies on mapping the distributions into a reproducing kernel Hilbert space. Applications of this technique can be found in two-sample tests, which are used for determining whether two sets of observations arise from the same distribution, covariate shift correction, local learning, measures of independence, and density estimation.},
	pages = {20},
	author = {Smola, Alex and Gretton, Arthur and Song, Le and Scholkopf, Bernhard},
	date = {2007},
	langid = {english},
	keywords = {tbr},
	file = {Smola et al_2007_A Hilbert Space Embedding for Distributions.pdf:/Users/wpq/Dropbox (MIT)/zotero/Smola et al_2007_A Hilbert Space Embedding for Distributions.pdf:application/pdf}
}

@article{grettonKernelMethodsMeasuring2005,
	title = {Kernel Methods for Measuring Independence},
	abstract = {We introduce two new functionals, the constrained covariance and the kernel mutual information, to measure the degree of independence of random variables. These quantities are both based on the covariance between functions of the random variables in reproducing kernel Hilbert spaces ({RKHSs}). We prove that when the {RKHSs} are universal, both functionals are zero if and only if the random variables are pairwise independent. We also show that the kernel mutual information is an upper bound near independence on the Parzen window estimate of the mutual information. Analogous results apply for two correlation-based dependence functionals introduced earlier: we show the kernel canonical correlation and the kernel generalised variance to be independence measures for universal kernels, and prove the latter to be an upper bound on the mutual information near independence. The performance of the kernel dependence functionals in measuring independence is veriﬁed in the context of independent component analysis.},
	pages = {55},
	author = {Gretton, Arthur and Herbrich, Ralf and Smola, Alexander and Bousquet, Olivier and Schölkopf, Bernhard},
	date = {2005},
	langid = {english},
	keywords = {tbr},
	file = {Gretton et al_2005_Kernel Methods for Measuring Independence.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gretton et al_2005_Kernel Methods for Measuring Independence.pdf:application/pdf}
}

@article{wilsonStochasticVariationalDeep2016,
	title = {Stochastic Variational Deep Kernel Learning},
	url = {https://arxiv.org/abs/1611.00336v2},
	abstract = {Deep kernel learning combines the non-parametric flexibility of kernel
methods with the inductive biases of deep learning architectures. We propose a
novel deep kernel learning model and stochastic variational inference procedure
which generalizes deep kernel learning approaches to enable classification,
multi-task learning, additive covariance structures, and stochastic gradient
training. Specifically, we apply additive base kernels to subsets of output
features from deep neural architectures, and jointly learn the parameters of
the base kernels and deep network through a Gaussian process marginal
likelihood objective. Within this framework, we derive an efficient form of
stochastic variational inference which leverages local kernel interpolation,
inducing points, and structure exploiting algebra. We show improved performance
over stand alone deep networks, {SVMs}, and state of the art scalable Gaussian
processes on several classification benchmarks, including an airline delay
dataset containing 6 million training points, {CIFAR}, and {ImageNet}.},
	author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
	urldate = {2020-12-28},
	date = {2016-11-01},
	langid = {english},
	keywords = {tbr},
	file = {Wilson et al_2016_Stochastic Variational Deep Kernel Learning.pdf:/Users/wpq/Dropbox (MIT)/zotero/Wilson et al_2016_Stochastic Variational Deep Kernel Learning.pdf:application/pdf}
}

@article{salakhutdinovDeepKernelLearning2016,
	title = {Deep Kernel Learning},
	abstract = {We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric ﬂexibility of kernel methods. Speciﬁcally, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation. These closed-form kernels can be used as drop-in replacements for standard kernels, with beneﬁts in expressive power and scalability. We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process. Inference and learning cost O(n) for n training points, and predictions cost O(1) per test point. On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with ﬂexible kernel learning models, and stand-alone deep architectures.},
	pages = {19},
	author = {Salakhutdinov, Ruslan and Hu, Zhiting and Xing, Eric P},
	date = {2016},
	langid = {english},
	keywords = {tbr},
	file = {Salakhutdinov et al_2016_Deep Kernel Learning.pdf:/Users/wpq/Dropbox (MIT)/zotero/Salakhutdinov et al_2016_Deep Kernel Learning.pdf:application/pdf}
}

@article{wilsonCovarianceKernelsFast2014,
	title = {Covariance Kernels for Fast Automatic Pattern Discovery and Extrapolation with Gaussian Processes},
	abstract = {Truly intelligent systems are capable of pattern discovery and extrapolation without human intervention. Bayesian nonparametric models, which can uniquely represent expressive prior information and detailed inductive biases, provide a distinct opportunity to develop intelligent systems, with applications in essentially any learning and prediction task.},
	pages = {226},
	author = {Wilson, Andrew Gordon},
	date = {2014},
	langid = {english},
	keywords = {tbr},
	file = {Wilson_2014_Covariance Kernels for Fast Automatic Pattern Discovery and Extrapolation with Gaussian Processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Wilson_2014_Covariance Kernels for Fast Automatic Pattern Discovery and Extrapolation with Gaussian Processes.pdf:application/pdf}
}

@article{binkowskiDemystifyingMMDGANs2018,
	title = {Demystifying {MMD} {GANs}},
	url = {http://arxiv.org/abs/1801.01401},
	abstract = {We investigate the training and performance of generative adversarial networks using the Maximum Mean Discrepancy ({MMD}) as critic, termed {MMD} {GANs}. As our main theoretical contribution, we clarify the situation with bias in {GAN} loss functions raised by recent work: we show that gradient estimators used in the optimization process for both {MMD} {GANs} and Wasserstein {GANs} are unbiased, but learning a discriminator based on samples leads to biased gradients for the generator parameters. We also discuss the issue of kernel choice for the {MMD} critic, and characterize the kernel corresponding to the energy distance used for the Cramer {GAN} critic. Being an integral probability metric, the {MMD} benefits from training strategies recently developed for Wasserstein {GANs}. In experiments, the {MMD} {GAN} is able to employ a smaller critic network than the Wasserstein {GAN}, resulting in a simpler and faster-training algorithm with matching performance. We also propose an improved measure of {GAN} convergence, the Kernel Inception Distance, and show how to use it to dynamically adapt learning rates during {GAN} training.},
	journaltitle = {{arXiv}:1801.01401 [cs, stat]},
	author = {Bińkowski, Mikołaj and Sutherland, Dougal J. and Arbel, Michael and Gretton, Arthur},
	urldate = {2020-12-28},
	date = {2018-03-21},
	eprinttype = {arxiv},
	eprint = {1801.01401},
	keywords = {tbr},
	file = {Binkowski et al_2018_Demystifying MMD GANs.pdf:/Users/wpq/Dropbox (MIT)/zotero/Binkowski et al_2018_Demystifying MMD GANs.pdf:application/pdf}
}

@article{grettonNotesMeanEmbeddings2019,
	title = {Notes on mean embeddings and covariance operators},
	url = {http://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture5_covarianceOperator.pdf},
	pages = {15},
	author = {Gretton, Arthur},
	date = {2019},
	langid = {english},
	keywords = {tbr},
	file = {Gretton_2019_Notes on mean embeddings and covariance operators.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gretton_2019_Notes on mean embeddings and covariance operators.pdf:application/pdf}
}

@article{grettonWhatRKHS2012,
	title = {What is an {RKHS}?},
	url = {http://www.stats.ox.ac.uk/~sejdinov/teaching/atml14/Theory_2014.pdf},
	author = {Gretton, Arthur},
	date = {2012},
	keywords = {good, read},
	file = {2012_What is an RKHS.pdf:/Users/wpq/Dropbox (MIT)/zotero/2012_What is an RKHS.pdf:application/pdf}
}