
@article{grettonKernelTwoSampleTest2012,
	title = {A Kernel Two-Sample Test},
	volume = {13},
	url = {http://jmlr.org/papers/v13/gretton12a.html},
	abstract = {We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions.  Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space ({RKHS}), and is called the maximum mean discrepancy ({MMD}).  We present two distribution-free tests based on large deviation bounds for the {MMD}, and a third test based on the asymptotic distribution of this statistic.  The {MMD} can be computed in quadratic time, although efficient linear time approximations are available.  Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an {RKHS}.  We apply our two-sample tests  to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly.  Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
	pages = {723--773},
	number = {25},
	journaltitle = {Journal of Machine Learning Research},
	author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Schölkopf, Bernhard and Smola, Alexander},
	urldate = {2020-06-21},
	date = {2012},
	keywords = {good, tbr},
	file = {Gretton et al_2012_A Kernel Two-Sample Test.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gretton et al_2012_A Kernel Two-Sample Test.pdf:application/pdf}
}

@article{sutherlandGenerativeModelsModel2017a,
	title = {Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy},
	url = {http://arxiv.org/abs/1611.04488},
	abstract = {We propose a method to optimize the representation and distinguishability of samples from two probability distributions, by maximizing the estimated power of a statistical test based on the maximum mean discrepancy ({MMD}). This optimized {MMD} is applied to the setting of unsupervised learning by generative adversarial networks ({GAN}), in which a model attempts to generate realistic samples, and a discriminator attempts to tell these apart from data samples. In this context, the {MMD} may be used in two roles: first, as a discriminator, either directly on the samples, or on features of the samples. Second, the {MMD} can be used to evaluate the performance of a generative model, by testing the model's samples against a reference data set. In the latter role, the optimized {MMD} is particularly helpful, as it gives an interpretable indication of how the model and data distributions differ, even in cases where individual model samples are not easily distinguished either by eye or by classifier.},
	journaltitle = {{arXiv}:1611.04488 [cs, stat]},
	author = {Sutherland, Dougal J. and Tung, Hsiao-Yu and Strathmann, Heiko and De, Soumyajit and Ramdas, Aaditya and Smola, Alex and Gretton, Arthur},
	urldate = {2020-06-22},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1611.04488},
	keywords = {read},
	file = {Sutherland et al_2019_Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy.pdf:/Users/wpq/Dropbox (MIT)/zotero/Sutherland et al_2019_Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy.pdf:application/pdf}
}

@incollection{jaakkolaExploitingGenerativeModels1999,
	title = {Exploiting Generative Models in Discriminative Classifiers},
	url = {http://papers.nips.cc/paper/1520-exploiting-generative-models-in-discriminative-classifiers.pdf},
	pages = {487--493},
	booktitle = {Advances in Neural Information Processing Systems 11},
	publisher = {{MIT} Press},
	author = {Jaakkola, Tommi and Haussler, David},
	editor = {Kearns, M. J. and Solla, S. A. and Cohn, D. A.},
	urldate = {2020-08-25},
	date = {1999},
	keywords = {tbr},
	file = {Jaakkola_Haussler_1999_Exploiting Generative Models in Discriminative Classifiers.pdf:/Users/wpq/Dropbox (MIT)/zotero/Jaakkola_Haussler_1999_Exploiting Generative Models in Discriminative Classifiers.pdf:application/pdf}
}

@article{sriperumbudurHilbertSpaceEmbeddings2010,
	title = {Hilbert Space Embeddings and Metrics on Probability Measures},
	abstract = {A Hilbert space embedding for probability measures has recently been proposed, with applications including dimensionality reduction, homogeneity testing, and independence testing. This embedding represents any probability measure as a mean element in a reproducing kernel Hilbert space ({RKHS}). A pseudometric on the space of probability measures can be deﬁned as the distance between distribution embeddings: we denote this as γk, indexed by the kernel function k that deﬁnes the inner product in the {RKHS}.},
	pages = {45},
	author = {Sriperumbudur, Bharath K and Gretton, Arthur and Fukumizu, Kenji and Scholkopf, Bernhard and Lanckriet, Gert R G},
	date = {2010},
	langid = {english},
	keywords = {tbr},
	file = {Sriperumbudur et al_2010_Hilbert Space Embeddings and Metrics on Probability Measures.pdf:/Users/wpq/Dropbox (MIT)/zotero/Sriperumbudur et al_2010_Hilbert Space Embeddings and Metrics on Probability Measures.pdf:application/pdf}
}

@inproceedings{leslieSPECTRUMKERNELSTRING2001,
	location = {Kauai, Hawaii, {USA}},
	title = {{THE} {SPECTRUM} {KERNEL}: A {STRING} {KERNEL} {FOR} {SVM} {PROTEIN} {CLASSIFICATION}},
	isbn = {978-981-02-4777-5 978-981-279-962-3},
	url = {http://www.worldscientific.com/doi/abs/10.1142/9789812799623_0053},
	doi = {10.1142/9789812799623_0053},
	shorttitle = {{THE} {SPECTRUM} {KERNEL}},
	eventtitle = {Proceedings of the Pacific Symposium},
	pages = {564--575},
	booktitle = {Biocomputing 2002},
	publisher = {{WORLD} {SCIENTIFIC}},
	author = {Leslie, Christina and Eskin, Eleazar and Noble, William Stafford},
	urldate = {2020-12-23},
	date = {2001-12},
	langid = {english},
	keywords = {tbr},
	file = {Leslie et al_2001_THE SPECTRUM KERNEL - A STRING KERNEL FOR SVM PROTEIN CLASSIFICATION.pdf:/Users/wpq/Dropbox (MIT)/zotero/Leslie et al_2001_THE SPECTRUM KERNEL - A STRING KERNEL FOR SVM PROTEIN CLASSIFICATION.pdf:application/pdf}
}

@inproceedings{choLargeMarginClassificationHyperbolic2019,
	title = {Large-Margin Classification in Hyperbolic Space},
	url = {http://proceedings.mlr.press/v89/cho19a.html},
	abstract = {Representing data in hyperbolic space can effectively capture latent hierarchical relationships. To enable accurate classification of points in hyperbolic space while respecting their hyperbolic ge...},
	eventtitle = {The 22nd International Conference on Artificial Intelligence and Statistics},
	pages = {1832--1840},
	booktitle = {The 22nd International Conference on Artificial Intelligence and Statistics},
	publisher = {{PMLR}},
	author = {Cho, Hyunghoon and {DeMeo}, Benjamin and Peng, Jian and Berger, Bonnie},
	urldate = {2020-12-24},
	date = {2019-04-11},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	keywords = {tbr},
	file = {Cho et al_2019_Large-Margin Classification in Hyperbolic Space.pdf:/Users/wpq/Dropbox (MIT)/zotero/Cho et al_2019_Large-Margin Classification in Hyperbolic Space.pdf:application/pdf}
}

@article{ngCs229kernel2019,
	title = {cs229-kernel},
	url = {http://cs229.stanford.edu/summer2020/cs229-notes3.pdf},
	author = {Ng, Andrew},
	date = {2019},
	keywords = {read},
	file = {Ng_2019_cs229-kernel.pdf:/Users/wpq/Dropbox (MIT)/zotero/Ng_2019_cs229-kernel.pdf:application/pdf}
}

@article{rahimiRandomFeaturesLargeScale2007,
	title = {Random Features for Large-Scale Kernel Machines},
	abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. Our randomized features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user speciﬁed shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classiﬁcation and regression tasks linear machine learning algorithms that use these features outperform state-of-the-art large-scale kernel machines.},
	pages = {8},
	author = {Rahimi, Ali and Recht, Ben},
	date = {2007},
	langid = {english},
	keywords = {read},
	file = {Rahimi_Recht_2007_Random Features for Large-Scale Kernel Machines.pdf:/Users/wpq/Dropbox (MIT)/zotero/Rahimi_Recht_2007_Random Features for Large-Scale Kernel Machines.pdf:application/pdf}
}

@article{vertPrimerKernelMethods2004,
	title = {A primer on kernel methods},
	pages = {42},
	author = {Vert, Jean-Philippe and Tsuda, Koji and Scholkopf, Bernhard},
	date = {2004},
	langid = {english},
	keywords = {read},
	file = {Vert et al_2004_A primer on kernel methods.pdf:/Users/wpq/Dropbox (MIT)/zotero/Vert et al_2004_A primer on kernel methods.pdf:application/pdf}
}

@article{boserTrainingAlgorithmOptimal1992,
	title = {A Training Algorithm for Optimal Margin Classifiers},
	author = {Boser, Bernhard and Guyon, Isabelle and Vapnik, Vladimir},
	date = {1992},
	keywords = {tbr},
	file = {Boser et al_1992_A Training Algorithm for Optimal Margin Classifiers.pdf:/Users/wpq/Dropbox (MIT)/zotero/Boser et al_1992_A Training Algorithm for Optimal Margin Classifiers.pdf:application/pdf}
}

@article{evgeniouRegularizationNetworksSupport2000,
	title = {Regularization Networks and Support Vector Machines},
	abstract = {Regularization Networks and Support Vector Machines are techniques for solving certain problems of learning from examples – in particular the regression problem of approximating a multivariate function from sparse data. Radial Basis Functions, for example, are a special case of both regularization and Support Vector Machines. We review both formulations in the context of Vapnik’s theory of statistical learning which provides a general foundation for the learning problem, combining functional analysis and statistics. The emphasis is on regression: classiﬁcation is treated as a special case.},
	pages = {53},
	author = {Evgeniou, Theodoros and Pontil, Massimiliano and Poggio, Tomaso},
	date = {2000},
	langid = {english},
	keywords = {tbr},
	file = {Evgeniou et al_2000_Regularization Networks and Support Vector Machines.pdf:/Users/wpq/Dropbox (MIT)/zotero/Evgeniou et al_2000_Regularization Networks and Support Vector Machines.pdf:application/pdf}
}

@article{wahbaIntroductionReproducingKernel2003,
	title = {An introduction to reproducing kernel hilbert spaces and why they are so useful},
	volume = {36},
	issn = {1474-6670},
	url = {http://www.sciencedirect.com/science/article/pii/S1474667017348152},
	doi = {10.1016/S1474-6670(17)34815-2},
	series = {13th {IFAC} Symposium on System Identification ({SYSID} 2003), Rotterdam, The Netherlands, 27-29 August, 2003},
	abstract = {We review some of the basic facts about reproducing kernel Hilbert spaces ({RKHS}), and the solution of optimization problems in {RKHS}, These facts provide some clues to how useful {RKHS}-based methods can be in curve fitting, function estimation, model description, model fitting and ill-posed inverse problems. A number of references are made to mostly older works of the author, colleagues and former students - not an attempt at a balanced review of the literature. The growth of the further development and application of {RKHS} based methods is demonstrated in the other papers in the invited Session {INV}-2 of this 13th {IFAC} Symposium on System Identification ({SYSID}-2003), and elsewhere in {SISID}-2003. A recent advanced google search for "reproducing kernel" turned up over 4400 entries.},
	pages = {525--528},
	number = {16},
	journaltitle = {{IFAC} Proceedings Volumes},
	shortjournal = {{IFAC} Proceedings Volumes},
	author = {Wahba, Grace},
	urldate = {2020-12-24},
	date = {2003-09-01},
	langid = {english},
	keywords = {tbr},
	file = {Wahba_2003_An introduction to reproducing kernel hilbert spaces and why they are so useful.pdf:/Users/wpq/Dropbox (MIT)/zotero/Wahba_2003_An introduction to reproducing kernel hilbert spaces and why they are so useful.pdf:application/pdf}
}

@book{rifkinEverythingOldNew2002,
	title = {Everything Old Is New Again: A Fresh Look at Historical Approaches {IN} {MACHINE} {LEARNING}},
	shorttitle = {Everything Old Is New Again},
	author = {Rifkin, Ryan Michael},
	date = {2002},
	keywords = {read},
	file = {Rifkin_2002_Everything Old Is New Again - A Fresh Look at Historical Approaches IN MACHINE LEARNING.pdf:/Users/wpq/Dropbox (MIT)/zotero/Rifkin_2002_Everything Old Is New Again - A Fresh Look at Historical Approaches IN MACHINE LEARNING.pdf:application/pdf}
}

@article{songLearningHilbertSpace2008,
	title = {Learning via Hilbert Space Embedding of Distributions},
	author = {Song, Le},
	date = {2008},
	keywords = {read},
	file = {Song_2008_Learning via Hilbert Space Embedding of Distributions.pdf:/Users/wpq/Dropbox (MIT)/zotero/Song_2008_Learning via Hilbert Space Embedding of Distributions.pdf:application/pdf}
}

@article{sunDifferentiableCompositionalKernel2018,
	title = {Differentiable Compositional Kernel Learning for Gaussian Processes},
	url = {http://arxiv.org/abs/1806.04326},
	abstract = {The generalization properties of Gaussian processes depend heavily on the choice of kernel, and this choice remains a dark art. We present the Neural Kernel Network ({NKN}), a flexible family of kernels represented by a neural network. The {NKN} architecture is based on the composition rules for kernels, so that each unit of the network corresponds to a valid kernel. It can compactly approximate compositional kernel structures such as those used by the Automatic Statistician (Lloyd et al., 2014), but because the architecture is differentiable, it is end-to-end trainable with gradient-based optimization. We show that the {NKN} is universal for the class of stationary kernels. Empirically we demonstrate pattern discovery and extrapolation abilities of {NKN} on several tasks that depend crucially on identifying the underlying structure, including time series and texture extrapolation, as well as Bayesian optimization.},
	journaltitle = {{arXiv}:1806.04326 [cs, stat]},
	author = {Sun, Shengyang and Zhang, Guodong and Wang, Chaoqi and Zeng, Wenyuan and Li, Jiaman and Grosse, Roger},
	urldate = {2020-12-25},
	date = {2018-08-05},
	eprinttype = {arxiv},
	eprint = {1806.04326},
	keywords = {tbr},
	file = {Sun et al_2018_Differentiable Compositional Kernel Learning for Gaussian Processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Sun et al_2018_Differentiable Compositional Kernel Learning for Gaussian Processes.pdf:application/pdf}
}

@article{sutherlandScalableFlexibleActive2016,
	title = {Scalable, Flexible and Active Learning on Distributions},
	author = {Sutherland, Dougal},
	date = {2016},
	keywords = {good, tbr},
	file = {Sutherland_2016_Scalable, Flexible and Active Learning on Distributions.pdf:/Users/wpq/Dropbox (MIT)/zotero/Sutherland_2016_Scalable, Flexible and Active Learning on Distributions.pdf:application/pdf}
}

@article{grettonIntroductionRKHSSimple2019,
	title = {Introduction to {RKHS}, and some simple kernel algorithms},
	url = {http://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf},
	pages = {33},
	author = {Gretton, Arthur},
	date = {2019},
	langid = {english},
	keywords = {read},
	file = {Gretton_2019_Introduction to RKHS, and some simple kernel algorithms.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gretton_2019_Introduction to RKHS, and some simple kernel algorithms.pdf:application/pdf}
}

@article{muandetKernelMeanEmbedding2017,
	title = {Kernel Mean Embedding of Distributions: A Review and Beyond},
	volume = {10},
	issn = {1935-8237, 1935-8245},
	url = {http://arxiv.org/abs/1605.09522},
	doi = {10.1561/2200000060},
	shorttitle = {Kernel Mean Embedding of Distributions},
	abstract = {A Hilbert space embedding of a distribution---in short, a kernel mean embedding---has recently emerged as a powerful tool for machine learning and inference. The basic idea behind this framework is to map distributions into a reproducing kernel Hilbert space ({RKHS}) in which the whole arsenal of kernel methods can be extended to probability measures. It can be viewed as a generalization of the original "feature map" common to support vector machines ({SVMs}) and other kernel methods. While initially closely associated with the latter, it has meanwhile found application in fields ranging from kernel machines and probabilistic modeling to statistical inference, causal discovery, and deep learning. The goal of this survey is to give a comprehensive review of existing work and recent advances in this research area, and to discuss the most challenging issues and open problems that could lead to new research directions. The survey begins with a brief introduction to the {RKHS} and positive definite kernels which forms the backbone of this survey, followed by a thorough discussion of the Hilbert space embedding of marginal distributions, theoretical guarantees, and a review of its applications. The embedding of distributions enables us to apply {RKHS} methods to probability measures which prompts a wide range of applications such as kernel two-sample testing, independent testing, and learning on distributional data. Next, we discuss the Hilbert space embedding for conditional distributions, give theoretical insights, and review some applications. The conditional mean embedding enables us to perform sum, product, and Bayes' rules---which are ubiquitous in graphical model, probabilistic inference, and reinforcement learning---in a non-parametric way. We then discuss relationships between this framework and other related areas. Lastly, we give some suggestions on future research directions.},
	pages = {1--141},
	number = {1},
	journaltitle = {Foundations and Trends® in Machine Learning},
	shortjournal = {{FNT} in Machine Learning},
	author = {Muandet, Krikamol and Fukumizu, Kenji and Sriperumbudur, Bharath and Schölkopf, Bernhard},
	urldate = {2020-12-25},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1605.09522},
	keywords = {good, read},
	file = {Muandet et al_2017_Kernel Mean Embedding of Distributions - A Review and Beyond.pdf:/Users/wpq/Dropbox (MIT)/zotero/Muandet et al_2017_Kernel Mean Embedding of Distributions - A Review and Beyond.pdf:application/pdf}
}

@incollection{scholkopfKernelPrincipalComponent1997,
	location = {Berlin, Heidelberg},
	title = {Kernel principal component analysis},
	volume = {1327},
	isbn = {978-3-540-63631-1 978-3-540-69620-9},
	url = {http://link.springer.com/10.1007/BFb0020217},
	abstract = {A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can e ciently compute principal components in high dimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.},
	pages = {583--588},
	booktitle = {Artificial Neural Networks — {ICANN}'97},
	publisher = {Springer Berlin Heidelberg},
	author = {Schölkopf, Bernhard and Smola, Alexander and Müller, Klaus-Robert},
	editor = {Gerstner, Wulfram and Germond, Alain and Hasler, Martin and Nicoud, Jean-Daniel},
	editorb = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan},
	editorbtype = {redactor},
	urldate = {2020-12-26},
	date = {1997},
	langid = {english},
	doi = {10.1007/BFb0020217},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {tbr},
	file = {Scholkopf et al_1997_Kernel principal component analysis.pdf:/Users/wpq/Dropbox (MIT)/zotero/Scholkopf et al_1997_Kernel principal component analysis.pdf:application/pdf}
}

@article{liuLearningDeepKernels2020,
	title = {Learning Deep Kernels for Non-Parametric Two-Sample Tests},
	url = {http://arxiv.org/abs/2002.09116},
	abstract = {We propose a class of kernel-based two-sample tests, which aim to determine whether two sets of samples are drawn from the same distribution. Our tests are constructed from kernels parameterized by deep neural nets, trained to maximize test power. These tests adapt to variations in distribution smoothness and shape over space, and are especially suited to high dimensions and complex data. By contrast, the simpler kernels used in prior kernel testing work are spatially homogeneous, and adaptive only in lengthscale. We explain how this scheme includes popular classifier-based two-sample tests as a special case, but improves on them in general. We provide the first proof of consistency for the proposed adaptation method, which applies both to kernels on deep features and to simpler radial basis kernels or multiple kernel learning. In experiments, we establish the superior performance of our deep kernels in hypothesis testing on benchmark and real-world data. The code of our deep-kernel-based two sample tests is available at https://github.com/fengliu90/{DK}-for-{TST}.},
	journaltitle = {{arXiv}:2002.09116 [cs, stat]},
	author = {Liu, Feng and Xu, Wenkai and Lu, Jie and Zhang, Guangquan and Gretton, Arthur and Sutherland, D. J.},
	urldate = {2020-12-26},
	date = {2020-07-15},
	eprinttype = {arxiv},
	eprint = {2002.09116},
	keywords = {read},
	file = {Liu et al_2020_Learning Deep Kernels for Non-Parametric Two-Sample Tests.pdf:/Users/wpq/Dropbox (MIT)/zotero/Liu et al_2020_Learning Deep Kernels for Non-Parametric Two-Sample Tests.pdf:application/pdf}
}

@article{smolaHilbertSpaceEmbedding2007,
	title = {A Hilbert Space Embedding for Distributions},
	abstract = {We describe a technique for comparing distributions without the need for density estimation as an intermediate step. Our approach relies on mapping the distributions into a reproducing kernel Hilbert space. Applications of this technique can be found in two-sample tests, which are used for determining whether two sets of observations arise from the same distribution, covariate shift correction, local learning, measures of independence, and density estimation.},
	pages = {20},
	author = {Smola, Alex and Gretton, Arthur and Song, Le and Scholkopf, Bernhard},
	date = {2007},
	langid = {english},
	keywords = {tbr},
	file = {Smola et al_2007_A Hilbert Space Embedding for Distributions.pdf:/Users/wpq/Dropbox (MIT)/zotero/Smola et al_2007_A Hilbert Space Embedding for Distributions.pdf:application/pdf}
}

@article{grettonKernelMethodsMeasuring2005,
	title = {Kernel Methods for Measuring Independence},
	abstract = {We introduce two new functionals, the constrained covariance and the kernel mutual information, to measure the degree of independence of random variables. These quantities are both based on the covariance between functions of the random variables in reproducing kernel Hilbert spaces ({RKHSs}). We prove that when the {RKHSs} are universal, both functionals are zero if and only if the random variables are pairwise independent. We also show that the kernel mutual information is an upper bound near independence on the Parzen window estimate of the mutual information. Analogous results apply for two correlation-based dependence functionals introduced earlier: we show the kernel canonical correlation and the kernel generalised variance to be independence measures for universal kernels, and prove the latter to be an upper bound on the mutual information near independence. The performance of the kernel dependence functionals in measuring independence is veriﬁed in the context of independent component analysis.},
	pages = {55},
	author = {Gretton, Arthur and Herbrich, Ralf and Smola, Alexander and Bousquet, Olivier and Schölkopf, Bernhard},
	date = {2005},
	langid = {english},
	keywords = {tbr},
	file = {Gretton et al_2005_Kernel Methods for Measuring Independence.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gretton et al_2005_Kernel Methods for Measuring Independence.pdf:application/pdf}
}

@article{wilsonStochasticVariationalDeep2016,
	title = {Stochastic Variational Deep Kernel Learning},
	url = {https://arxiv.org/abs/1611.00336v2},
	abstract = {Deep kernel learning combines the non-parametric flexibility of kernel
methods with the inductive biases of deep learning architectures. We propose a
novel deep kernel learning model and stochastic variational inference procedure
which generalizes deep kernel learning approaches to enable classification,
multi-task learning, additive covariance structures, and stochastic gradient
training. Specifically, we apply additive base kernels to subsets of output
features from deep neural architectures, and jointly learn the parameters of
the base kernels and deep network through a Gaussian process marginal
likelihood objective. Within this framework, we derive an efficient form of
stochastic variational inference which leverages local kernel interpolation,
inducing points, and structure exploiting algebra. We show improved performance
over stand alone deep networks, {SVMs}, and state of the art scalable Gaussian
processes on several classification benchmarks, including an airline delay
dataset containing 6 million training points, {CIFAR}, and {ImageNet}.},
	author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
	urldate = {2020-12-28},
	date = {2016-11-01},
	langid = {english},
	keywords = {tbr},
	file = {Wilson et al_2016_Stochastic Variational Deep Kernel Learning.pdf:/Users/wpq/Dropbox (MIT)/zotero/Wilson et al_2016_Stochastic Variational Deep Kernel Learning.pdf:application/pdf}
}

@article{salakhutdinovDeepKernelLearning2016,
	title = {Deep Kernel Learning},
	abstract = {We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric ﬂexibility of kernel methods. Speciﬁcally, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation. These closed-form kernels can be used as drop-in replacements for standard kernels, with beneﬁts in expressive power and scalability. We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process. Inference and learning cost O(n) for n training points, and predictions cost O(1) per test point. On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with ﬂexible kernel learning models, and stand-alone deep architectures.},
	pages = {19},
	author = {Salakhutdinov, Ruslan and Hu, Zhiting and Xing, Eric P},
	date = {2016},
	langid = {english},
	keywords = {good, tbr},
	file = {Salakhutdinov et al_2016_Deep Kernel Learning.pdf:/Users/wpq/Dropbox (MIT)/zotero/Salakhutdinov et al_2016_Deep Kernel Learning.pdf:application/pdf}
}

@article{wilsonCovarianceKernelsFast2014,
	title = {Covariance Kernels for Fast Automatic Pattern Discovery and Extrapolation with Gaussian Processes},
	abstract = {Truly intelligent systems are capable of pattern discovery and extrapolation without human intervention. Bayesian nonparametric models, which can uniquely represent expressive prior information and detailed inductive biases, provide a distinct opportunity to develop intelligent systems, with applications in essentially any learning and prediction task.},
	pages = {226},
	author = {Wilson, Andrew Gordon},
	date = {2014},
	langid = {english},
	keywords = {tbr},
	file = {Wilson_2014_Covariance Kernels for Fast Automatic Pattern Discovery and Extrapolation with Gaussian Processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Wilson_2014_Covariance Kernels for Fast Automatic Pattern Discovery and Extrapolation with Gaussian Processes.pdf:application/pdf}
}

@article{binkowskiDemystifyingMMDGANs2018,
	title = {Demystifying {MMD} {GANs}},
	url = {http://arxiv.org/abs/1801.01401},
	abstract = {We investigate the training and performance of generative adversarial networks using the Maximum Mean Discrepancy ({MMD}) as critic, termed {MMD} {GANs}. As our main theoretical contribution, we clarify the situation with bias in {GAN} loss functions raised by recent work: we show that gradient estimators used in the optimization process for both {MMD} {GANs} and Wasserstein {GANs} are unbiased, but learning a discriminator based on samples leads to biased gradients for the generator parameters. We also discuss the issue of kernel choice for the {MMD} critic, and characterize the kernel corresponding to the energy distance used for the Cramer {GAN} critic. Being an integral probability metric, the {MMD} benefits from training strategies recently developed for Wasserstein {GANs}. In experiments, the {MMD} {GAN} is able to employ a smaller critic network than the Wasserstein {GAN}, resulting in a simpler and faster-training algorithm with matching performance. We also propose an improved measure of {GAN} convergence, the Kernel Inception Distance, and show how to use it to dynamically adapt learning rates during {GAN} training.},
	journaltitle = {{arXiv}:1801.01401 [cs, stat]},
	author = {Bińkowski, Mikołaj and Sutherland, Dougal J. and Arbel, Michael and Gretton, Arthur},
	urldate = {2020-12-28},
	date = {2018-03-21},
	eprinttype = {arxiv},
	eprint = {1801.01401},
	keywords = {tbr},
	file = {Binkowski et al_2018_Demystifying MMD GANs.pdf:/Users/wpq/Dropbox (MIT)/zotero/Binkowski et al_2018_Demystifying MMD GANs.pdf:application/pdf}
}

@article{grettonNotesMeanEmbeddings2019,
	title = {Notes on mean embeddings and covariance operators},
	url = {http://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture5_covarianceOperator.pdf},
	pages = {15},
	author = {Gretton, Arthur},
	date = {2019},
	langid = {english},
	keywords = {read},
	file = {Gretton_2019_Notes on mean embeddings and covariance operators.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gretton_2019_Notes on mean embeddings and covariance operators.pdf:application/pdf}
}

@article{grettonWhatRKHS2012,
	title = {What is an {RKHS}?},
	url = {http://www.stats.ox.ac.uk/~sejdinov/teaching/atml14/Theory_2014.pdf},
	author = {Gretton, Arthur},
	date = {2012},
	keywords = {read},
	file = {Gretton_2012_What is an RKHS.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gretton_2012_What is an RKHS.pdf:application/pdf}
}

@article{yangCarteLearningFast2015,
	title = {A la Carte — Learning Fast Kernels},
	abstract = {Kernel methods have great promise for learning rich statistical representations of large modern datasets. However, compared to neural networks, kernel methods have been perceived as lacking in scalability and ﬂexibility. We introduce a family of fast, ﬂexible, lightly parametrized and general purpose kernel learning methods, derived from Fastfood basis function expansions. We provide mechanisms to learn the properties of groups of spectral frequencies in these expansions, which require only O(m log d) time and O(m) memory, for m basis functions and d input dimensions. We show that the proposed methods can learn a wide class of kernels, outperforming the alternatives in accuracy, speed, and memory consumption.},
	pages = {9},
	author = {Yang, Zichao and Smola, Alexander J and Song, Le and Wilson, Andrew Gordon},
	date = {2015},
	langid = {english},
	keywords = {tbr},
	file = {Yang et al_2015_A la Carte — Learning Fast Kernels.pdf:/Users/wpq/Dropbox (MIT)/zotero/Yang et al_2015_A la Carte — Learning Fast Kernels.pdf:application/pdf}
}

@book{lampertKernelMethodsComputer2009,
	title = {Kernel Methods in Computer Vision},
	author = {Lampert, Christoph H.},
	date = {2009},
	keywords = {read},
	file = {Lampert_2009_Kernel Methods in Computer Vision.pdf:/Users/wpq/Dropbox (MIT)/zotero/Lampert_2009_Kernel Methods in Computer Vision.pdf:application/pdf}
}

@article{songFeatureSelectionDependence2012a,
	title = {Feature Selection via Dependence Maximization},
	abstract = {We introduce a framework for feature selection based on dependence maximization between the selected features and the labels of an estimation problem, using the Hilbert-Schmidt Independence Criterion. The key idea is that good features should be highly dependent on the labels. Our approach leads to a greedy procedure for feature selection. We show that a number of existing feature selectors are special cases of this framework. Experiments on both artiﬁcial and real-world data show that our feature selector works well in practice.},
	pages = {42},
	author = {Song, Le},
	date = {2012},
	langid = {english},
	keywords = {tbr},
	file = {Song_2012_Feature Selection via Dependence Maximization.pdf:/Users/wpq/Dropbox (MIT)/zotero/Song_2012_Feature Selection via Dependence Maximization.pdf:application/pdf}
}

@article{liKernelDependenceRegularizers2019,
	title = {Kernel Dependence Regularizers and Gaussian Processes with Applications to Algorithmic Fairness},
	url = {http://arxiv.org/abs/1911.04322},
	abstract = {Current adoption of machine learning in industrial, societal and economical activities has raised concerns about the fairness, equity and ethics of automated decisions. Predictive models are often developed using biased datasets and thus retain or even exacerbate biases in their decisions and recommendations. Removing the sensitive covariates, such as gender or race, is insufficient to remedy this issue since the biases may be retained due to other related covariates. We present a regularization approach to this problem that trades off predictive accuracy of the learned models (with respect to biased labels) for the fairness in terms of statistical parity, i.e. independence of the decisions from the sensitive covariates. In particular, we consider a general framework of regularized empirical risk minimization over reproducing kernel Hilbert spaces and impose an additional regularizer of dependence between predictors and sensitive covariates using kernel-based measures of dependence, namely the Hilbert-Schmidt Independence Criterion ({HSIC}) and its normalized version. This approach leads to a closed-form solution in the case of squared loss, i.e. ridge regression. Moreover, we show that the dependence regularizer has an interpretation as modifying the corresponding Gaussian process ({GP}) prior. As a consequence, a {GP} model with a prior that encourages fairness to sensitive variables can be derived, allowing principled hyperparameter selection and studying of the relative relevance of covariates under fairness constraints. Experimental results in synthetic examples and in real problems of income and crime prediction illustrate the potential of the approach to improve fairness of automated decisions.},
	journaltitle = {{arXiv}:1911.04322 [cs, stat]},
	author = {Li, Zhu and Perez-Suay, Adrian and Camps-Valls, Gustau and Sejdinovic, Dino},
	urldate = {2020-12-30},
	date = {2019-11-11},
	eprinttype = {arxiv},
	eprint = {1911.04322},
	keywords = {good, tbr},
	file = {Li et al_2019_Kernel Dependence Regularizers and Gaussian Processes with Applications to Algorithmic Fairness.pdf:/Users/wpq/Dropbox (MIT)/zotero/Li et al_2019_Kernel Dependence Regularizers and Gaussian Processes with Applications to Algorithmic Fairness.pdf:application/pdf}
}

@article{muandetLearningDistributionsSupport2013,
	title = {Learning from Distributions via Support Measure Machines},
	url = {http://arxiv.org/abs/1202.6504},
	abstract = {This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space ({RKHS}), we are able to apply many standard kernel-based learning techniques in straightforward fashion. To accomplish this, we construct a generalization of the support vector machine ({SVM}) called a support measure machine ({SMM}). Our analyses of {SMMs} provides several insights into their relationship to traditional {SVMs}. Based on such insights, we propose a flexible {SVM} (Flex-{SVM}) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework.},
	journaltitle = {{arXiv}:1202.6504 [cs, stat]},
	author = {Muandet, Krikamol and Fukumizu, Kenji and Dinuzzo, Francesco and Schölkopf, Bernhard},
	urldate = {2020-12-30},
	date = {2013-01-12},
	eprinttype = {arxiv},
	eprint = {1202.6504},
	keywords = {read},
	file = {Muandet et al_2013_Learning from Distributions via Support Measure Machines.pdf:/Users/wpq/Dropbox (MIT)/zotero/Muandet et al_2013_Learning from Distributions via Support Measure Machines.pdf:application/pdf}
}

@inproceedings{grettonMeasuringStatisticalDependence2005,
	location = {Berlin, Heidelberg},
	title = {Measuring statistical dependence with hilbert-schmidt norms},
	isbn = {978-3-540-29242-5},
	url = {https://doi.org/10.1007/11564089_7},
	doi = {10.1007/11564089_7},
	series = {{ALT}'05},
	abstract = {We propose an independence criterion based on the eigenspectrum of covariance operators in reproducing kernel Hilbert spaces ({RKHSs}), consisting of an empirical estimate of the Hilbert-Schmidt norm of the cross-covariance operator (we term this a Hilbert-Schmidt Independence Criterion, or {HSIC}). This approach has several advantages, compared with previous kernel-based independence criteria. First, the empirical estimate is simpler than any other kernel dependence test, and requires no user-defined regularisation. Second, there is a clearly defined population quantity which the empirical estimate approaches in the large sample limit, with exponential convergence guaranteed between the two: this ensures that independence tests based on {HSIC} do not suffer from slow learning rates. Finally, we show in the context of independent component analysis ({ICA}) that the performance of {HSIC} is competitive with that of previously published kernel-based criteria, and of other recently published {ICA} methods.},
	pages = {63--77},
	booktitle = {Proceedings of the 16th international conference on Algorithmic Learning Theory},
	publisher = {Springer-Verlag},
	author = {Gretton, Arthur and Bousquet, Olivier and Smola, Alex and Schölkopf, Bernhard},
	urldate = {2021-01-01},
	date = {2005-10-08},
	keywords = {read},
	file = {Gretton et al_2005_Measuring statistical dependence with hilbert-schmidt norms.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gretton et al_2005_Measuring statistical dependence with hilbert-schmidt norms.pdf:application/pdf}
}

@report{fukumizuDimensionalityReductionSupervised2003,
	location = {Fort Belvoir, {VA}},
	title = {Dimensionality Reduction for Supervised Learning With Reproducing Kernel Hilbert Spaces:},
	url = {http://www.dtic.mil/docs/citations/ADA446572},
	shorttitle = {Dimensionality Reduction for Supervised Learning With Reproducing Kernel Hilbert Spaces},
	abstract = {We propose a novel method of dimensionality reduction for supervised learning problems. Given a regression or classiﬁcation problem in which we wish to predict a response variable Y from an explanatory variable X, we treat the problem of dimensionality reduction as that of ﬁnding a low-dimensional “eﬀective subspace” for X which retains the statistical relationship between X and Y . We show that this problem can be formulated in terms of conditional independence. To turn this formulation into an optimization problem we establish a general nonparametric characterization of conditional independence using covariance operators on reproducing kernel Hilbert spaces. This characterization allows us to derive a contrast function for estimation of the eﬀective subspace. Unlike many conventional methods for dimensionality reduction in supervised learning, the proposed method requires neither assumptions on the marginal distribution of X, nor a parametric model of the conditional distribution of Y . We present experiments that compare the performance of the method with conventional methods.},
	institution = {Defense Technical Information Center},
	author = {Fukumizu, Kenji and Bach, Francis R. and Jordan, Michael I.},
	urldate = {2021-01-01},
	date = {2003-05-25},
	langid = {english},
	doi = {10.21236/ADA446572},
	keywords = {tbr},
	file = {Fukumizu et al_2003_Dimensionality Reduction for Supervised Learning With Reproducing Kernel Hilbert Spaces.pdf:/Users/wpq/Dropbox (MIT)/zotero/Fukumizu et al_2003_Dimensionality Reduction for Supervised Learning With Reproducing Kernel Hilbert Spaces.pdf:application/pdf}
}

@article{bakerJointMeasuresCrossCovariance1973,
	title = {Joint Measures and Cross-Covariance Operators},
	abstract = {Let H. (resp., H ) be a real and separable Hubert space with Borel O'field T (resp., rj, and let (H. X //-, T, X T.) be the product measurable space generated by the measurable rectangles. This paper develops relations between probability measures on (H. x H , V x T.), i.e., joint measures, and the projections of such measures on (H., T.) and (H , Y ). In particular, the class of all joint Gaussian measures having two specified Gaussian measures as projections is characterized, and conditions are obtained for two joint Gaussian measures to be mutually absolutely continuous. The cross-covariance operator of a joint measure plays a major role in these results and these operators are characterized.},
	pages = {17},
	author = {Baker, Charlesr},
	date = {1973},
	langid = {english},
	keywords = {tbr},
	file = {Baker_1973_Joint Measures and Cross-Covariance Operators.pdf:/Users/wpq/Dropbox (MIT)/zotero/Baker_1973_Joint Measures and Cross-Covariance Operators.pdf:application/pdf}
}

@article{liangCS229TSTAT231Statistical2016,
	title = {{CS}229T/{STAT}231: Statistical Learning Theory (Winter 2016)},
	pages = {217},
	author = {Liang, Percy},
	date = {2016},
	langid = {english},
	keywords = {tbr},
	file = {Liang_2016_CS229T-STAT231 - Statistical Learning Theory (Winter 2016).pdf:/Users/wpq/Dropbox (MIT)/zotero/Liang_2016_CS229T-STAT231 - Statistical Learning Theory (Winter 2016).pdf:application/pdf}
}

@article{liImplicitKernelLearning2019,
	title = {Implicit Kernel Learning},
	url = {http://arxiv.org/abs/1902.10214},
	abstract = {Kernels are powerful and versatile tools in machine learning and statistics. Although the notion of universal kernels and characteristic kernels has been studied, kernel selection still greatly influences the empirical performance. While learning the kernel in a data driven way has been investigated, in this paper we explore learning the spectral distribution of kernel via implicit generative models parametrized by deep neural networks. We called our method Implicit Kernel Learning ({IKL}). The proposed framework is simple to train and inference is performed via sampling random Fourier features. We investigate two applications of the proposed {IKL} as examples, including generative adversarial networks with {MMD} ({MMD} {GAN}) and standard supervised learning. Empirically, {MMD} {GAN} with {IKL} outperforms vanilla predefined kernels on both image and text generation benchmarks; using {IKL} with Random Kitchen Sinks also leads to substantial improvement over existing state-of-the-art kernel learning algorithms on popular supervised learning benchmarks. Theory and conditions for using {IKL} in both applications are also studied as well as connections to previous state-of-the-art methods.},
	journaltitle = {{arXiv}:1902.10214 [cs, stat]},
	author = {Li, Chun-Liang and Chang, Wei-Cheng and Mroueh, Youssef and Yang, Yiming and Póczos, Barnabás},
	urldate = {2021-01-04},
	date = {2019-02-26},
	eprinttype = {arxiv},
	eprint = {1902.10214},
	keywords = {read},
	file = {Li et al_2019_Implicit Kernel Learning.pdf:/Users/wpq/Dropbox (MIT)/zotero/Li et al_2019_Implicit Kernel Learning.pdf:application/pdf}
}

@article{wilsonGaussianProcessKernels2013,
	title = {Gaussian Process Kernels for Pattern Discovery and Extrapolation},
	abstract = {Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation. We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation. These kernels are derived by modelling a spectral density – the Fourier transform of a kernel – with a Gaussian mixture. The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic. We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric {CO}2 trends and airline passenger data. We also show that it is possible to reconstruct several popular standard covariances within our framework.},
	pages = {9},
	author = {Wilson, Andrew Gordon and Adams, Ryan Prescott},
	date = {2013},
	langid = {english},
	keywords = {tbr},
	file = {Wilson and Adams - Gaussian Process Kernels for Pattern Discovery and.pdf:/Users/wpq/Zotero/storage/KF7ABATZ/Wilson and Adams - Gaussian Process Kernels for Pattern Discovery and.pdf:application/pdf}
}

@inproceedings{grettonOptimalKernelChoice2012,
	title = {Optimal kernel choice for large-scale two-sample tests},
	volume = {25},
	abstract = {Given samples from distributions p and q, a two-sample test determines whether to reject the null hypothesis that p = q, based on the value of a test statistic measuring the distance between the samples. One choice of test statistic is the maximum mean discrepancy ({MMD}), which is a distance between embeddings of the probability distributions in a reproducing kernel Hilbert space. The kernel used in obtaining these embeddings is critical in ensuring the test has high power, and correctly distinguishes unlike distributions with high probability. A means of parameter selection for the two-sample test based on the {MMD} is proposed. For a given test level (an upper bound on the probability of making a Type I error), the kernel is chosen so as to maximize the test power, and minimize the probability of making a Type {II} error. The test statistic, test threshold, and optimization over the kernel parameters are obtained with cost linear in the sample size. These properties make the kernel selection and test procedures suited to data streams, where the observations cannot all be stored in memory. In experiments, the new kernel selection approach yields a more powerful test than earlier kernel selection heuristics.},
	eventtitle = {Advances in Neural Information Processing Systems},
	author = {Gretton, Arthur and Sriperumbudur, Bharath and Sejdinovic, Dino and Strathmann, Heiko and Balakrishnan, Sivaraman and Pontil, Massimiliano and Kenji, Fukumizu},
	date = {2012-01-01},
	keywords = {tbr},
	file = {Gretton et al_2012_Optimal kernel choice for large-scale two-sample tests.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gretton et al_2012_Optimal kernel choice for large-scale two-sample tests.pdf:application/pdf}
}

@article{chwialkowskiFastTwoSampleTesting2015a,
	title = {Fast Two-Sample Testing with Analytic Representations of Probability Measures},
	url = {http://arxiv.org/abs/1506.04725},
	abstract = {We propose a class of nonparametric two-sample tests with a cost linear in the sample size. Two tests are given, both based on an ensemble of distances between analytic functions representing each of the distributions. The first test uses smoothed empirical characteristic functions to represent the distributions, the second uses distribution embeddings in a reproducing kernel Hilbert space. Analyticity implies that differences in the distributions may be detected almost surely at a finite number of randomly chosen locations/frequencies. The new tests are consistent against a larger class of alternatives than the previous linear-time tests based on the (non-smoothed) empirical characteristic functions, while being much faster than the current state-of-the-art quadratic-time kernel-based or energy distance-based tests. Experiments on artificial benchmarks and on challenging real-world testing problems demonstrate that our tests give a better power/time tradeoff than competing approaches, and in some cases, better outright power than even the most expensive quadratic-time tests. This performance advantage is retained even in high dimensions, and in cases where the difference in distributions is not observable with low order statistics.},
	journaltitle = {{arXiv}:1506.04725 [stat]},
	author = {Chwialkowski, Kacper and Ramdas, Aaditya and Sejdinovic, Dino and Gretton, Arthur},
	urldate = {2021-01-05},
	date = {2015-06-15},
	eprinttype = {arxiv},
	eprint = {1506.04725},
	keywords = {read},
	file = {Chwialkowski et al_2015_Fast Two-Sample Testing with Analytic Representations of Probability Measures.pdf:/Users/wpq/Dropbox (MIT)/zotero/Chwialkowski et al_2015_Fast Two-Sample Testing with Analytic Representations of Probability Measures2.pdf:application/pdf}
}

@article{jitkrittumInterpretableDistributionFeatures2016a,
	title = {Interpretable Distribution Features with Maximum Testing Power},
	url = {http://arxiv.org/abs/1605.06796},
	abstract = {Two semimetrics on probability distributions are proposed, given as the sum of differences of expectations of analytic functions evaluated at spatial or frequency locations (i.e, features). The features are chosen so as to maximize the distinguishability of the distributions, by optimizing a lower bound on test power for a statistical test using these features. The result is a parsimonious and interpretable indication of how and where two distributions differ locally. An empirical estimate of the test power criterion converges with increasing sample size, ensuring the quality of the returned features. In real-world benchmarks on high-dimensional text and image data, linear-time tests using the proposed semimetrics achieve comparable performance to the state-of-the-art quadratic-time maximum mean discrepancy test, while returning human-interpretable features that explain the test results.},
	journaltitle = {{arXiv}:1605.06796 [cs, stat]},
	author = {Jitkrittum, Wittawat and Szabo, Zoltan and Chwialkowski, Kacper and Gretton, Arthur},
	urldate = {2021-01-05},
	date = {2016-10-28},
	eprinttype = {arxiv},
	eprint = {1605.06796},
	keywords = {tbr},
	file = {Jitkrittum et al_2016_Interpretable Distribution Features with Maximum Testing Power.pdf:/Users/wpq/Dropbox (MIT)/zotero/Jitkrittum et al_2016_Interpretable Distribution Features with Maximum Testing Power2.pdf:application/pdf}
}

@article{bentonFunctionSpaceDistributionsKernels2019,
	title = {Function-Space Distributions over Kernels},
	url = {http://arxiv.org/abs/1910.13565},
	abstract = {Gaussian processes are flexible function approximators, with inductive biases controlled by a covariance kernel. Learning the kernel is the key to representation learning and strong predictive performance. In this paper, we develop functional kernel learning ({FKL}) to directly infer functional posteriors over kernels. In particular, we place a transformed Gaussian process over a spectral density, to induce a non-parametric distribution over kernel functions. The resulting approach enables learning of rich representations, with support for any stationary kernel, uncertainty over the values of the kernel, and an interpretable specification of a prior directly over kernels, without requiring sophisticated initialization or manual intervention. We perform inference through elliptical slice sampling, which is especially well suited to marginalizing posteriors with the strongly correlated priors typical to function space modelling. We develop our approach for non-uniform, large-scale, multi-task, and multidimensional data, and show promising performance in a wide range of settings, including interpolation, extrapolation, and kernel recovery experiments.},
	journaltitle = {{arXiv}:1910.13565 [cs, stat]},
	author = {Benton, Gregory W. and Maddox, Wesley J. and Salkey, Jayson P. and Albinati, Julio and Wilson, Andrew Gordon},
	urldate = {2021-01-05},
	date = {2019-10-29},
	eprinttype = {arxiv},
	eprint = {1910.13565},
	keywords = {good, tbr},
	file = {Benton et al_2019_Function-Space Distributions over Kernels.pdf:/Users/wpq/Dropbox (MIT)/zotero/Benton et al_2019_Function-Space Distributions over Kernels.pdf:application/pdf}
}

@article{jitkrittumAdaptiveTestIndependence2017,
	title = {An Adaptive Test of Independence with Analytic Kernel Embeddings},
	abstract = {A new computationally efﬁcient dependence measure, and an adaptive statistical test of independence, are proposed. The dependence measure is the difference between analytic embeddings of the joint distribution and the product of the marginals, evaluated at a ﬁnite set of locations (features). These features are chosen so as to maximize a lower bound on the test power, resulting in a test that is data-efﬁcient, and that runs in linear time (with respect to the sample size n). The optimized features can be interpreted as evidence to reject the null hypothesis, indicating regions in the joint domain where the joint distribution and the product of the marginals differ most. Consistency of the independence test is established, for an appropriate choice of features. In real-world benchmarks, independence tests using the optimized features perform comparably to the state-of-the-art quadratic-time {HSIC} test, and outperform competing O(n) and O(n log n) tests.},
	pages = {10},
	author = {Jitkrittum, Wittawat and Szabó, Zoltán and Gretton, Arthur},
	date = {2017},
	langid = {english},
	keywords = {tbr},
	file = {Jitkrittum et al. - An Adaptive Test of Independence with Analytic Ker.pdf:/Users/wpq/Zotero/storage/EQRL7HG6/Jitkrittum et al. - An Adaptive Test of Independence with Analytic Ker.pdf:application/pdf}
}

@article{chwialkowskiKernelTestGoodness2016,
	title = {A Kernel Test of Goodness of Fit},
	url = {http://arxiv.org/abs/1602.02964},
	abstract = {We propose a nonparametric statistical test for goodness-of-fit: given a set of samples, the test determines how likely it is that these were generated from a target density function. The measure of goodness-of-fit is a divergence constructed via Stein's method using functions from a Reproducing Kernel Hilbert Space. Our test statistic is based on an empirical estimate of this divergence, taking the form of a V-statistic in terms of the log gradients of the target density and the kernel. We derive a statistical test, both for i.i.d. and non-i.i.d. samples, where we estimate the null distribution quantiles using a wild bootstrap procedure. We apply our test to quantifying convergence of approximate Markov Chain Monte Carlo methods, statistical model criticism, and evaluating quality of fit vs model complexity in nonparametric density estimation.},
	journaltitle = {{arXiv}:1602.02964 [stat]},
	author = {Chwialkowski, Kacper and Strathmann, Heiko and Gretton, Arthur},
	urldate = {2021-01-05},
	date = {2016-09-27},
	eprinttype = {arxiv},
	eprint = {1602.02964},
	keywords = {tbr},
	file = {Chwialkowski et al_2016_A Kernel Test of Goodness of Fit.pdf:/Users/wpq/Dropbox (MIT)/zotero/Chwialkowski et al_2016_A Kernel Test of Goodness of Fit.pdf:application/pdf}
}

@article{jitkrittumLinearTimeKernelGoodnessofFit2017,
	title = {A Linear-Time Kernel Goodness-of-Fit Test},
	url = {http://arxiv.org/abs/1705.07673},
	abstract = {We propose a novel adaptive test of goodness-of-fit, with computational cost linear in the number of samples. We learn the test features that best indicate the differences between observed samples and a reference model, by minimizing the false negative rate. These features are constructed via Stein's method, meaning that it is not necessary to compute the normalising constant of the model. We analyse the asymptotic Bahadur efficiency of the new test, and prove that under a mean-shift alternative, our test always has greater relative efficiency than a previous linear-time kernel test, regardless of the choice of parameters for that test. In experiments, the performance of our method exceeds that of the earlier linear-time test, and matches or exceeds the power of a quadratic-time kernel test. In high dimensions and where model structure may be exploited, our goodness of fit test performs far better than a quadratic-time two-sample test based on the Maximum Mean Discrepancy, with samples drawn from the model.},
	journaltitle = {{arXiv}:1705.07673 [cs, stat]},
	author = {Jitkrittum, Wittawat and Xu, Wenkai and Szabo, Zoltan and Fukumizu, Kenji and Gretton, Arthur},
	urldate = {2021-01-05},
	date = {2017-10-24},
	eprinttype = {arxiv},
	eprint = {1705.07673},
	keywords = {tbr},
	file = {Jitkrittum et al_2017_A Linear-Time Kernel Goodness-of-Fit Test.pdf:/Users/wpq/Dropbox (MIT)/zotero/Jitkrittum et al_2017_A Linear-Time Kernel Goodness-of-Fit Test.pdf:application/pdf}
}

@article{grettonKernelStatisticalTest2007,
	title = {A Kernel Statistical Test of Independence},
	abstract = {Although kernel measures of independence have been widely applied in machine learning (notably in kernel {ICA}), there is as yet no method to determine whether they have detected statistically signiﬁcant dependence. We provide a novel test of the independence hypothesis for one particular kernel independence measure, the Hilbert-Schmidt independence criterion ({HSIC}). The resulting test costs O(m2), where m is the sample size. We demonstrate that this test outperforms established contingency table and functional correlation-based tests, and that this advantage is greater for multivariate data. Finally, we show the {HSIC} test also applies to text (and to structured data more generally), for which no other independence test presently exists.},
	pages = {8},
	author = {Gretton, Arthur and Fukumizu, Kenji and Teo, Choon Hui},
	date = {2007},
	langid = {english},
	keywords = {tbr},
	file = {Gretton et al. - A Kernel Statistical Test of Independence.pdf:/Users/wpq/Zotero/storage/U9PPZ592/Gretton et al. - A Kernel Statistical Test of Independence.pdf:application/pdf}
}

@article{yangNystromMethodVs2012,
	title = {Nystrom Method vs Random Fourier Features: A Theoretical and Empirical Comparison},
	abstract = {Both random Fourier features and the Nystro¨m method have been successfully applied to efﬁcient kernel learning. In this work, we investigate the fundamental difference between these two approaches, and how the difference could affect their generalization performances. Unlike approaches based on random Fourier features where the basis functions (i.e., cosine and sine functions) are sampled from a distribution independent from the training data, basis functions used by the Nystro¨m method are randomly sampled from the training examples and are therefore data dependent. By exploring this difference, we show that when there is a large gap in the eigen-spectrum of the kernel matrix, approaches based on the Nystro¨m method can yield impressively better generalization error bound than random Fourier features based approach. We empirically verify our theoretical ﬁndings on a wide range of large data sets.},
	pages = {9},
	author = {Yang, Tianbao and Li, Yu-Feng and Mahdavi, Mehrdad and Jin, Rong and Zhou, Zhi-Hua},
	date = {2012},
	langid = {english},
	keywords = {tbr},
	file = {Yang et al_2012_Nystrom Method vs Random Fourier Features - A Theoretical and Empirical Comparison.pdf:/Users/wpq/Dropbox (MIT)/zotero/Yang et al_2012_Nystrom Method vs Random Fourier Features - A Theoretical and Empirical Comparison.pdf:application/pdf}
}

@article{greenfeldRobustLearningHilbertSchmidt2020,
	title = {Robust Learning with the Hilbert-Schmidt Independence Criterion},
	url = {http://arxiv.org/abs/1910.00270},
	abstract = {We investigate the use of a non-parametric independence measure, the Hilbert-Schmidt Independence Criterion ({HSIC}), as a loss-function for learning robust regression and classification models. This loss-function encourages learning models where the distribution of the residuals between the label and the model prediction is statistically independent of the distribution of the instances themselves. This loss-function was first proposed by Mooij et al. (2009) in the context of learning causal graphs. We adapt it to the task of learning for unsupervised covariate shift: learning on a source domain without access to any instances or labels from the unknown target domain, but with the assumption that \$p(y{\textbar}x)\$ (the conditional probability of labels given instances) remains the same in the target domain. We show that the proposed loss is expected to give rise to models that generalize well on a class of target domains characterised by the complexity of their description within a reproducing kernel Hilbert space. Experiments on unsupervised covariate shift tasks demonstrate that models learned with the proposed loss-function outperform models learned with standard loss functions, achieving state-of-the-art results on a challenging cell-microscopy unsupervised covariate shift task.},
	journaltitle = {{arXiv}:1910.00270 [cs, stat]},
	author = {Greenfeld, Daniel and Shalit, Uri},
	urldate = {2021-01-06},
	date = {2020-07-11},
	eprinttype = {arxiv},
	eprint = {1910.00270},
	keywords = {tbr},
	file = {Greenfeld_Shalit_2020_Robust Learning with the Hilbert-Schmidt Independence Criterion.pdf:/Users/wpq/Dropbox (MIT)/zotero/Greenfeld_Shalit_2020_Robust Learning with the Hilbert-Schmidt Independence Criterion.pdf:application/pdf}
}

@article{scholkopfNonlinearComponentAnalysis1998,
	title = {Nonlinear Component Analysis as a Kernel Eigenvalue Problem},
	volume = {10},
	issn = {0899-7667},
	doi = {10.1162/089976698300017467},
	abstract = {A new method for performing a nonlinear form of principal component analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear map—for instance, the space of all possible five-pixel products in 16 × 16 images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.},
	pages = {1299--1319},
	number = {5},
	journaltitle = {Neural Computation},
	author = {Schölkopf, B. and Smola, A. and Müller, K.},
	date = {1998-07},
	note = {Conference Name: Neural Computation},
	keywords = {tbr},
	file = {Scholkopf et al_1998_Nonlinear Component Analysis as a Kernel Eigenvalue Problem.pdf:/Users/wpq/Dropbox (MIT)/zotero/Scholkopf et al_1998_Nonlinear Component Analysis as a Kernel Eigenvalue Problem.pdf:application/pdf}
}

@article{wangPracticalGuideRandomized2015,
	title = {A Practical Guide to Randomized Matrix Computations with {MATLAB} Implementations},
	url = {http://arxiv.org/abs/1505.07570},
	abstract = {Matrix operations such as matrix inversion, eigenvalue decomposition, singular value decomposition are ubiquitous in real-world applications. Unfortunately, many of these matrix operations so time and memory expensive that they are prohibitive when the scale of data is large. In real-world applications, since the data themselves are noisy, machine-precision matrix operations are not necessary at all, and one can sacrifice a reasonable amount of accuracy for computational efficiency. In recent years, a bunch of randomized algorithms have been devised to make matrix computations more scalable. Mahoney (2011) and Woodruff (2014) have written excellent but very technical reviews of the randomized algorithms. Differently, the focus of this manuscript is on intuition, algorithm derivation, and implementation. This manuscript should be accessible to people with knowledge in elementary matrix algebra but unfamiliar with randomized matrix computations. The algorithms introduced in this manuscript are all summarized in a user-friendly way, and they can be implemented in lines of {MATLAB} code. The readers can easily follow the implementations even if they do not understand the maths and algorithms.},
	journaltitle = {{arXiv}:1505.07570 [cs]},
	author = {Wang, Shusen},
	urldate = {2021-01-07},
	date = {2015-11-02},
	eprinttype = {arxiv},
	eprint = {1505.07570},
	keywords = {tbr},
	file = {Wang_2015_A Practical Guide to Randomized Matrix Computations with MATLAB Implementations.pdf:/Users/wpq/Dropbox (MIT)/zotero/Wang_2015_A Practical Guide to Randomized Matrix Computations with MATLAB Implementations.pdf:application/pdf}
}

@article{olivaBayesianNonparametricKernelLearning2018,
	title = {Bayesian Nonparametric Kernel-Learning},
	url = {http://arxiv.org/abs/1506.08776},
	abstract = {Kernel methods are ubiquitous tools in machine learning. However, there is often little reason for the common practice of selecting a kernel a priori. Even if a universal approximating kernel is selected, the quality of the finite sample estimator may be greatly affected by the choice of kernel. Furthermore, when directly applying kernel methods, one typically needs to compute a \$N {\textbackslash}times N\$ Gram matrix of pairwise kernel evaluations to work with a dataset of \$N\$ instances. The computation of this Gram matrix precludes the direct application of kernel methods on large datasets, and makes kernel learning especially difficult. In this paper we introduce Bayesian nonparmetric kernel-learning ({BaNK}), a generic, data-driven framework for scalable learning of kernels. {BaNK} places a nonparametric prior on the spectral distribution of random frequencies allowing it to both learn kernels and scale to large datasets. We show that this framework can be used for large scale regression and classification tasks. Furthermore, we show that {BaNK} outperforms several other scalable approaches for kernel learning on a variety of real world datasets.},
	journaltitle = {{arXiv}:1506.08776 [stat]},
	author = {Oliva, Junier and Dubey, Avinava and Wilson, Andrew G. and Poczos, Barnabas and Schneider, Jeff and Xing, Eric P.},
	urldate = {2021-01-08},
	date = {2018-01-29},
	eprinttype = {arxiv},
	eprint = {1506.08776},
	keywords = {read},
	file = {Oliva et al_2018_Bayesian Nonparametric Kernel-Learning.pdf:/Users/wpq/Dropbox (MIT)/zotero/Oliva et al_2018_Bayesian Nonparametric Kernel-Learning.pdf:application/pdf}
}

@article{sinhaLearningKernelsRandom2016,
	title = {Learning Kernels with Random Features},
	abstract = {Randomized features provide a computationally efﬁcient way to approximate kernel machines in machine learning tasks. However, such methods require a user-deﬁned kernel as input. We extend the randomized-feature approach to the task of learning a kernel (via its associated random features). Speciﬁcally, we present an efﬁcient optimization problem that learns a kernel in a supervised manner. We prove the consistency of the estimated kernel as well as generalization bounds for the class of estimators induced by the optimized kernel, and we experimentally evaluate our technique on several datasets. Our approach is efﬁcient and highly scalable, and we attain competitive results with a fraction of the training cost of other techniques.},
	pages = {20},
	author = {Sinha, Aman and Duchi, John},
	date = {2016},
	langid = {english},
	keywords = {tbr},
	file = {Sinha_Duchi_2016_Learning Kernels with Random Features.pdf:/Users/wpq/Dropbox (MIT)/zotero/Sinha_Duchi_2016_Learning Kernels with Random Features.pdf:application/pdf}
}

@article{cristianiniKernelTargetAlignment2002,
	title = {On Kernel-Target Alignment},
	abstract = {We introduce the notion of kernel-alignment, a measure of similarity between two kernel functions or between a kernel and a target function. This quantity captures the degree of agreement between a kernel and a given learning task, and has very natural interpretations in machine learning, leading also to simple algorithms for model selection and learning. We analyse its theoretical properties, proving that it is sharply concentrated around its expected value, and we discuss its relation with other standard measures of performance. Finally we describe some of the algorithms that can be obtained within this framework, giving experimental results showing that adapting the kernel to improve alignment on the labelled data significantly increases the alignment on the test set, giving improved classification accuracy. Hence, the approach provides a principled method of performing transduction.},
	pages = {7},
	author = {Cristianini, Nello and Shawe-Taylor, John and Elisseeff, André and Kandola, Jaz S},
	date = {2002},
	langid = {english},
	keywords = {read},
	file = {Cristianini et al_2002_On Kernel-Target Alignment.pdf:/Users/wpq/Dropbox (MIT)/zotero/Cristianini et al_2002_On Kernel-Target Alignment.pdf:application/pdf}
}

@article{wangOverviewKernelAlignment2015,
	title = {An overview of kernel alignment and its applications},
	volume = {43},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-012-9369-4},
	doi = {10.1007/s10462-012-9369-4},
	abstract = {The success of kernel methods is very much dependent on the choice of kernel. Kernel design and learning a kernel from the data require evaluation measures to assess the quality of the kernel. In recent years, the notion of kernel alignment, which measures the degree of agreement between a kernel and a learning task, is widely used for kernel selection due to its effectiveness and low computational complexity. In this paper, we present an overview of the research progress of kernel alignment and its applications. We introduce the basic idea of kernel alignment and its theoretical properties, as well as the extensions and improvements for specific learning problems. The typical applications, including kernel parameter tuning, multiple kernel learning, spectral kernel learning and feature selection and extraction, are reviewed in the context of classification framework. The relationship between kernel alignment and other evaluation measures is also explored. Finally, concluding remarks and future directions are presented.},
	pages = {179--192},
	number = {2},
	journaltitle = {Artificial Intelligence Review},
	shortjournal = {Artif Intell Rev},
	author = {Wang, Tinghua and Zhao, Dongyan and Tian, Shengfeng},
	urldate = {2021-01-09},
	date = {2015-02-01},
	langid = {english},
	keywords = {tbr},
	file = {Wang et al_2015_An overview of kernel alignment and its applications.pdf:/Users/wpq/Dropbox (MIT)/zotero/Wang et al_2015_An overview of kernel alignment and its applications.pdf:application/pdf}
}

@article{cortesAlgorithmsLearningKernels2012,
	title = {Algorithms for Learning Kernels Based on Centered Alignment},
	abstract = {This paper presents new and effective algorithms for learning kernels. In particular, as shown by our empirical results, these algorithms consistently outperform the so-called uniform combination solution that has proven to be difﬁcult to improve upon in the past, as well as other algorithms for learning kernels based on convex combinations of base kernels in both classiﬁcation and regression. Our algorithms are based on the notion of centered alignment which is used as a similarity measure between kernels or kernel matrices. We present a number of novel algorithmic, theoretical, and empirical results for learning kernels based on our notion of centered alignment. In particular, we describe efﬁcient algorithms for learning a maximum alignment kernel by showing that the problem can be reduced to a simple {QP} and discuss a one-stage algorithm for learning both a kernel and a hypothesis based on that kernel using an alignment-based regularization. Our theoretical results include a novel concentration bound for centered alignment between kernel matrices, the proof of the existence of effective predictors for kernels with high alignment, both for classiﬁcation and for regression, and the proof of stability-based generalization bounds for a broad family of algorithms for learning kernels based on centered alignment. We also report the results of experiments with our centered alignment-based algorithms in both classiﬁcation and regression.},
	pages = {34},
	author = {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
	date = {2012},
	langid = {english},
	keywords = {tbr},
	file = {Cortes et al. - Algorithms for Learning Kernels Based on Centered .pdf:/Users/wpq/Zotero/storage/3TC8HU6W/Cortes et al. - Algorithms for Learning Kernels Based on Centered .pdf:application/pdf}
}

@article{wangTwoStageFuzzyMultiple2018,
	title = {Two-Stage Fuzzy Multiple Kernel Learning Based on Hilbert–Schmidt Independence Criterion},
	volume = {26},
	issn = {1941-0034},
	doi = {10.1109/TFUZZ.2018.2848224},
	abstract = {Multiple kernel learning ({MKL}) is a principled approach to kernel combination and selection for a variety of learning tasks, such as classification, clustering, and dimensionality reduction. In this paper, we develop a novel fuzzy multiple kernel learning model based on the Hilbert-Schmidt independence criterion ({HSIC}) for classification, which we call {HSIC}-{FMKL}. In this model, we first propose an {HSIC} Lasso-based {MKL} formulation, which not only has a clear statistical interpretation that minimum redundant kernels with maximum dependence on output labels are found and combined, but also enables the global optimal solution to be computed efficiently by solving a Lasso optimization problem. Since the traditional support vector machine ({SVM}) is sensitive to outliers or noises in the dataset, fuzzy {SVM} ({FSVM}) is used to select the prediction hypothesis once the optimal kernel has been obtained. The main advantage of {FSVM} is that we can associate a fuzzy membership with each data point such that these data points can have different effects on the training of the learning machine. We propose a new fuzzy membership function using a heuristic strategy based on the {HSIC}. The proposed {HSIC}-{FMKL} is a two-stage kernel learning approach and the {HSIC} is applied in both stages. We perform extensive experiments on real-world datasets from the {UCI} benchmark repository and the application domain of computational biology which validate the superiority of the proposed model in terms of prediction accuracy.},
	pages = {3703--3714},
	number = {6},
	journaltitle = {{IEEE} Transactions on Fuzzy Systems},
	author = {Wang, T. and Lu, J. and Zhang, G.},
	date = {2018-12},
	note = {Conference Name: {IEEE} Transactions on Fuzzy Systems},
	keywords = {read},
	file = {Wang et al_2018_Two-Stage Fuzzy Multiple Kernel Learning Based on Hilbert–Schmidt Independence Criterion.pdf:/Users/wpq/Dropbox (MIT)/zotero/Wang et al_2018_Two-Stage Fuzzy Multiple Kernel Learning Based on Hilbert–Schmidt Independence Criterion.pdf:application/pdf}
}

@article{gonenMultipleKernelLearning2011,
	title = {Multiple Kernel Learning Algorithms},
	abstract = {In recent years, several methods have been proposed to combine multiple kernels instead of using a single one. These different kernels may correspond to using different notions of similarity or may be using information coming from multiple sources (different representations or different feature subsets). In trying to organize and highlight the similarities and differences between them, we give a taxonomy of and review several multiple kernel learning algorithms. We perform experiments on real data sets for better illustration and comparison of existing algorithms. We see that though there may not be large differences in terms of accuracy, there is difference between them in complexity as given by the number of stored support vectors, the sparsity of the solution as given by the number of used kernels, and training time complexity. We see that overall, using multiple kernels instead of a single one is useful and believe that combining kernels in a nonlinear or data-dependent way seems more promising than linear combination in fusing information provided by simple linear kernels, whereas linear methods are more reasonable when combining complex Gaussian kernels.},
	pages = {58},
	author = {Gonen, Mehmet and Alpaydın, Ethem and Tr, Boun Edu and Tr, Boun Edu},
	date = {2011},
	langid = {english},
	keywords = {tbr},
	file = {Gonen et al. - Multiple Kernel Learning Algorithms.pdf:/Users/wpq/Zotero/storage/YHNYI45W/Gonen et al. - Multiple Kernel Learning Algorithms.pdf:application/pdf}
}

@inproceedings{jebaraMultitaskFeatureKernel2004,
	location = {Banff, Alberta, Canada},
	title = {Multi-task feature and kernel selection for {SVMs}},
	url = {http://portal.acm.org/citation.cfm?doid=1015330.1015426},
	doi = {10.1145/1015330.1015426},
	abstract = {We compute a common feature selection or kernel selection conﬁguration for multiple support vector machines ({SVMs}) trained on diﬀerent yet inter-related datasets. The method is advantageous when multiple classiﬁcation tasks and diﬀerently labeled datasets exist over a common input space. Diﬀerent datasets can mutually reinforce a common choice of representation or relevant features for their various classiﬁers. We derive a multi-task representation learning approach using the maximum entropy discrimination formalism. The resulting convex algorithms maintain the global solution properties of support vector machines. However, in addition to multiple {SVM} classiﬁcation/regression parameters they also jointly estimate an optimal subset of features or optimal combination of kernels. Experiments are shown on standardized datasets.},
	eventtitle = {Twenty-first international conference},
	pages = {55},
	booktitle = {Twenty-first international conference on Machine learning  - {ICML} '04},
	publisher = {{ACM} Press},
	author = {Jebara, Tony},
	urldate = {2021-01-09},
	date = {2004},
	langid = {english},
	keywords = {good, tbr},
	file = {Jebara - 2004 - Multi-task feature and kernel selection for SVMs.pdf:/Users/wpq/Zotero/storage/ZPYX6JJK/Jebara - 2004 - Multi-task feature and kernel selection for SVMs.pdf:application/pdf}
}

@article{ganLearningAttributesEquals2016,
	title = {Learning Attributes Equals Multi-Source Domain Generalization},
	url = {http://arxiv.org/abs/1605.00743},
	abstract = {Attributes possess appealing properties and benefit many computer vision problems, such as object recognition, learning with humans in the loop, and image retrieval. Whereas the existing work mainly pursues utilizing attributes for various computer vision problems, we contend that the most basic problem---how to accurately and robustly detect attributes from images---has been left under explored. Especially, the existing work rarely explicitly tackles the need that attribute detectors should generalize well across different categories, including those previously unseen. Noting that this is analogous to the objective of multi-source domain generalization, if we treat each category as a domain, we provide a novel perspective to attribute detection and propose to gear the techniques in multi-source domain generalization for the purpose of learning cross-category generalizable attribute detectors. We validate our understanding and approach with extensive experiments on four challenging datasets and three different problems.},
	journaltitle = {{arXiv}:1605.00743 [cs]},
	author = {Gan, Chuang and Yang, Tianbao and Gong, Boqing},
	urldate = {2021-01-09},
	date = {2016-05-02},
	eprinttype = {arxiv},
	eprint = {1605.00743},
	keywords = {tbr},
	file = {Gan et al_2016_Learning Attributes Equals Multi-Source Domain Generalization.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gan et al_2016_Learning Attributes Equals Multi-Source Domain Generalization.pdf:application/pdf}
}

@article{drineasNystromMethodApproximating2005,
	title = {On the Nystrom Method for Approximating a Gram Matrix for Improved Kernel-Based Learning},
	pages = {23},
	author = {Drineas, Petros and Mahoney, Michael W},
	date = {2005},
	langid = {english},
	keywords = {tbr},
	file = {Drineas_Mahoney_2005_On the Nystrom Method for Approximating a Gram Matrix for Improved Kernel-Based Learning.pdf:/Users/wpq/Dropbox (MIT)/zotero/Drineas_Mahoney_2005_On the Nystrom Method for Approximating a Gram Matrix for Improved Kernel-Based Learning.pdf:application/pdf}
}

@article{williamsUsingNystromMethod2000,
	title = {Using the Nyström Method to Speed Up Kernel Machines},
	volume = {13},
	url = {https://proceedings.neurips.cc/paper/2000/hash/19de10adbaa1b2ee13f77f679fa1483a-Abstract.html},
	pages = {682--688},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Williams, Christopher and Seeger, Matthias},
	urldate = {2021-01-09},
	date = {2000},
	langid = {english},
	keywords = {tbr},
	file = {Williams_Seeger_2000_Using the Nystrom Method to Speed Up Kernel Machines.pdf:/Users/wpq/Dropbox (MIT)/zotero/Williams_Seeger_2000_Using the Nystrom Method to Speed Up Kernel Machines.pdf:application/pdf}
}

@article{rawatSampledSoftmaxRandom,
	title = {Sampled Softmax with Random Fourier Features},
	abstract = {The computational cost of training with softmax cross entropy loss grows linearly with the number of classes. For the settings where a large number of classes are involved, a common method to speed up training is to sample a subset of classes and utilize an estimate of the loss gradient based on these classes, known as the sampled softmax method. However, the sampled softmax provides a biased estimate of the gradient unless the samples are drawn from the exact softmax distribution, which is again expensive to compute. Therefore, a widely employed practical approach involves sampling from a simpler distribution in the hope of approximating the exact softmax distribution. In this paper, we develop the ﬁrst theoretical understanding of the role that different sampling distributions play in determining the quality of sampled softmax. Motivated by our analysis and the work on kernel-based sampling, we propose the Random Fourier Softmax ({RFsoftmax}) method that utilizes the powerful Random Fourier Features to enable more efﬁcient and accurate sampling from an approximate softmax distribution. We show that {RF}-softmax leads to low bias in estimation in terms of both the full softmax distribution and the full softmax gradient. Furthermore, the cost of {RF}-softmax scales only logarithmically with the number of classes.},
	pages = {11},
	author = {Rawat, Ankit Singh and Chen, Jiecao and Yu, Felix Xinnan X and Suresh, Ananda Theertha and Kumar, Sanjiv},
	langid = {english},
	keywords = {tbr},
	file = {Rawat et al. - Sampled Softmax with Random Fourier Features.pdf:/Users/wpq/Zotero/storage/82ADR7GU/Rawat et al. - Sampled Softmax with Random Fourier Features.pdf:application/pdf}
}

@inproceedings{mooijRegressionDependenceMinimization2009,
	location = {Montreal, Quebec, Canada},
	title = {Regression by dependence minimization and its application to causal inference in additive noise models},
	isbn = {978-1-60558-516-1},
	url = {http://portal.acm.org/citation.cfm?doid=1553374.1553470},
	doi = {10.1145/1553374.1553470},
	abstract = {Motivated by causal inference problems, we propose a novel method for regression that minimizes the statistical dependence between regressors and residuals. The key advantage of this approach to regression is that it does not assume a particular distribution of the noise, i.e., it is non-parametric with respect to the noise distribution. We argue that the proposed regression method is well suited to the task of causal inference in additive noise models. A practical disadvantage is that the resulting optimization problem is generally non-convex and can be diﬃcult to solve. Nevertheless, we report good results on one of the tasks of the {NIPS} 2008 Causality Challenge, where the goal is to distinguish causes from eﬀects in pairs of statistically dependent variables. In addition, we propose an algorithm for eﬃciently inferring causal models from observational data for more than two variables. The required number of regressions and independence tests is quadratic in the number of variables, which is a signiﬁcant improvement over the simple method that tests all possible {DAGs}.},
	eventtitle = {the 26th Annual International Conference},
	pages = {1--8},
	booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning - {ICML} '09},
	publisher = {{ACM} Press},
	author = {Mooij, Joris and Janzing, Dominik and Peters, Jonas and Schölkopf, Bernhard},
	urldate = {2021-01-13},
	date = {2009},
	langid = {english},
	keywords = {tbr},
	file = {Mooij et al. - 2009 - Regression by dependence minimization and its appl.pdf:/Users/wpq/Zotero/storage/HSYQQVCV/Mooij et al. - 2009 - Regression by dependence minimization and its appl.pdf:application/pdf}
}