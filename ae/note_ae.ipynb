{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introductory-participation",
   "metadata": {},
   "source": [
    "goal \n",
    "\n",
    "- code for running denoising autoencoder \n",
    "- replicate 1d, 2d example that r(x)-x approximates the score of data distribution.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. What regularized auto-encoders learn from the data-generating distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "from jax import random\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 25\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "cmap = plt.cm.get_cmap('bwr')\n",
    "\n",
    "from jax_utils import get_data_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-purple",
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameters\n",
    "\n",
    "# number of data points\n",
    "n = 1000\n",
    "\n",
    "# dimension of inputs\n",
    "d = 2\n",
    "\n",
    "# dimension of encoder/decoder mapping\n",
    "encoder_dims = [200,100]\n",
    "decoder_dims = encoder_dims[::-1][1:] + [d]\n",
    "print(encoder_dims, decoder_dims)\n",
    "\n",
    "# \\in {'l2_norm', 'bce'}\n",
    "criterion_name = 'l2_norm'\n",
    "# \\in {'tanh', 'relu'}\n",
    "nonlinearity_name = 'relu'\n",
    "\n",
    "# optimization\n",
    "lr = .002\n",
    "batch_size = 50\n",
    "n_epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/google/flax/blob/main/examples/vae/train.py\n",
    "#\n",
    "# x --Encoder--> z --Decoder --> x_recon\n",
    "#\n",
    "\n",
    "def nonlinearity():\n",
    "    if nonlinearity_name == 'tanh':\n",
    "        return nn.tanh\n",
    "    elif nonlinearity_name == 'relu':\n",
    "        return nn.relu\n",
    "    else:\n",
    "        raise ValueError(f'{nonlinearity_name} not valid.')\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for i, dim in enumerate(encoder_dims[:-1]):\n",
    "            x = nn.Dense(dim, name=f'fc{i+1}')(x)\n",
    "            x = nonlinearity()(x)\n",
    "        x = nn.Dense(encoder_dims[-1], name=f'fc{len(encoder_dims)}')(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for i, dim in enumerate(decoder_dims[:-1]):\n",
    "            x = nn.Dense(dim, name=f'fc{i+1}')(x)\n",
    "            x = nonlinearity()(x)\n",
    "        x = nn.Dense(decoder_dims[-1], name=f'fc{len(decoder_dims)}')(x)\n",
    "        return x\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    \n",
    "    def setup(self):\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "\n",
    "    def score_apx(self, x):\n",
    "        x_recon = self.__call__(x)\n",
    "        return x_recon - x\n",
    "    \n",
    "\n",
    "@jax.vmap\n",
    "def bce_with_logits(logits, labels):\n",
    "    logits = nn.log_sigmoid(logits)\n",
    "    loss = -np.sum(labels * logits + (1. - labels) * np.log(-np.expm1(logits)))\n",
    "    return loss\n",
    "\n",
    "def l2_loss(predictions, targets):\n",
    "    return np.linalg.norm(predictions-targets) ** 2\n",
    "\n",
    "def get_criterion():\n",
    "    if criterion_name == 'l2_norm':\n",
    "        return l2_loss\n",
    "    elif criterion_name == 'bce':\n",
    "        return (lambda x, y: bce_with_logits(x, y).mean())\n",
    "    else:\n",
    "        raise ValueError(f'{criterion_name} not valid.')\n",
    "\n",
    "def model():\n",
    "    return Autoencoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-library",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def func_1d(key):\n",
    "    x = random.uniform(key, shape=(n,1), minval=-1, maxval=1)\n",
    "    y = np.sin(x*3*3.14) + 0.3*np.cos(x*9*3.14) + 0.5*np.sin(x*7*3.14)\n",
    "    return x, y\n",
    "\n",
    "def func_2d(key, minval=3, maxval=12):\n",
    "    t = random.uniform(key, shape=(n,1), minval=minval, maxval=maxval)\n",
    "    x = 0.04*np.sin(t)*t\n",
    "    y = 0.04*np.cos(t)*t\n",
    "    xy = np.hstack((x,y))\n",
    "    return xy\n",
    "\n",
    "σ = .06\n",
    "mask_ratio = .5\n",
    "\n",
    "def noise_additive_gaussian(key, X):\n",
    "    return X + σ*random.normal(key, X.shape)\n",
    "\n",
    "def noise_mask(key, X):\n",
    "    mask = ( random.uniform(key, X.shape) < mask_ratio ).astype(np.float32)\n",
    "    return mask*X\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "X = func_2d(key)\n",
    "X̃ = noise_additive_gaussian(key, X)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12,6))\n",
    "ax = axs[0]\n",
    "ax.scatter(X[:,0], X[:,1])\n",
    "ax.grid()\n",
    "ax.set_xlim((-1,1))\n",
    "ax.set_ylim((-1,1))\n",
    "ax.set_title(r'$X$')\n",
    "\n",
    "ax = axs[1]\n",
    "ax.scatter(X̃[:,0], X̃[:,1], alpha=.1)\n",
    "ax.grid()\n",
    "ax.set_xlim((-1,1))\n",
    "ax.set_ylim((-1,1))\n",
    "ax.set_title(r'$\\tilde{X}$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-calibration",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch, rng):\n",
    "    def loss_fn(params):\n",
    "        X̃ = noise_additive_gaussian(rng, batch)\n",
    "        x_recon = model().apply(params, X̃)\n",
    "        loss = get_criterion()(x_recon, batch)\n",
    "        return loss, x_recon\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, x_recon), grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss, x_recon\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def ae_recon_jit(state, X):\n",
    "    return model().apply(state.params, X)\n",
    "\n",
    "@jax.jit\n",
    "def ae_score_apx_jit(state, X):\n",
    "    return model().apply(state.params, X,\n",
    "                         method=Autoencoder.score_apx)\n",
    "\n",
    "\n",
    "def plt_score_apx_one(ax, state, xlim, ylim, n_point_per_side = 30):\n",
    "    xgrid, ygrid = np.meshgrid(np.linspace(xlim[0],xlim[1],n_point_per_side),\n",
    "                               np.linspace(ylim[0],ylim[1],n_point_per_side))\n",
    "    Xgrid = np.vstack((xgrid.flatten(),\n",
    "                       ygrid.flatten())).T\n",
    "    score_apx = ae_score_apx_jit(state, Xgrid)\n",
    "\n",
    "    Xt = func_2d(key, minval=3, maxval=12)\n",
    "    Xt_recon = ae_recon_jit(state, Xt)\n",
    "\n",
    "    ax.scatter(Xt[:,0], Xt[:,1], color='b', label=r'$X$')\n",
    "    ax.scatter(Xt_recon[:,0], Xt_recon[:,1], color='r', label=r'$\\tilde{X}$')\n",
    "    ax.quiver(Xgrid[:,0], Xgrid[:,1], score_apx[:,0], score_apx[:,1])\n",
    "    ax.grid()\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_title(f'{np.linalg.norm(score_apx, axis=-1).mean():.4f}')\n",
    "    \n",
    "\n",
    "def plt_score_apx(state):\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15,7))\n",
    "    xlim = np.array([-1,1])\n",
    "    ylim = np.array([-1,1])\n",
    "    plt_score_apx_one(axs[0], state, xlim, ylim)\n",
    "    plt_score_apx_one(axs[1], state, xlim*.3, ylim*.3)\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-prospect",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_init = np.ones((1,d), np.float32)\n",
    "params = model().init(key, x_init)\n",
    "\n",
    "state = train_state.TrainState.create(\n",
    "    apply_fn=model().apply,\n",
    "    params=params,\n",
    "    tx=optax.adam(lr))\n",
    "\n",
    "n_batches, batches = get_data_stream(\n",
    "    key, batch_size, X)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for j in range(n_batches):\n",
    "        batch = next(batches)\n",
    "        key, rng = random.split(key)\n",
    "        state, loss, x_recon = train_step(state, batch, rng)\n",
    "        \n",
    "    if epoch%(n_epochs//10)==0:\n",
    "        print(f'[{epoch}] loss={loss}')\n",
    "        fig = plt_score_apx(state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:misc_impl] *",
   "language": "python",
   "name": "conda-env-misc_impl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
