{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reference:\n",
    "#    https://peterroelants.github.io/posts/gaussian-process-kernels/\n",
    "#    https://distill.pub/2019/visual-exploration-gaussian-processes/\n",
    "#    http://gregorygundersen.com/blog/2019/06/27/gp-regression/\n",
    "#\n",
    "import numpy as onp\n",
    "import numpy.random as npr\n",
    "np.set_printoptions(precision=3,suppress=True)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import jax\n",
    "from jax import grad, jit, vmap, device_put\n",
    "import jax.numpy as np\n",
    "import jax.numpy.linalg as linalg\n",
    "from jax.experimental import optimizers\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "# https://matplotlib.org/3.1.1/gallery/style_sheets/style_sheets_reference.html\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 25\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "cmap = plt.cm.get_cmap('bwr')\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "import sys\n",
    "sys.path.append('../kernel')\n",
    "from jaxkern import (cov_se, cov_rq, cov_pe, LookupKernel, normalize_K, mtgp_k)\n",
    "\n",
    "from plt_utils import plt_savefig, plt_scaled_colobar_ax\n",
    "from gpax import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "\n",
    "M = 2\n",
    "n_train = 100\n",
    "n_test = 50\n",
    "ylim = (-3,3)\n",
    "xlim = (-.2,1.2)\n",
    "σns = [.03, .1]\n",
    "ℓ = .2\n",
    "mode = 'stgp'\n",
    "mode = 'mtgp'\n",
    "mode = 'amtgp'\n",
    "lr = .0005\n",
    "num_steps = 200\n",
    "verbose = True\n",
    "scale = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "X0 = np.sort(npr.rand(int(n_train*task0_n_ratio), 1), axis=0)\n",
    "X1 = np.sort(npr.rand(n_train-len(X0), 1)*.5, axis=0)\n",
    "X_train = np.vstack((np.hstack((X0, np.zeros_like(X0))),\n",
    "                     np.hstack((X1, np.ones_like(X1)))))\n",
    "\n",
    "f0 = lambda X: np.round(np.sin(12*X)*2)/2\n",
    "f1 = lambda X: np.sin(12*X + shift)\n",
    "fs = [f0,f1]\n",
    "Y0 = f0(X0) + npr.randn(*X0.shape)*σns[0]\n",
    "Y1 = f1(X1) + npr.randn(*X1.shape)*σns[1]\n",
    "y_train = np.vstack((Y0,Y1))\n",
    "\n",
    "plt.plot(X0, f0(X0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "def log_func(i, f, params):\n",
    "    if verbose:\n",
    "        print(tabulate([[f(params)]+[x.item() if x.size==1 else x for x in list(params.values())]],\n",
    "                       tablefmt=\"plain\",\n",
    "                       floatfmt=\".3f\",\n",
    "                       headers=['loss']+list(params.keys()) if i == 0 else ()))\n",
    "\n",
    "\n",
    "## Plotting\n",
    "\n",
    "colors_b = [cmap(.1), cmap(.3)]\n",
    "colors_r = [cmap(.9), cmap(.7)]\n",
    "mses = {}\n",
    "modes = ['stgp', 'mtgp', 'amtgp']\n",
    "shifts = [0., .1, .2, .3, .4, .5, 1.]\n",
    "task0_n_ratio = 2/3\n",
    "\n",
    "# modes = ['stgp', 'mtgp', 'amtgp']\n",
    "# shifts = [0.]\n",
    "# task0_n_ratio = 3/4\n",
    "\n",
    "for si, shift in enumerate(shifts): # \n",
    "    \n",
    "    nrows = len(modes)\n",
    "    gridspec_kw = {'width_ratios': [2, 1], 'height_ratios': [1 for _ in range(nrows)]}\n",
    "    fig, axs = plt.subplots(nrows, 2, gridspec_kw=gridspec_kw)\n",
    "    fig.set_size_inches(15, 5*nrows)\n",
    "\n",
    "\n",
    "    for i, mode in enumerate(modes):\n",
    "        \n",
    "        print(shift,mode)\n",
    "\n",
    "        ## Data\n",
    "        np.random.seed(0)\n",
    "\n",
    "        B = np..eye(M)\n",
    "\n",
    "        X0 = np.sort(np.random.rand(int(n_train*task0_n_ratio), 1), axis=0)\n",
    "        X1 = np.sort(np.random.rand(n_train-len(X0), 1)*.5, axis=0)\n",
    "        X_train = np.vstack((np.hstack((X0, np.zeros_like(X0))),\n",
    "                             np.hstack((X1, np.ones_like(X1)))))\n",
    "\n",
    "        f0 = lambda X: np.sin(scale*X)\n",
    "        f1 = lambda X: np.sin(scale*X + shift)\n",
    "        fs = [f0,f1]\n",
    "        Y0 = f0(X0) + np.random.randn(*X0.shape)*σns[0]\n",
    "        Y1 = f1(X1) + np.random.randn(*X1.shape)*σns[1]\n",
    "        y_train = np.vstack((Y0,Y1))\n",
    "\n",
    "        X_test = np.vstack((np.tile(np.linspace(xlim[0], xlim[1], n_test), M),\n",
    "                            np.hstack([t*np.ones(n_test) for t in range(M)]))).T\n",
    "\n",
    "\n",
    "        ## Training\n",
    "\n",
    "        def mtgp_k(XT, XTp, logℓ, B):\n",
    "            X, Xp = XT[:,0], XTp[:,0]\n",
    "            Kx = cov_se(X, Xp, logℓ=logℓ)\n",
    "            T, Tp = np.asarray(XT[:,1], np.int), np.asarray(XTp[:,1], np.int)\n",
    "            Kt = LookupKernel(T, Tp, B)\n",
    "            K = Kx*Kt\n",
    "            return K\n",
    "\n",
    "        def mtgp_k_soft(XY, XYp, logℓx, logℓy, logσy):\n",
    "            X, Xp = XY[:,0], XYp[:,0]\n",
    "            Kx = cov_se(X, Xp, logℓ=logℓx)\n",
    "            Y, Yp = XY[:,1], XYp[:,1]\n",
    "            Ky = cov_se(Y, Yp, logℓ=logℓy, logσ=logσy)\n",
    "            K = Kx*Ky\n",
    "            return K\n",
    "\n",
    "        if mode == 'stgp':\n",
    "            def nmll(params):\n",
    "                k = lambda X, Y: mtgp_k(X, Y, params['logℓ'], B)\n",
    "                μ, Σ, mll = gp_regression_chol(\n",
    "                    X_train, y_train, X_test, k, logsn=params['logsn'])\n",
    "                return -mll\n",
    "            params = {'logℓ': np..log(1.),\n",
    "                      'logsn': np..log(.1*np..ones(M))}\n",
    "            res = run_sgd(nmll, params, lr=lr, num_steps=num_steps, log_func=None)\n",
    "            logℓ, logsn = res['logℓ'].item(), res['logsn']\n",
    "            ℓ, σn = np..exp(logℓ), np..exp(logsn)\n",
    "        if mode == 'mtgp':\n",
    "            def nmll(params):\n",
    "                L = np..exp(params['logL'])\n",
    "                B = L@L.T\n",
    "                k = lambda X, Y: mtgp_k(X, Y, params['logℓ'], B)\n",
    "                μ, Σ, mll = gp_regression_chol(\n",
    "                    X_train, y_train, X_test, k, logsn=params['logsn'])\n",
    "                return -mll\n",
    "            params = {'logℓ': np..log(1.),\n",
    "                      'logsn': np..log(.1*np..ones(M)),\n",
    "                      'logL': np..log(np..array(np.eye(M))*shift + np.random.rand(M,M) )}\n",
    "            res = run_sgd(nmll, params, lr=lr, num_steps=num_steps, log_func=None)\n",
    "            logℓ, logsn = res['logℓ'].item(), res['logsn']\n",
    "            ℓ, σn = np..exp(logℓ), np..exp(logsn)\n",
    "            L = np..exp(params['logL'])\n",
    "            B = L@L.T\n",
    "        if mode == 'amtgp':\n",
    "            # Train independently on auxiliary task 0\n",
    "            #\n",
    "            I = X_train[:,1] == 0\n",
    "            X_train_aux = X_train[I,0]\n",
    "            y_train_aux = y_train[I]\n",
    "            def nmll(params):\n",
    "                k = lambda X, Y: cov_se(X, Y, logℓ=params['logℓ'])\n",
    "                μ, Σ, mll = gp_regression_chol(\n",
    "                    X_train_aux, y_train_aux, X_test, k, logsn=params['logsn'])\n",
    "                return -mll\n",
    "            params = {'logℓ': np..log(1.),'logsn': np..log(.1)}\n",
    "            res = run_sgd(nmll, params, lr=lr, num_steps=num_steps, log_func=None)\n",
    "            logℓ0, logsn0 = res['logℓ'].item(), res['logsn']\n",
    "            ℓ0, σn0 = np..exp(logℓ0), np..exp(logsn0)\n",
    "\n",
    "            # Predict at task=1's location\n",
    "            I = X_train[:,1]==1\n",
    "            k = lambda X, Y: cov_se(X, Y, logℓ=logℓ0)\n",
    "            μ, _, _ = gp_regression_chol(X_train_aux, y_train_aux, X_train[I,0], k, logsn0)\n",
    "            # (X1, Y0μ)\n",
    "            X_train_main = np.vstack((X_train[I,0], μ.squeeze())).T\n",
    "            y_train_main = y_train[I]\n",
    "            def nmll(params):\n",
    "                k = lambda X, Y: mtgp_k_soft(X, Y, params['logℓx'], params['logℓy'], params['logσy'])\n",
    "                μ, Σ, mll = gp_regression_chol(\n",
    "                    X_train_main, y_train_main, X_test, k, logsn=params['logsn'])\n",
    "                return -mll\n",
    "            params = {'logℓx': np..log(1.),\n",
    "                      'logℓy': np..log(1.),\n",
    "                      'logσy': np..log(1.),\n",
    "                      'logsn': np..log(.1)}\n",
    "            res = run_sgd(nmll, params, lr=lr, num_steps=num_steps, log_func=None)\n",
    "            logℓx, logℓy, logsn1, logσy = res['logℓx'].item(), res['logℓy'].item(), res['logsn'].item(), res['logσy'].item()\n",
    "            ℓx, ℓy, σn1, σy = np..exp(logℓx), np.exp(logℓy), np..exp(logsn1), np..exp(logσy)\n",
    "\n",
    "\n",
    "        ## Plotting\n",
    "\n",
    "        if mode in ['stgp', 'mtgp']:\n",
    "            ax = axs[i, 0]\n",
    "            k = lambda X, Y: mtgp_k(X, Y, logℓ, B)\n",
    "            μ, Σ, mll = gp_regression_chol(X_train, y_train, X_test, k, logsn)\n",
    "            std = np.expand_dims(np.sqrt(np.diag(Σ)), 1)\n",
    "\n",
    "            for t in range(M):\n",
    "                # task-specific mll\n",
    "                I = X_test[:,1] == t\n",
    "                # posterior predictive distribution\n",
    "                X_test_, μ_, std_ = X_test[I,0].squeeze(), μ[I].squeeze(), std[I].squeeze()\n",
    "                ax.plot(X_test_, μ_, color=colors_b[t], lw=2)\n",
    "                ax.fill_between(X_test_, μ_-2*std_, μ_+2*std_, alpha=.2, color=colors_b[t])\n",
    "                # generating function for main task\n",
    "                if t == 1:\n",
    "                    ax.plot(X_test_, fs[t](X_test_), color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "                mse = mean_squared_error(μ[I], fs[t](X_test[I,0]))\n",
    "                if t==1: mses[(shift,mode)]=mse\n",
    "                # train data points\n",
    "                I = X_train[:,1] == t\n",
    "                ax.scatter(X_train[I,0], y_train[I],\n",
    "                           marker='x', color=colors_r[t], s=50,\n",
    "                           label=f'Task {t}'+' ($\\sigma_n$'+f'={σn[t]:.2f}, '+'$mse$'+f'={mse:.3f})')\n",
    "\n",
    "            ax.grid()\n",
    "            ax.set_xlim(xlim)\n",
    "            ax.set_ylim(ylim)\n",
    "            ax.set_ylabel('$'+f'{mode}'+'}$')\n",
    "            ax.legend(loc='upper left', fontsize=15)\n",
    "            title = '$\\ell$'+f'={ℓ:.2f}'+ \\\n",
    "                ' $B_{01}/B_{00}$'+f'={B[0,1]*2/(B[0,0]+B[1,1]):.2f}'+ \\\n",
    "                ' $-mll$'+f'={-mll:.2f}'\n",
    "            ax.set_title(title, fontsize=30)\n",
    "            \n",
    "\n",
    "\n",
    "            ax = axs[i, 1]\n",
    "            XX = np.vstack((X_train, X_test[X_test[:,1]==1]))\n",
    "            K = k(XX, XX)\n",
    "            im = ax.imshow(normalize_K(K), cmap=cmap)\n",
    "            fig.colorbar(im, cax=plt_scaled_colobar_ax(ax))\n",
    "            ax.set_title('$K(X_{train}, X_{test@1})$')\n",
    "\n",
    "\n",
    "        if mode == 'amtgp':\n",
    "            ax = axs[i, 0]\n",
    "            # Predict at task=1's test location\n",
    "            I = X_test[:,1]==1\n",
    "            k = lambda X, Y: cov_se(X, Y, logℓ=logℓ0)\n",
    "            Y0μ, Σ, _ = gp_regression_chol(X_train_aux, y_train_aux, X_test[I,0], k, logsn0)\n",
    "            # Use posterior mean of task=0 regressor as additional input to task=1 regressor\n",
    "            X_test_main = np.vstack((X_test[I,0], Y0μ.squeeze())).T\n",
    "            k = lambda X, Y: mtgp_k_soft(X, Y, logℓx, logℓy, logσy)\n",
    "            Y1μ, Σ, mll = gp_regression_chol(X_train_main, y_train_main, X_test_main, k, logsn1)\n",
    "            Y1std = np.expand_dims(np.sqrt(np.diag(Σ)), 1).squeeze()\n",
    "            Y1μ = Y1μ.squeeze()\n",
    "\n",
    "            t = 1\n",
    "            ax.plot(X_test_main[:,0], Y1μ, color=colors_b[t], lw=2)\n",
    "            ax.fill_between(X_test_main[:,0], Y1μ-2*Y1std, Y1μ+2*Y1std, alpha=.2, color=colors_b[t])\n",
    "            ax.plot(X_test_main[:,0], fs[t](X_test_main[:,0]), color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "            mse = mean_squared_error(Y1μ, fs[1](X_test[I,0]))\n",
    "            mses[(shift, mode)] = mse\n",
    "            I = X_train[:,1] == t\n",
    "            ax.scatter(X_train[I,0], y_train[I],\n",
    "                       marker='x', color=colors_r[t], s=50,\n",
    "                       label=f'Task {t}'+' ($\\sigma_n$'+f'={σn1:.2f}, '+'$mse$'+f'={mse:.3f})')\n",
    "\n",
    "            ax.grid()\n",
    "            ax.set_xlim(xlim)\n",
    "            ax.set_ylim(ylim)\n",
    "            ax.set_ylabel('$'+f'{mode}'+'}$')\n",
    "            ax.legend(loc='upper left', fontsize=15)\n",
    "            title = '$\\ell_x,\\ell_y$'+f'={ℓx:.2f},{ℓy:.2f}'+' $\\sigma_y$'+f'={σy:.2f}'+' $-mll_{1}$'+f'={-mll:.2f}'\n",
    "            ax.set_title(title, fontsize=30)\n",
    "\n",
    "            ax = axs[i, 1]\n",
    "            XX = np.vstack((X_train_main, X_test_main))\n",
    "            k = lambda X, Y: mtgp_k_soft(X, Y, logℓx, logℓy, logσy)\n",
    "            K = k(XX, XX)\n",
    "            im = ax.imshow(normalize_K(K), cmap=cmap)\n",
    "            fig.colorbar(im, cax=plt_scaled_colobar_ax(ax))\n",
    "            ax.set_title('$K(X_{train@1}, X_{test@1})$')\n",
    "\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "#     plt_savefig(fig, f'summary/assets/plt_mtgp_coorperative.png')\n",
    "#     plt_savefig(fig, f'summary/assets/plt_mtgp_competitive.png')\n",
    "#     plt_savefig(fig, f'summary/assets/plt_amtgp_shift={shift}_scale={scale}.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "shifts = np.unique([x[0] for x in mses.keys()])\n",
    "for s in shifts:\n",
    "    data.append([s] + list([mses[(s,mode)] for mode in modes]))\n",
    "\n",
    "print(tabulate(data, tablefmt='simple', headers=['shift']+modes, floatfmt='.3f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(data, tablefmt='latex', headers=['shift']+modes, floatfmt='.3f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X0 = np.sort(np.random.rand(int(n_train*task0_n_ratio), 1), axis=0)\n",
    "X1 = np.sort(np.random.rand(n_train-len(X0), 1)*.5, axis=0)\n",
    "X_train = np.vstack((np.hstack((X0, np.zeros_like(X0))),\n",
    "                     np.hstack((X1, np.ones_like(X1)))))\n",
    "\n",
    "f0 = lambda X: np.round(np.sin(6*X)*2)/2\n",
    "f1 = lambda X: np.sin(6*X + shift)\n",
    "fs = [f0,f1]\n",
    "Y0 = f0(X0) + np.random.randn(*X0.shape)*σns[0]\n",
    "Y1 = f1(X1) + np.random.randn(*X1.shape)*σns[1]\n",
    "y_train = np.vstack((Y0,Y1))\n",
    "\n",
    "plt.scatter(X0, f0(X0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio=5/6\n",
    "\n",
    "#    0.000 &  0.845 &  0.020 &   0.426 \\\\\n",
    "#    0.100 &  0.983 &  0.013 &   0.187 \\\\\n",
    "#    0.300 &  1.240 &  0.036 &   0.166 \\\\\n",
    "#    0.500 &  1.440 &  0.136 &   0.287 \\\\\n",
    "#    1.000 &  1.493 &  0.420 &   0.369 \\\\\n",
    "#    1.500 &  0.923 &  0.362 &   0.629 \\\\\n",
    "#    2.000 &  0.294 &  0.232 &   0.285 \\\\\n",
    "    \n",
    "# ratio=1/2\n",
    "#    0.000 &  0.159 &  0.012 &   0.055 \\\\\n",
    "#    0.100 &  0.150 &  0.009 &   0.105 \\\\\n",
    "#    0.300 &  0.141 &  0.027 &   0.175 \\\\\n",
    "#    0.500 &  0.141 &  0.034 &   0.167 \\\\\n",
    "#    1.000 &  0.156 &  0.058 &   0.174 \\\\\n",
    "#    1.500 &  0.140 &  0.044 &   0.173 \\\\\n",
    "#    2.000 &  0.078 &  0.046 &   0.132 \\\\\n",
    "\n",
    "# scale=12, ratio=2/3 n=100\n",
    "# \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
