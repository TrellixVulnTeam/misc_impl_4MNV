{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://flax.readthedocs.io/en/latest/notebooks/flax_basics.html\n",
    "\n",
    "import jax\n",
    "from typing import Any, Callable, Sequence, Optional\n",
    "from jax import lax, random, numpy as jnp\n",
    "import flax\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as nn\n",
    "\n",
    "from jax.config import config\n",
    "config.enable_omnistaging() # Linen requires enabling omnistaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://flax.readthedocs.io/en/latest/notebooks/flax_basics.html\n",
    "# \n",
    "\n",
    "# only need to define output shape\n",
    "model = nn.Dense(features=5)\n",
    "\n",
    "key1, key2 = random.split(random.PRNGKey(0))\n",
    "# (n, m) = 10, 5\n",
    "# dim(x) = 10, dim(y) = 5\n",
    "x = random.normal(key1, (10,)) # Dummy input\n",
    "# shape inference triggered with `x` here\n",
    "params = model.init(key2, x) # Initialization call\n",
    "# returns an `immutable` frozen dict\n",
    "print(jax.tree_map(lambda x: x.shape, params)) # Checking output shapes\n",
    "\n",
    "# Set problem dimensions\n",
    "nsamples = 20\n",
    "xdim = 10\n",
    "ydim = 5\n",
    "\n",
    "# Generate random ground truth W and b\n",
    "key = random.PRNGKey(0)\n",
    "k1, k2 = random.split(key)\n",
    "W = random.normal(k1, (xdim, ydim))\n",
    "b = random.normal(k2, (ydim,))\n",
    "true_params = freeze({'params': {'bias': b, 'kernel': W}})\n",
    "\n",
    "# Generate samples with additional noise\n",
    "ksample, knoise = random.split(k1)\n",
    "x_samples = random.normal(ksample, (nsamples, xdim))\n",
    "y_samples = jnp.dot(x_samples, W) + b\n",
    "y_samples += 0.1*random.normal(knoise,(nsamples, ydim)) # Adding noise\n",
    "print('x shape:', x_samples.shape, '; y shape:', y_samples.shape)\n",
    "\n",
    "\n",
    "def make_mse_func(x_batched, y_batched):\n",
    "    def mse(params):\n",
    "        # Define the squared loss for a single pair (x,y)\n",
    "        def squared_error(x, y):\n",
    "            pred = model.apply(params, x)\n",
    "            return jnp.inner(y-pred, y-pred)/2.0\n",
    "        # We vectorize the previous to compute the average of the loss on all (batched) samples!\n",
    "        return jnp.mean(jax.vmap(squared_error)(x_batched,y_batched), axis=0)\n",
    "    return jax.jit(mse) # And finally we jit the result.\n",
    "\n",
    "# Get the sampled loss\n",
    "loss = make_mse_func(x_samples, y_samples)\n",
    "\n",
    "\n",
    "alpha = 0.3 # Gradient step size\n",
    "print('Loss for \"true\" W,b: ', loss(true_params))\n",
    "grad_fn = jax.value_and_grad(loss)\n",
    "\n",
    "for i in range(101):\n",
    "    # We perform one gradient update\n",
    "    loss_val, grad = grad_fn(params)\n",
    "    params = jax.tree_multimap(lambda old, grad: old - alpha * grad,\n",
    "                               params, grad)\n",
    "    if i % 10 == 0:\n",
    "        print('Loss step {}: '.format(i), loss_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_samples.shape, y_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import optim\n",
    "from gp import flax_create_optimizer\n",
    "\n",
    "def flax_create_optimizer(params, lr, optimizer='GradientDescent'):\n",
    "    optimizer_cls = getattr(optim, optimizer)\n",
    "    return optimizer_cls(learning_rate=lr).create(params)\n",
    "\n",
    "\n",
    "\n",
    "# explicit model definition\n",
    "#\n",
    "class MLP(nn.Module):\n",
    "    features: Sequence[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for feat in self.features[:-1]:\n",
    "            x = nn.relu(nn.Dense(feat)(x))\n",
    "        x = nn.Dense(self.features[-1])(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
    "model = MLP([ydim])\n",
    "params = model.init(key2, jnp.ones((1,xdim)))\n",
    "\n",
    "\n",
    "print('initialized parameter shapes:\\n', jax.tree_map(jnp.shape, unfreeze(params)))\n",
    "print('output:\\n', model.apply(params, x_samples[0]))\n",
    "\n",
    "def get_loss_fn(x_batched, y_batched):\n",
    "    def loss_fn(params):\n",
    "        # params -> objective\n",
    "        def loss_fn_one(x, y):\n",
    "            pred = model.apply(params, x)\n",
    "            return jnp.inner(y-pred, y-pred)/2.0\n",
    "        return jnp.mean(jax.vmap(loss_fn_one)(x_batched, y_batched), axis=0)\n",
    "    return jax.jit(loss_fn)\n",
    "\n",
    "def loss_fn(params):\n",
    "    # params -> objective\n",
    "    def loss_fn_one(x, y):\n",
    "        pred = model.apply(params, x)\n",
    "        return jnp.inner(y-pred, y-pred)/2.0\n",
    "    return jnp.mean(jax.vmap(loss_fn_one)(x_samples, y_samples), axis=0)\n",
    "\n",
    "loss = jax.jit(loss_fn)\n",
    "f = loss_fn\n",
    "\n",
    "\n",
    "# `flax.optim`\n",
    "\n",
    "# optimizer = create_optimizer(params, alpha)\n",
    "\n",
    "\n",
    "from gp import flax_run_optim, log_func_default\n",
    "\n",
    "params = flax_run_optim(loss_fn, params, lr=.2, num_steps=10,\n",
    "                        log_func=lambda i,f,params: print(i, f(params)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SimpleDense(nn.Module):\n",
    "    features: int\n",
    "    kernel_init: Callable = nn.initializers.lecun_normal()\n",
    "    bias_init: Callable = nn.initializers.zeros\n",
    "        \n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        kernel = self.param('kernel',\n",
    "                            self.kernel_init,\n",
    "                            (inputs.shape[-1], self.features))\n",
    "        \n",
    "        print(kernel.shape, inputs.shape)\n",
    "        \n",
    "        y = lax.dot_general(inputs, kernel,\n",
    "                            (((inputs.ndim - 1,), (0,)), ((), ())),) # TODO Why not jnp.dot?\n",
    "        bias = self.param('bias', self.bias_init, (self.features,))\n",
    "        y = y + bias\n",
    "        return y\n",
    "\n",
    "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
    "x = random.uniform(key1, (4,4))\n",
    "\n",
    "model = SimpleDense(features=3)\n",
    "params = model.init(key2, x)\n",
    "y = model.apply(params, x)\n",
    "\n",
    "print('initialized parameters:\\n', params)\n",
    "print('output:\\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:misc_impl] *",
   "language": "python",
   "name": "conda-env-misc_impl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
