{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reference:\n",
    "#    https://peterroelants.github.io/posts/gaussian-process-kernels/\n",
    "#    https://distill.pub/2019/visual-exploration-gaussian-processes/\n",
    "#    http://gregorygundersen.com/blog/2019/06/27/gp-regression/\n",
    "#\n",
    "import numpy as np\n",
    "from numpy.linalg import inv, det, cholesky\n",
    "from numpy.linalg import solve as backsolve\n",
    "np.set_printoptions(precision=3,suppress=True)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import jax\n",
    "from jax import grad, jit, vmap, device_put\n",
    "import jax.numpy as jnp\n",
    "import jax.numpy.linalg as jnp_linalg\n",
    "from jax.experimental import optimizers\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "# https://matplotlib.org/3.1.1/gallery/style_sheets/style_sheets_reference.html\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 25\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "cmap = plt.cm.get_cmap('bwr')\n",
    "\n",
    "import sys\n",
    "sys.path.append('../kernel')\n",
    "from jaxkern import (cov_se, cov_se2, cov_rq, cov_pe, LookupKernel, normalize_K)\n",
    "\n",
    "from plt_utils import plt_savefig, plt_scaled_colobar_ax\n",
    "from gp import gp_regression_chol, run_sgd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Parameters\n",
    "\n",
    "M = 2\n",
    "n_train = 50\n",
    "n_test = 50\n",
    "ylim = (-3,3)\n",
    "xlim = (-.2,1.2)\n",
    "σn = [.03, .1]\n",
    "ℓ = .2\n",
    "max_mml = True\n",
    "mtl = True\n",
    "lr = .002\n",
    "num_steps = 200\n",
    "verbose = False\n",
    "\n",
    "def log_func(f, params):\n",
    "    if verbose:\n",
    "        print(f\"loss={f(params):.3f}\\t\"\n",
    "              f\"ℓ={jnp.exp(params['logℓ']):.3f}\\t\"\n",
    "              f\"σn={np.asarray(jnp.exp(params['logsn']))}\\t\")\n",
    "\n",
    "## Data\n",
    "np.random.seed(0)\n",
    "\n",
    "B = jnp.eye(M)\n",
    "\n",
    "X0 = np.random.rand(n_train*2//4, 1) # *.5+.5\n",
    "X1 = np.random.rand(n_train-len(X0), 1)*.5\n",
    "X_train = np.vstack((np.hstack((X0, np.zeros_like(X0))),\n",
    "                     np.hstack((X1, np.ones_like(X1)))))\n",
    "\n",
    "f0 = lambda X: np.sin(6*X)\n",
    "f1 = lambda X: np.sin(6*X + 1)\n",
    "fs = [f0,f1]\n",
    "Y0 = f0(X0) + np.random.randn(*X0.shape)*σn[0]\n",
    "Y1 = f1(X1) + np.random.randn(*X1.shape)*σn[1]\n",
    "y_train = np.vstack((Y0,Y1))\n",
    "\n",
    "X_test = np.vstack((np.tile(np.linspace(xlim[0], xlim[1], n_test), M),\n",
    "                    np.hstack([t*np.ones(n_test) for t in range(M)]))).T\n",
    "\n",
    "## Plotting\n",
    "\n",
    "colors_b = [cmap(.1), cmap(.3)]\n",
    "colors_r = [cmap(.9), cmap(.7)]\n",
    "\n",
    "gridspec_kw = {'width_ratios': [2, 1], 'height_ratios': [1, 1]}\n",
    "fig, axs = plt.subplots(2, 2, gridspec_kw=gridspec_kw)\n",
    "fig.set_size_inches(15, 10)\n",
    "\n",
    "\n",
    "for i, mtl in enumerate([False, True]):\n",
    "    ax = axs[i, 0]\n",
    "\n",
    "    ## Training\n",
    "\n",
    "    def mtgp_k(XT, XTp, logℓ, B):\n",
    "        X, Xp = XT[:,0], XTp[:,0]\n",
    "        Kx = cov_se2(X, Xp, logℓ=logℓ)\n",
    "        T, Tp = np.asarray(XT[:,1], np.int), np.asarray(XTp[:,1], np.int)\n",
    "        Kt = LookupKernel(T, Tp, B)\n",
    "        K = Kx*Kt\n",
    "        return K\n",
    "\n",
    "    if max_mml:\n",
    "        if mtl:\n",
    "            def nmll(params):\n",
    "                L = jnp.exp(params['logL'])\n",
    "                B = L@L.T\n",
    "                k = lambda X, Y: mtgp_k(X, Y, params['logℓ'], B)\n",
    "                μ, Σ, mll = gp_regression_chol(\n",
    "                    X_train, y_train, X_test, k, logsn=params['logsn'])\n",
    "                return -mll\n",
    "            params = {'logℓ': jnp.log(1.),\n",
    "                      'logsn': jnp.log(.1*jnp.ones(M)),\n",
    "                      'logL': jnp.log(jnp.array(np.random.rand(M,M)))}\n",
    "            res = run_sgd(nmll, params, lr=lr, num_steps=num_steps, log_func=log_func)\n",
    "            logℓ, logsn = res['logℓ'].item(), res['logsn']\n",
    "            ℓ, σn = jnp.exp(logℓ), jnp.exp(logsn)\n",
    "            L = jnp.exp(params['logL'])\n",
    "            B = L@L.T\n",
    "        else:\n",
    "            def nmll(params):\n",
    "                k = lambda X, Y: mtgp_k(X, Y, params['logℓ'], B)\n",
    "                μ, Σ, mll = gp_regression_chol(\n",
    "                    X_train, y_train, X_test, k, logsn=params['logsn'])\n",
    "                return -mll\n",
    "            params = {'logℓ': jnp.log(1.),\n",
    "                      'logsn': jnp.log(.1*jnp.ones(M))}\n",
    "            res = run_sgd(nmll, params, lr=lr, num_steps=num_steps, log_func=None)\n",
    "            logℓ, logsn = res['logℓ'].item(), res['logsn']\n",
    "            ℓ, σn = jnp.exp(logℓ), jnp.exp(logsn)\n",
    "\n",
    "    ## Plotting\n",
    "\n",
    "    k = lambda X, Y: mtgp_k(X, Y, logℓ, B)\n",
    "    μ, Σ, mll = gp_regression_chol(X_train, y_train, X_test, k, logsn)\n",
    "    std = np.expand_dims(np.sqrt(np.diag(Σ)), 1)\n",
    "\n",
    "    for t in range(M):\n",
    "        # task-specific mll\n",
    "        I = X_test[:,1] == t\n",
    "        # posterior predictive distribution\n",
    "        X_test_, μ_, std_ = X_test[I,0].squeeze(), μ[I].squeeze(), std[I].squeeze()\n",
    "        ax.plot(X_test_, μ_, color=colors_b[t], lw=2)\n",
    "        ax.fill_between(X_test_, μ_-2*std_, μ_+2*std_, alpha=.2, color=colors_b[t])\n",
    "        # generating function for main task\n",
    "        if t == 1:\n",
    "            ax.plot(X_test_, fs[t](X_test_), color='k', linestyle='dashed', linewidth=1)\n",
    "        \n",
    "        mse = mean_squared_error(μ[I], f1(X_test[I,0]))\n",
    "        # train data points\n",
    "        I = X_train[:,1] == t\n",
    "        ax.scatter(X_train[I,0], y_train[I],\n",
    "                   marker='x', color=colors_r[t], s=50,\n",
    "                   label=f'Task {t}'+' ($\\sigma_n$'+f'={σn[t]:.2f}, '+'$mse$'+f'={mse:.3f})')\n",
    "        \n",
    "    ax.grid()\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.legend(fontsize=15)\n",
    "    title = '$\\ell$'+f'={ℓ:.2f}'+ \\\n",
    "        ' $B_{01}/B_{00}$'+f'={B[0,1]*2/(B[0,0]+B[1,1]):.2f}'+ \\\n",
    "        ' $-mll$'+f'={-mll:.2f}'\n",
    "    ax.set_title(title, fontsize=30)\n",
    "    \n",
    "\n",
    "    ax = axs[i, 1]\n",
    "    XX = np.vstack((X_train, X_test[X_test[:,1]==1]))\n",
    "    K = k(XX, XX)\n",
    "    im = ax.imshow(normalize_K(K), cmap=cmap)\n",
    "    fig.colorbar(im, cax=plt_scaled_colobar_ax(ax))\n",
    "    ax.set_title('$K(X_{train}, X_{test@1})$')\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt_savefig(fig, 'summary/assets/plt_mtgp.png')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
