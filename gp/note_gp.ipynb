{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reference:\n",
    "#    https://peterroelants.github.io/posts/gaussian-process-kernels/\n",
    "#    https://distill.pub/2019/visual-exploration-gaussian-processes/\n",
    "#    http://gregorygundersen.com/blog/2019/06/27/gp-regression/\n",
    "#\n",
    "import numpy as np\n",
    "from numpy.linalg import inv, det, cholesky\n",
    "from numpy.linalg import solve as backsolve\n",
    "np.set_printoptions(precision=3,suppress=True)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import jax\n",
    "from jax import grad, jit, vmap, device_put\n",
    "import jax.numpy as jnp\n",
    "import jax.numpy.linalg as jnp_linalg\n",
    "from jax.experimental import optimizers\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "# https://matplotlib.org/3.1.1/gallery/style_sheets/style_sheets_reference.html\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 25\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "cmap = plt.cm.get_cmap('bwr')\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "import sys\n",
    "sys.path.append('../kernel')\n",
    "from jaxkern import (cov_se, cov_se2, cov_rq, cov_pe, LookupKernel, normalize_K)\n",
    "\n",
    "from plt_utils import plt_savefig, plt_scaled_colobar_ax\n",
    "from gp import gp_regression_chol, run_sgd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Parameters\n",
    "\n",
    "M = 2\n",
    "n_train = 50\n",
    "n_test = 50\n",
    "ylim = (-3,3)\n",
    "xlim = (-.2,1.2)\n",
    "σn = [.03, .1]\n",
    "ℓ = .2\n",
    "mode = 'stgp'\n",
    "mode = 'mtgp'\n",
    "mode = 'amtgp'\n",
    "lr = .002\n",
    "num_steps = 200\n",
    "verbose = True\n",
    "        \n",
    "def log_func(i, f, params):\n",
    "    if verbose:\n",
    "        print(tabulate([[f(params)]+[x.item() if x.size==1 else x for x in list(params.values())]],\n",
    "                       tablefmt=\"plain\",\n",
    "                       floatfmt=\".3f\",\n",
    "                       headers=['loss']+list(params.keys()) if i == 0 else ()))\n",
    "\n",
    "\n",
    "## Plotting\n",
    "\n",
    "colors_b = [cmap(.1), cmap(.3)]\n",
    "colors_r = [cmap(.9), cmap(.7)]\n",
    "\n",
    "for si, shift in enumerate([1.5]):\n",
    "    \n",
    "    nrows = 3\n",
    "    gridspec_kw = {'width_ratios': [2, 1], 'height_ratios': [1 for _ in range(nrows)]}\n",
    "    fig, axs = plt.subplots(nrows, 2, gridspec_kw=gridspec_kw)\n",
    "    fig.set_size_inches(15, 5*nrows)\n",
    "\n",
    "\n",
    "    for i, mode in enumerate(['stgp', 'mtgp', 'amtgp']):\n",
    "\n",
    "        ## Data\n",
    "        np.random.seed(0)\n",
    "\n",
    "        B = jnp.eye(M)\n",
    "\n",
    "        X0 = np.random.rand(n_train*2//4, 1) # *.5+.5\n",
    "        X1 = np.random.rand(n_train-len(X0), 1)*.5\n",
    "        X_train = np.vstack((np.hstack((X0, np.zeros_like(X0))),\n",
    "                             np.hstack((X1, np.ones_like(X1)))))\n",
    "\n",
    "        f0 = lambda X: np.sin(6*X)\n",
    "        f1 = lambda X: np.sin(6*X + shift)\n",
    "        fs = [f0,f1]\n",
    "        Y0 = f0(X0) + np.random.randn(*X0.shape)*σn[0]\n",
    "        Y1 = f1(X1) + np.random.randn(*X1.shape)*σn[1]\n",
    "        y_train = np.vstack((Y0,Y1))\n",
    "\n",
    "        X_test = np.vstack((np.tile(np.linspace(xlim[0], xlim[1], n_test), M),\n",
    "                            np.hstack([t*np.ones(n_test) for t in range(M)]))).T\n",
    "\n",
    "\n",
    "        ## Training\n",
    "\n",
    "        def mtgp_k(XT, XTp, logℓ, B):\n",
    "            X, Xp = XT[:,0], XTp[:,0]\n",
    "            Kx = cov_se2(X, Xp, logℓ=logℓ)\n",
    "            T, Tp = np.asarray(XT[:,1], np.int), np.asarray(XTp[:,1], np.int)\n",
    "            Kt = LookupKernel(T, Tp, B)\n",
    "            K = Kx*Kt\n",
    "            return K\n",
    "\n",
    "        def mtgp_k_soft(XY, XYp, logℓx, logℓy):\n",
    "            X, Xp = XY[:,0], XYp[:,0]\n",
    "            Kx = cov_se2(X, Xp, logℓ=logℓx)\n",
    "            Y, Yp = XY[:,1], XYp[:,1]\n",
    "            Ky = cov_se2(Y, Yp, logℓ=logℓy)\n",
    "            K = Kx*Ky\n",
    "            return K\n",
    "\n",
    "        if mode == 'stgp':\n",
    "            def nmll(params):\n",
    "                k = lambda X, Y: mtgp_k(X, Y, params['logℓ'], B)\n",
    "                μ, Σ, mll = gp_regression_chol(\n",
    "                    X_train, y_train, X_test, k, logsn=params['logsn'])\n",
    "                return -mll\n",
    "            params = {'logℓ': jnp.log(1.),\n",
    "                      'logsn': jnp.log(.1*jnp.ones(M))}\n",
    "            res = run_sgd(nmll, params, lr=lr, num_steps=num_steps, log_func=None)\n",
    "            logℓ, logsn = res['logℓ'].item(), res['logsn']\n",
    "            ℓ, σn = jnp.exp(logℓ), jnp.exp(logsn)\n",
    "        if mode == 'mtgp':\n",
    "            def nmll(params):\n",
    "                L = jnp.exp(params['logL'])\n",
    "                B = L@L.T\n",
    "                k = lambda X, Y: mtgp_k(X, Y, params['logℓ'], B)\n",
    "                μ, Σ, mll = gp_regression_chol(\n",
    "                    X_train, y_train, X_test, k, logsn=params['logsn'])\n",
    "                return -mll\n",
    "            params = {'logℓ': jnp.log(1.),\n",
    "                      'logsn': jnp.log(.1*jnp.ones(M)),\n",
    "                      'logL': jnp.log(jnp.array(np.random.rand(M,M)))}\n",
    "            res = run_sgd(nmll, params, lr=lr, num_steps=num_steps, log_func=None)\n",
    "            logℓ, logsn = res['logℓ'].item(), res['logsn']\n",
    "            ℓ, σn = jnp.exp(logℓ), jnp.exp(logsn)\n",
    "            L = jnp.exp(params['logL'])\n",
    "            B = L@L.T\n",
    "        if mode == 'amtgp':\n",
    "            # Train independently on auxiliary task 0\n",
    "            #\n",
    "            I = X_train[:,1] == 0\n",
    "            X_train_aux = X_train[I,0]\n",
    "            y_train_aux = y_train[I]\n",
    "            def nmll(params):\n",
    "                k = lambda X, Y: cov_se2(X, Y, logℓ=params['logℓ'])\n",
    "                μ, Σ, mll = gp_regression_chol(\n",
    "                    X_train_aux, y_train_aux, X_test, k, logsn=params['logsn'])\n",
    "                return -mll\n",
    "            params = {'logℓ': jnp.log(1.),'logsn': jnp.log(.1)}\n",
    "            res = run_sgd(nmll, params, lr=lr, num_steps=num_steps, log_func=None)\n",
    "            logℓ0, logsn0 = res['logℓ'].item(), res['logsn']\n",
    "            ℓ0, σn0 = jnp.exp(logℓ0), jnp.exp(logsn0)\n",
    "\n",
    "            # Predict at task=1's location\n",
    "            I = X_train[:,1]==1\n",
    "            k = lambda X, Y: cov_se2(X, Y, logℓ=logℓ0)\n",
    "            μ, _, _ = gp_regression_chol(X_train_aux, y_train_aux, X_train[I,0], k, logsn0)\n",
    "            # (X1, Y0μ)\n",
    "            X_train_main = np.vstack((X_train[I,0], μ.squeeze())).T\n",
    "            y_train_main = y_train[I]\n",
    "            def nmll(params):\n",
    "                k = lambda X, Y: mtgp_k_soft(X, Y, params['logℓx'], params['logℓy'])\n",
    "                μ, Σ, mll = gp_regression_chol(\n",
    "                    X_train_main, y_train_main, X_test, k, logsn=params['logsn'])\n",
    "                return -mll\n",
    "            params = {'logℓx': jnp.log(1.),\n",
    "                      'logℓy': jnp.log(1.),\n",
    "                      'logsn': jnp.log(.1)}\n",
    "            res = run_sgd(nmll, params, lr=lr, num_steps=num_steps, log_func=None)\n",
    "            logℓx, logℓy, logsn1 = res['logℓx'].item(), res['logℓy'].item(), res['logsn'].item()\n",
    "            ℓx, ℓy, σn1 = jnp.exp(logℓx), np.exp(logℓy), jnp.exp(logsn1)\n",
    "\n",
    "\n",
    "        ## Plotting\n",
    "\n",
    "        if mode in ['stgp', 'mtgp']:\n",
    "            ax = axs[i, 0]\n",
    "            k = lambda X, Y: mtgp_k(X, Y, logℓ, B)\n",
    "            μ, Σ, mll = gp_regression_chol(X_train, y_train, X_test, k, logsn)\n",
    "            std = np.expand_dims(np.sqrt(np.diag(Σ)), 1)\n",
    "\n",
    "            for t in range(M):\n",
    "                # task-specific mll\n",
    "                I = X_test[:,1] == t\n",
    "                # posterior predictive distribution\n",
    "                X_test_, μ_, std_ = X_test[I,0].squeeze(), μ[I].squeeze(), std[I].squeeze()\n",
    "                ax.plot(X_test_, μ_, color=colors_b[t], lw=2)\n",
    "                ax.fill_between(X_test_, μ_-2*std_, μ_+2*std_, alpha=.2, color=colors_b[t])\n",
    "                # generating function for main task\n",
    "                if t == 1:\n",
    "                    ax.plot(X_test_, fs[t](X_test_), color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "                mse = mean_squared_error(μ[I], f1(X_test[I,0]))\n",
    "                # train data points\n",
    "                I = X_train[:,1] == t\n",
    "                ax.scatter(X_train[I,0], y_train[I],\n",
    "                           marker='x', color=colors_r[t], s=50,\n",
    "                           label=f'Task {t}'+' ($\\sigma_n$'+f'={σn[t]:.2f}, '+'$mse$'+f'={mse:.3f})')\n",
    "\n",
    "            ax.grid()\n",
    "            ax.set_xlim(xlim)\n",
    "            ax.set_ylim(ylim)\n",
    "            ax.legend(fontsize=15)\n",
    "            title = '$\\ell$'+f'={ℓ:.2f}'+ \\\n",
    "                ' $B_{01}/B_{00}$'+f'={B[0,1]*2/(B[0,0]+B[1,1]):.2f}'+ \\\n",
    "                ' $-mll$'+f'={-mll:.2f}'\n",
    "            ax.set_title(title, fontsize=30)\n",
    "\n",
    "\n",
    "            ax = axs[i, 1]\n",
    "            XX = np.vstack((X_train, X_test[X_test[:,1]==1]))\n",
    "            K = k(XX, XX)\n",
    "            im = ax.imshow(normalize_K(K), cmap=cmap)\n",
    "            fig.colorbar(im, cax=plt_scaled_colobar_ax(ax))\n",
    "            ax.set_title('$K(X_{train}, X_{test@1})$')\n",
    "\n",
    "\n",
    "        if mode == 'amtgp':\n",
    "            ax = axs[i, 0]\n",
    "            # Predict at task=1's test location\n",
    "            I = X_test[:,1]==1\n",
    "            k = lambda X, Y: cov_se2(X, Y, logℓ=logℓ0)\n",
    "            Y0μ, Σ, _ = gp_regression_chol(X_train_aux, y_train_aux, X_test[I,0], k, logsn0)\n",
    "            # Use posterior mean of task=0 regressor as additional input to task=1 regressor\n",
    "            X_test_main = np.vstack((X_test[I,0], Y0μ.squeeze())).T\n",
    "            k = lambda X, Y: mtgp_k_soft(X, Y, logℓx, logℓy)\n",
    "            Y1μ, Σ, mll = gp_regression_chol(X_train_main, y_train_main, X_test_main, k, logsn1)\n",
    "            Y1std = np.expand_dims(np.sqrt(np.diag(Σ)), 1).squeeze()\n",
    "            Y1μ = Y1μ.squeeze()\n",
    "\n",
    "            t = 1\n",
    "            ax.plot(X_test_main[:,0], Y1μ, color=colors_b[t], lw=2)\n",
    "            ax.fill_between(X_test_main[:,0], Y1μ-2*Y1std, Y1μ+2*Y1std, alpha=.2, color=colors_b[t])\n",
    "            ax.plot(X_test_main[:,0], fs[t](X_test_main[:,0]), color='k', linestyle='dashed', linewidth=1)\n",
    "\n",
    "            mse = mean_squared_error(Y1μ, f1(X_test[I,0]))\n",
    "            I = X_train[:,1] == t\n",
    "            ax.scatter(X_train[I,0], y_train[I],\n",
    "                       marker='x', color=colors_r[t], s=50,\n",
    "                       label=f'Task {t}'+' ($\\sigma_n$'+f'={σn1:.2f}, '+'$mse$'+f'={mse:.3f})')\n",
    "\n",
    "            ax.grid()\n",
    "            ax.set_xlim(xlim)\n",
    "            ax.set_ylim(ylim)\n",
    "            ax.legend(fontsize=15)\n",
    "            title = '$\\ell_x,\\ell_y$'+f'={ℓx:.2f},{ℓy:.2f}'+' $-mll_{1}$'+f'={-mll:.2f}'\n",
    "            ax.set_title(title, fontsize=30)\n",
    "\n",
    "            ax = axs[i, 1]\n",
    "            XX = np.vstack((X_train_main, X_test_main))\n",
    "            k = lambda X, Y: mtgp_k_soft(X, Y, logℓx, logℓy)\n",
    "            K = k(XX, XX)\n",
    "            im = ax.imshow(normalize_K(K), cmap=cmap)\n",
    "            fig.colorbar(im, cax=plt_scaled_colobar_ax(ax))\n",
    "            ax.set_title('$K(X_{train@1}, X_{test@1})$')\n",
    "\n",
    "\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt_savefig(fig, f'summary/assets/plt_mtgp_shift={shift}.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    [0,  .069, .052, .046],\n",
    "    [.1, .083, .052, .082],\n",
    "    [.5, .188, .098, .187],\n",
    "    [1,  .360, .403, .366],\n",
    "    [1.5,.433, .881, .458]\n",
    "]\n",
    "\n",
    "\n",
    "print(tabulate(data,\n",
    "               tablefmt='latex',\n",
    "               headers=['shift','stgp', 'mtgp', 'amtgp'],\n",
    "               floatfmt='.3f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    [0,  1.41/11.02],\n",
    "    [.1, 1.45/2],\n",
    "    [.5, 2.74/.31],\n",
    "    [1,  7.24/.31],\n",
    "    [1.5,9.4/.32]\n",
    "]\n",
    "\n",
    "\n",
    "print(tabulate(data,\n",
    "               tablefmt='latex',\n",
    "               headers=['shift', 'ℓy/ℓx'],\n",
    "               floatfmt='.1f'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
