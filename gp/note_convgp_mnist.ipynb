{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a2488e5",
   "metadata": {},
   "source": [
    "goal\n",
    "- [done] SVGP + dirichlet training on mnist\n",
    "    - recreate evidential DL example ... \n",
    "- [done] variational learning of supporting image patches !\n",
    "    - [done] impl STN ... \n",
    "    - observations\n",
    "        - perf okay if allows finetune encoder network \n",
    "        - not so much as evidence that is localized ... \n",
    "            - perhaps due to fact shared inducing locations ... \\\n",
    "                would want to retain all info (not localized) and \\\n",
    "                use variational mean to modulate evidence for class\n",
    "            - so to get localized info ... might want to do per-class inducing variables\n",
    "            - also might want to put STN to kernel hyperparam ...\\\n",
    "                and put product kernel over both image and affine trans matrix \n",
    "            - might also try just using one STN for entire thing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c3c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import time\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import scipy\n",
    "import numpy as onp\n",
    "onp.set_printoptions(precision=3,suppress=True)\n",
    "\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap, device_put, random\n",
    "from flax import linen as nn\n",
    "from jax.scipy.stats import dirichlet\n",
    "\n",
    "from jax.lib import xla_bridge\n",
    "print('jax/jaxlib: ', jax.__version__, jax.lib.version)\n",
    "print(xla_bridge.get_backend().platform)\n",
    "print(jax.local_device_count())\n",
    "print(jax.devices())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.tri as tri\n",
    "# https://matplotlib.org/3.1.1/gallery/style_sheets/style_sheets_reference.html\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 25\n",
    "mpl.rcParams['font.family'] = 'Times New Roman' \n",
    "cmap = plt.cm.get_cmap('bwr')\n",
    "\n",
    "from tabulate import tabulate\n",
    "from functools import partial\n",
    "\n",
    "from plt_utils import *\n",
    "from gpax import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-quick",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "def dataset_subset(dataset, y):\n",
    "    # Convert labels to to [0,...,len(y)]\n",
    "    #     e.g. y = [6, 8] y_train/y_test will only have {0,1}\n",
    "    #\n",
    "    ind = np.any(np.stack([dataset.targets.numpy()==i for i in y]), axis=0)\n",
    "    ind = torch.where(torch.tensor(onp.array(ind))==True)[0]\n",
    "    F = torch.zeros((10,), dtype=torch.float32)\n",
    "    F[y] = torch.arange(len(y), dtype=torch.float32)\n",
    "    dataset.targets = F[dataset.targets[ind].to(torch.int64)]\n",
    "    dataset.data = dataset.data[ind]\n",
    "    return dataset\n",
    "\n",
    "key = random.PRNGKey(1)\n",
    "\n",
    "# https://stackoverflow.com/questions/66577151/http-error-when-trying-to-download-mnist-data\n",
    "new_mirror = 'https://ossci-datasets.s3.amazonaws.com/mnist'\n",
    "torchvision.datasets.MNIST.resources = [\n",
    "   ('/'.join([new_mirror, url.split('/')[-1]]), md5)\n",
    "   for url, md5 in torchvision.datasets.MNIST.resources\n",
    "]\n",
    "\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    lambda x: np.asarray(x)[...,np.newaxis] / 255.\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST('./data', train=True, transform=transforms, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST('./data', train=False, transform=transforms, download=True)\n",
    "digits = [1,2,5]\n",
    "train_dataset = dataset_subset(train_dataset, digits)\n",
    "test_dataset = dataset_subset(test_dataset, digits)\n",
    "X_train = jax_to_gpu(np.asarray(train_dataset.data[...,np.newaxis]) / 255.)\n",
    "y_train = jax_to_gpu(np.asarray(train_dataset.targets[...,np.newaxis]))\n",
    "X_test = jax_to_gpu(np.asarray(test_dataset.data[...,np.newaxis]) / 255.)\n",
    "y_test = jax_to_gpu(np.asarray(test_dataset.targets[...,np.newaxis]))\n",
    "data_train = (X_train, y_train)\n",
    "data_test = (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-syntax",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.where(y_test==0)[0][:10])\n",
    "\n",
    "ind = 1\n",
    "n_ims = 20\n",
    "\n",
    "fig, axs = plt.subplots(1,n_ims,figsize=(2*n_ims,2))\n",
    "[axi.set_xticks([]) for axi in axs.ravel()]; [axi.set_yticks([]) for axi in axs.ravel()]\n",
    "\n",
    "x, y = X_test[ind], y_test[ind]\n",
    "\n",
    "x_rot = rotated_ims(x, n_ims=n_ims)\n",
    "for i in range(n_ims):\n",
    "    ax = axs[i]\n",
    "    ax.imshow(x_rot[i], cmap='Greys')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = len(digits)\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"A simple CNN model.\"\"\"\n",
    "    output_dim: int = 2\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        x = nn.Dense(features=128)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=self.output_dim)(x)\n",
    "        x = nn.log_softmax(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class CNNTrunk(nn.Module):\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        x = nn.Dense(features=128)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def cross_entropy_loss(logits, labels):\n",
    "    y_onehot = jax.nn.one_hot(labels, num_classes=output_dim).squeeze()\n",
    "    return -np.mean(np.sum(y_onehot * logits, axis=-1))\n",
    "\n",
    "\n",
    "def compute_metrics(logits, labels):\n",
    "    loss = cross_entropy_loss(logits, labels)\n",
    "    pred = np.argmax(logits, -1).reshape(-1,1)\n",
    "    accuracy = np.mean(pred == labels)\n",
    "    metrics = {'loss': loss,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(opt, batch, key):\n",
    "    key, subkey = random.split(key)\n",
    "    X, y = batch\n",
    "    def loss_fn(params):\n",
    "        logits = CNN(output_dim=output_dim).apply(params, X)\n",
    "        loss = cross_entropy_loss(logits, y)\n",
    "        return loss, logits\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grad = grad_fn(opt.target)\n",
    "    opt = opt.apply_gradient(grad)\n",
    "    metrics = compute_metrics(logits, y)\n",
    "    log = {'loss': loss,\n",
    "           'accuracy': metrics['accuracy'],\n",
    "           'dense0_kernel_gradnorm': linalg.norm(grad['params']['Dense_0']['kernel'])}\n",
    "    return opt, log, key\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(params, X):\n",
    "    logits = CNN(output_dim=output_dim).apply(params, X)\n",
    "    return logits\n",
    "\n",
    "\n",
    "def eval_model(params, data_test, logit_fn=eval_step):\n",
    "    test_n_batches, test_batches = get_data_stream(\n",
    "        random.PRNGKey(0), 100, data_test)\n",
    "\n",
    "    logits = []; labels = []\n",
    "    for _ in range(test_n_batches):\n",
    "        batch = next(test_batches)\n",
    "        X, y = batch\n",
    "        logit = logit_fn(params, X)\n",
    "        labels.append(y.reshape(-1, 1))\n",
    "        logits.append(logit)\n",
    "\n",
    "    logits = np.vstack(logits)\n",
    "    labels = np.vstack(labels)\n",
    "    metrics = compute_metrics(logits, labels)\n",
    "    metrics = jax.tree_map(lambda x: x.item(), metrics)\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-selection",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = CNN(output_dim=output_dim)\n",
    "params = model.init(key, np.ones((1,28,28,1)))\n",
    "opt = flax_create_optimizer(params, 'Adam', {'learning_rate': .03})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-durham",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics = eval_model(params, data_test)\n",
    "print(f'[{0:3}] test \\t'\n",
    "      f'Loss={metrics[\"loss\"]:.3f}\\t'\n",
    "      f'accuracy={metrics[\"accuracy\"]:.3f}\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-malawi",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "bsz = 64\n",
    "train_n_batches, train_batches = get_data_stream(key, bsz, data_train)\n",
    "n_epochs = 10\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    logs = defaultdict(list)\n",
    "    for it in range(train_n_batches):\n",
    "        step = epoch*train_n_batches+it\n",
    "        batch = next(train_batches)\n",
    "        opt, log, key = train_step(opt, batch, key)\n",
    "        params = opt.target\n",
    "        for k, v in log.items():\n",
    "            logs[k].append(v)\n",
    "        if step%(train_n_batches//10)==0:\n",
    "            avg_metrics = {k: np.mean(np.array(v))\n",
    "                           for k, v in logs.items()}\n",
    "            print(f'[{epoch:3}|{100*it/train_n_batches:5.2f}%]\\t'\n",
    "                  f'Loss={avg_metrics[\"loss\"]:.3f}\\t'\n",
    "                  f'accuracy={avg_metrics[\"accuracy\"]:.3f}\\t'\n",
    "                  f'norm(Dense0.k)={avg_metrics[\"dense0_kernel_gradnorm\"]:.3f}')\n",
    "    \n",
    "    metrics = eval_model(params, data_test)\n",
    "    print(f'[{epoch:3}] test \\t'\n",
    "          f'Loss={metrics[\"loss\"]:.3f}\\t'\n",
    "          f'accuracy={metrics[\"accuracy\"]:.3f}\\t')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-constraint",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_save_path = f'./cnn_params_{\",\".join([str(x) for x in digits])}.pkl'\n",
    "pytree_save(opt.target, cnn_save_path)\n",
    "params = pyhttp://localhost:8895/notebooks/misc_impl/gp/note_svgp_mnist.ipynb#tree_load(CNN(output_dim=output_dim).init(key, np.ones((1,28,28,1))), cnn_save_path)\n",
    "\n",
    "metrics = eval_model(params, data_test)\n",
    "print(f'Loss={metrics[\"loss\"]:.3f}\\t'\n",
    "      f'accuracy={metrics[\"accuracy\"]:.3f}\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8468176",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Variations of inducing points\n",
    "#     1. Application of differentiable transformation, defined via `transform_cls`\n",
    "#     2. Whether inducing points wrt patches or images.\n",
    "# \n",
    "\n",
    "\n",
    "def reinitialize_inducing(params, key, transform_cls, X_train, L=10):\n",
    "    \"\"\"Make full use of the set of inducing points.\n",
    "        - find L inducing points with smallest average μ magnitude\n",
    "        - re-initialize transformation parameters randomly\n",
    "        - update corresponding images in `Xu/X`\n",
    "    \"\"\"\n",
    "\n",
    "    ## find L inducing points with smallest average μ magnitude\n",
    "    im = pytree_leaf(params, 'params/Xu/X')\n",
    "    qμ = pytree_leaf(params, 'params/q/μ')\n",
    "    μconst = pytree_leaf(params, 'params/mean_fn/c')\n",
    "    qμ_mag = np.mean(np.abs(qμ - μconst[...,np.newaxis]), axis=0)\n",
    "    ind = np.argsort(qμ_mag)[:L]\n",
    "\n",
    "    ## re-initialize transformation parameters randomly\n",
    "\n",
    "    def reinitialize_T(T):\n",
    "        trans = transform_cls()\n",
    "        trans_params = {'params': pytree_leaf(params, 'params/Xu/transform')}\n",
    "        default_T_init_fn = trans.apply(trans_params, method=trans.default_T_init)[1]\n",
    "        return jax.ops.index_update(T, ind, default_T_init_fn(key, (L,)))\n",
    "\n",
    "    params = pytree_mutate_with_fn(params, 'params/Xu/transform/T', reinitialize_T)\n",
    "\n",
    "    ## re-initialize random images `Xu/X`\n",
    "\n",
    "    key, k2 = random.split(key)\n",
    "    def reinitialize_X(X):\n",
    "        Xind = np.take(X_train, random.randint(k2, (L,), 0, len(X_train)), axis=0)\n",
    "        return jax.ops.index_update(X, ind, Xind)\n",
    "    params = pytree_mutate_with_fn(params, 'params/Xu/X', reinitialize_X)\n",
    "    \n",
    "    return params, key\n",
    "\n",
    "params = opt.target\n",
    "params, key = reinitialize_inducing(params, key, transform_cls, X_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "S = pytree_leaf(params, 'params/Xu/X')[ind]\n",
    "fn = vmap(transform_to_matrix, (0, None, None), 0)\n",
    "A = fn(pytree_leaf(params, 'params/Xu/transform/T'),\n",
    "       transform_cls().T_type,\n",
    "       transform_cls().A_init_val)[ind]\n",
    "\n",
    "fn = vmap(spatial_transform_details, (0, 0, None), 0)\n",
    "T, Gs = fn(A, S, patch_shape)\n",
    "fig, axs = plt.subplots(2, len(A), figsize=(3*len(A),3*2))\n",
    "for i in range(len(T)):\n",
    "    plt_spatial_transform(axs[:,i], Gs[i], S[i], T[i])\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b8b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)\n",
    "\n",
    "lik_type = 'LikMulticlassDirichlet' # LikMulticlassDirichlet, LikMulticlassSoftmax, LikMultipleNormalKron\n",
    "α_ϵ = 1; α_δ = 10; n_mc_samples = 20\n",
    "image_shape = (28,28,1)\n",
    "patch_shape = (7,7)\n",
    "n_inducing = 50\n",
    "T_type = 'transl' # 'transl', etc.\n",
    "use_loc_kernel = True\n",
    "\n",
    "\n",
    "init_val_m = gamma_to_lognormal(np.array([1.]))[0] \\\n",
    "    if lik_type == 'LikMulticlassDirichlet' else np.array([0.5])\n",
    "mean_fn_cls = partial(MeanConstant, output_dim=output_dim, init_val_m=init_val_m, flat=False)\n",
    "if lik_type == 'LikMulticlassDirichlet':\n",
    "    lik_cls = partial(LikMulticlassDirichlet, output_dim=output_dim, init_val_α_ϵ=α_ϵ, init_val_α_δ=α_δ, n_mc_samples=n_mc_samples)\n",
    "elif lik_type == 'LikMulticlassSoftmax':\n",
    "    lik_cls = partial(LikMulticlassSoftmax, output_dim=output_dim, n_mc_samples=n_mc_samples)\n",
    "else:\n",
    "    lik_cls = partial(LikMultipleNormalKron, output_dim=output_dim)\n",
    "\n",
    "g_cls = LayerIdentity # CNNTrunk\n",
    "# kx_cls = partial(CovSE, output_scaling=True)\n",
    "\n",
    "kl_cls = CovSE if use_loc_kernel else partial(CovConstant, train_σ2=False)\n",
    "kx_cls = partial(CovConvolutional, image_shape=image_shape, patch_shape=patch_shape,\n",
    "                 kg_cls=CovSE, patch_inducing_loc=True, kl_cls=kl_cls)\n",
    "k_cls = partial(CovMultipleOutputIndependent, k_cls=kx_cls, output_dim=output_dim, g_cls=g_cls)\n",
    "if T_type == '':\n",
    "    transform_cls = LayerIdentity\n",
    "    Xu_initial = get_init_patches(key, X_train, n_inducing, image_shape, patch_shape)\n",
    "else:\n",
    "    # transform_type = 'transl+isot_scal'; T_init_fn = lambda k,s: np.tile(np.array([.25, 0, 0]), (s[0], 1)); A_init_val = np.array([[1.,0,0],[0,1.,0]])\n",
    "    scal = np.array(patch_shape)/np.array(image_shape[:2])\n",
    "    A_init_val = trans2x3_from_scal_transl(scal,(0,0))\n",
    "    T_init_fn = None\n",
    "    transform_cls = partial(SpatialTransform, shape=patch_shape, n_transforms=n_inducing, \n",
    "                            T_type=T_type, A_init_val=A_init_val, output_transform=use_loc_kernel)\n",
    "    Xu_initial = np.take(X_train, random.randint(key, (n_inducing,), 0, len(X_train)), axis=0)\n",
    "\n",
    "inducing_loc_cls = partial(InducingLocations,\n",
    "                           shape=Xu_initial.shape,\n",
    "                           init_fn=lambda k,s: Xu_initial,\n",
    "                           transform_cls=transform_cls)\n",
    "\n",
    "print('Xu:', Xu_initial.shape)\n",
    "\n",
    "model = SVGP(mean_fn_cls=mean_fn_cls,\n",
    "             k_cls=k_cls,\n",
    "             lik_cls=lik_cls,\n",
    "             inducing_loc_cls=inducing_loc_cls,\n",
    "             n_data=len(X_train),\n",
    "             output_dim=output_dim)\n",
    "\n",
    "\n",
    "params = model.get_init_params(model, key, X_shape=image_shape)\n",
    "print(model)\n",
    "pytree_keys(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e8260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_patches_2d_scal_transl(image_shape, patch_shape)\n",
    "\n",
    "\n",
    "class BijBox(object):\n",
    "    lh: np.ndarray\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        \"\"\" x -> exp(x) \\in \\R+ \"\"\"\n",
    "        return np.exp(jax.nn.sigmoid())\n",
    "\n",
    "    @staticmethod\n",
    "    def reverse(y):\n",
    "        return np.log(y)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a19678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if isinstance(inducing_loc_cls(), InducingLocations):\n",
    "#     first_n = min(40, n_inducing)\n",
    "#     Xu = inducing_loc_cls().apply({'params': pytree_leaf(params, 'params/Xu')})\n",
    "#     fig, axs = plt.subplots(1,first_n,figsize=(3*first_n, 3))\n",
    "#     for i in range(first_n):\n",
    "#         axs[i].imshow(Xu[i,...].reshape(*patch_shape), cmap='Greys', vmin=0, vmax=1)\n",
    "\n",
    "# # if isinstance(inducing_loc_cls(), InducingLocationsSpatialTransform):\n",
    "# #     first_n = min(40, n_inducing)\n",
    "# #     Xu = inducing_loc_cls().apply({'params': pytree_leaf(params, 'params/Xu')})\n",
    "# #     fig, axs = plt.subplots(1,first_n,figsize=(3*first_n, 3))\n",
    "# #     for i in range(first_n):\n",
    "# #         axs[i].imshow(Xu[i,...].reshape(*patch_shape), cmap='Greys', vmin=0, vmax=1)\n",
    "    \n",
    "if isinstance(transform_cls(), SpatialTransform):\n",
    "    ind = np.arange(n_inducing)\n",
    "    \n",
    "    m = model.bind(params)\n",
    "    fn = vmap(transform_to_matrix, (0, None, None), 0)\n",
    "    A = fn(pytree_leaf(params, 'params/Xu/transform/T'), m.Xu.transform.T_type, m.Xu.transform.A_init_val)\n",
    "    S = pytree_leaf(params, 'params/Xu/X')\n",
    "    A = A[ind]; S = S[ind]\n",
    "\n",
    "    fn = vmap(spatial_transform_details, (0, 0, None), 0)\n",
    "    T, Gs = fn(A, S, patch_shape)\n",
    "    fig, axs = plt.subplots(2, len(A), figsize=(3*len(A),3*2))\n",
    "    for i in range(len(T)):\n",
    "        plt_spatial_transform(axs[:,i], Gs[i], S[i], T[i])\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-tracy",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load pretrained weights & set initial values\n",
    "if g_cls != LayerIdentity:\n",
    "    g_path = 'params/k/kx/g' if isinstance(k_cls(), CovICM) else 'params/k/g'\n",
    "    encoder_params = pytree_load({'params': pytree_leaf(params, g_path)}, cnn_save_path)\n",
    "    encoder_params_kvs = pytree_get_kvs(encoder_params)\n",
    "    params = pytree_mutate(params, {f'{g_path}/{k}': v for k,v in encoder_params_kvs.items()})\n",
    "# set initial lengthscales\n",
    "if isinstance(k_cls(), CovICM):\n",
    "    params = pytree_mutate(params, {'params/k/kx/ls': softplus_inv(np.array([2.]))})\n",
    "else:\n",
    "    params = pytree_mutate(params, {f'params/k/ks_{i}/ls': softplus_inv(np.array([2.]))\n",
    "                                    for i in range(output_dim)})\n",
    "\n",
    "kwd_notrain = ['mean_fn', 'Xu/X'] + [f'params/k/ks_{i}/kl/σ2' for i in range(output_dim)]\n",
    "kwd_trainslow = ['Xu/transform'] # 'Xu/transform'\n",
    "opt = flax_create_multioptimizer(\n",
    "    params, 'Adam',\n",
    "    [{'learning_rate': 0.}, {'learning_rate': .01}, {'learning_rate': .03}],\n",
    "    [lambda p, v: pytree_path_contains_keywords(p, kwd_notrain),\n",
    "     lambda p, v: pytree_path_contains_keywords(p, kwd_trainslow),\n",
    "     lambda p, v: not pytree_path_contains_keywords(p, kwd_notrain+kwd_trainslow)])\n",
    "\n",
    "flax_check_multiopt(params, opt)\n",
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-associate",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "        \n",
    "@jax.jit\n",
    "def eval_step2(params, X):\n",
    "    Ey, Vy = model.apply(params, X, method=model.pred_y, rngs={'lik_mc_samples': key})\n",
    "    return Ey\n",
    "\n",
    "@jax.jit\n",
    "def train_step2(step, opt, batch, key):\n",
    "    key, subkey = random.split(key)\n",
    "    X, y = batch\n",
    "    y_onehot = jax.nn.one_hot(y.squeeze(), num_classes=output_dim)\n",
    "    def loss_fn(params):\n",
    "        fx = model.apply(params,\n",
    "                         (X, y_onehot),\n",
    "                         method=model.mll,\n",
    "                         rngs={'lik_mc_samples': subkey})\n",
    "        return -fx, {}\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    params = opt.target\n",
    "    (loss, aux), grad = grad_fn(params)\n",
    "    opt = opt.apply_gradient(grad)\n",
    "    log = {'loss': loss}\n",
    "    return opt, log, key\n",
    "\n",
    "\n",
    "bsz = 64\n",
    "train_n_batches, train_batches = get_data_stream(\n",
    "    key, bsz, data_train)\n",
    "\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start = time.time()\n",
    "    for it in range(train_n_batches):\n",
    "        step = epoch*train_n_batches+it\n",
    "        batch = next(train_batches)\n",
    "        opt, log, key = train_step2(step, opt, batch, key)\n",
    "        params = opt.target\n",
    "        if step%(train_n_batches//5)==0:\n",
    "            insert_str = '/kg' if isinstance(model.k_cls().k_cls(), CovConvolutional) else ''\n",
    "            log.update({'k.ls': jax.nn.softplus(pytree_leaf(params, f'params/k/kx/ls')\n",
    "                                           if isinstance(model.k_cls(), CovICM) else \n",
    "                                           np.hstack([pytree_leaf(params, f'params/k/ks_{i}{insert_str}/ls')\n",
    "                                                      for i in range(output_dim)])),\n",
    "                   'k.σ2': jax.nn.softplus(pytree_leaf(params, f'params/k/kx/σ2')\n",
    "                                           if isinstance(model.k_cls(), CovICM) else \n",
    "                                           np.hstack([pytree_leaf(params, f'params/k/ks_{i}{insert_str}/σ2')\n",
    "                                                      for i in range(output_dim)])),\n",
    "                   'kl.ℓ': jax.nn.softplus(np.hstack([pytree_leaf(params, f'params/k/ks_{i}/kl/ls')\n",
    "                                                      for i in range(output_dim)])\n",
    "                                           if  isinstance(model.k_cls(), CovMultipleOutputIndependent) and \\\n",
    "                                               not isinstance(model.k_cls().k_cls().kl_cls(), CovConstant) else np.array([np.nan]))})\n",
    "            print(f'[{epoch:3}|{100*it/train_n_batches:5.2f}%]\\t'\n",
    "                  f'Loss={log[\"loss\"]:.3f}\\t'\n",
    "                  f'Time={time.time()-start:.3f}\\t'\n",
    "                  f'k.ls={log[\"k.ls\"][:3]}\\t'\n",
    "                  f'kl.ℓ = {log[\"kl.ℓ\"][:3]}\\t'\n",
    "                  f'k.σ2={log[\"k.σ2\"][:3]}\\t')\n",
    "            start = time.time()\n",
    "\n",
    "\n",
    "    metrics = eval_model(params, data_test, logit_fn=eval_step2)\n",
    "    print(f'[{epoch:3}] test \\t'\n",
    "          f'Loss={metrics[\"loss\"]:.3f}\\t'\n",
    "          f'accuracy={metrics[\"accuracy\"]:.3f}\\t')\n",
    "\n",
    "parmas = opt.target\n",
    "m = model.bind(params, rngs={'lik_mc_samples': key})\n",
    "\n",
    "# \n",
    "# n_inducing=50, LikMulticlassDirichlet\n",
    "# CovConvolutional(patch_inducing_loc=False) acc=.7\n",
    "# CovConvolutional(patch_inducing_loc=True, patch_shape=(3,3)) acc=.83\n",
    "# CovConvolutional(patch_inducing_loc=True, patch_shape=(7,7)) acc=.95\n",
    "\n",
    "# n_inducing=50, LikMulticlassDirichlet, patch_shape=(7, 7), patch_inducing_loc=True\n",
    "# compare using location kernel over `transl`\n",
    "# use_loc_kernel=False: center initialization of patches, acc=.93\n",
    "# use_loc_kernel=True: center initialization of patches, acc=.93\n",
    "\n",
    "# [  0| 0.00%]\tTime=26.826\tLoss=60202.617\tk.ls=[1. 1. 1.]\tkl.ℓ = [nan]\tk.σ2=[1. 1. 1.]\t\n",
    "# [  0|19.72%]\tTime=50.971\tLoss=23863.398\tk.ls=[1.601 1.362 1.369]\tkl.ℓ = [nan]\tk.σ2=[0.426 0.397 0.403]\t\n",
    "# [  0|39.44%]\tTime=2.870\tLoss=2301.921\tk.ls=[1.715 1.88  2.032]\tkl.ℓ = [nan]\tk.σ2=[0.361 0.567 0.594]\t\n",
    "# [  0|59.15%]\tTime=2.881\tLoss=-2140.403\tk.ls=[1.759 2.083 2.174]\tkl.ℓ = [nan]\tk.σ2=[0.241 1.098 1.122]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e0c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank inducing patches based on variational μ for different classes\n",
    "\n",
    "m = model.bind(params, rngs={'lik_mc_samples': key})\n",
    "Xu = m.Xu()[0] if use_loc_kernel else m.Xu()\n",
    "qm = m.q.μ - m.mean_fn.c[...,np.newaxis]\n",
    "\n",
    "M = len(Xu)\n",
    "    \n",
    "\n",
    "gridspec_kw = {'width_ratios': np.ones((output_dim,)), 'height_ratios': [3,1]}\n",
    "fig, axs = plt.subplots(2,3,figsize=(25,10))\n",
    "n_top = n_inducing\n",
    "c = 0; ind = np.argsort(qm[c,:])\n",
    "ylim = (np.min(qm), np.max(qm))\n",
    "\n",
    "for c in range(output_dim):\n",
    "    ind = np.argsort(qm[c,:])[::-1]\n",
    "    # variational μ\n",
    "    ax = axs[0,c]\n",
    "    for co in range(output_dim):\n",
    "        ls = '--' if co != c else '-'\n",
    "        ax.plot(np.arange(n_inducing), qm[co,ind], ls, label=f'{digits[co]}')\n",
    "    ax.set_title(f'qμ ({digits[c]})', fontsize=35)\n",
    "    ax.grid()\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.legend(fontsize=20)\n",
    "    \n",
    "    # top weighted patches\n",
    "    ax = axs[1,c]\n",
    "    ims = Xu[ind].reshape((-1,*patch_shape))[:n_top]\n",
    "    grid = make_im_grid(ims, im_per_row=min(len(ims), 10))\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.imshow(grid, cmap='Greys', vmin=0, vmax=1)\n",
    "    \n",
    "fig.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c334fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank inducing patches based on average variational μ\n",
    "#     goal is to see if there are patches not used as evidence for any class \n",
    "#     sometime its patches where all classes have ... so cannot be used as discriminating features \n",
    "#\n",
    "\n",
    "Xu = m.Xu()[0] if use_loc_kernel else m.Xu()\n",
    "qm = pytree_leaf(params, 'params/q/μ')\n",
    "μconst = pytree_leaf(params, 'params/mean_fn/c')[...,np.newaxis]\n",
    "qm_mag = np.mean(np.abs(qm - μconst), axis=0)\n",
    "ind = np.argsort(qm_mag)[::-1]\n",
    "\n",
    "fig, axs = plt.subplots(2,1,figsize=(8,10))\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(np.arange(n_inducing), qm_mag[ind], '-')\n",
    "ax.set_title(f'qμ magnitude', fontsize=35)\n",
    "ax.grid()\n",
    "ax.set_ylim((-.5, np.max(qm_mag)+.5))\n",
    "fig.tight_layout()\n",
    "\n",
    "ax = axs[1]\n",
    "ims = Xu[ind].reshape((-1,*patch_shape))\n",
    "grid = make_im_grid(ims, im_per_row=min(len(ims), 10))\n",
    "ax.set_xticks([]); ax.set_yticks([])\n",
    "ax.imshow(grid, cmap='Greys', vmin=0, vmax=1)\n",
    "\n",
    "\n",
    "## plot spatial transform as well\n",
    "\n",
    "fn = vmap(transform_to_matrix, (0, None, None), 0)\n",
    "A = fn(pytree_leaf(params, 'params/Xu/transform/T'),\n",
    "       transform_cls().T_type,\n",
    "       transform_cls().A_init_val)\n",
    "S = pytree_leaf(params, 'params/Xu/X')\n",
    "A = A[ind]; S = S[ind]\n",
    "\n",
    "fn = vmap(spatial_transform_details, (0, 0, None), 0)\n",
    "T, Gs = fn(A, S, patch_shape)\n",
    "fig, axs = plt.subplots(2, len(A), figsize=(3*len(A),3*2))\n",
    "for i in range(len(T)):\n",
    "    plt_spatial_transform(axs[:,i], Gs[i], S[i], T[i])\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a749c39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank inducing patches based on variational μ for different classes\n",
    "\n",
    "m = model.bind(params)\n",
    "Xu = m.Xu()\n",
    "qm = m.q.μ - m.mean_fn.c[...,np.newaxis]\n",
    "\n",
    "M = len(Xu)\n",
    "    \n",
    "\n",
    "gridspec_kw = {'width_ratios': np.ones((output_dim,)), 'height_ratios': [3,1]}\n",
    "fig, axs = plt.subplots(2,3,figsize=(25,10))\n",
    "n_top = n_inducing\n",
    "c = 0; ind = np.argsort(qm[c,:])\n",
    "ylim = (np.min(qm), np.max(qm))\n",
    "\n",
    "for c in range(output_dim):\n",
    "    ind = np.argsort(qm[c,:])[::-1]\n",
    "    # variational μ\n",
    "    ax = axs[0,c]\n",
    "    for co in range(output_dim):\n",
    "        ls = '--' if co != c else '-'\n",
    "        ax.plot(np.arange(n_inducing), qm[co,ind], ls, label=f'{digits[co]}')\n",
    "    ax.set_title(f'qμ ({digits[c]})', fontsize=35)\n",
    "    ax.grid()\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.legend(fontsize=20)\n",
    "    \n",
    "    # top weighted patches\n",
    "    ax = axs[1,c]\n",
    "    ims = Xu[ind].reshape((-1,*patch_shape))[:n_top]\n",
    "    grid = make_im_grid(ims, im_per_row=min(len(ims), 10))\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.imshow(grid, cmap='Greys', vmin=0, vmax=1)\n",
    "    \n",
    "fig.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4955c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank inducing patches based on average variational μ\n",
    "#     goal is to see if there are patches not used as evidence for any class \n",
    "#     sometime its patches where all classes have ... so cannot be used as discriminating features \n",
    "#\n",
    "\n",
    "Xu = inducing_loc_cls().apply({'params': pytree_leaf(params, 'params/Xu')})\n",
    "qm = pytree_leaf(params, 'params/q/μ')\n",
    "μconst = pytree_leaf(params, 'params/mean_fn/c')[...,np.newaxis]\n",
    "qm_mag = np.mean(np.abs(qm - μconst), axis=0)\n",
    "ind = np.argsort(qm_mag)[::-1]\n",
    "\n",
    "fig, axs = plt.subplots(2,1,figsize=(8,10))\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(np.arange(n_inducing), qm_mag[ind], '-')\n",
    "ax.set_title(f'qμ magnitude', fontsize=35)\n",
    "ax.grid()\n",
    "ax.set_ylim((-.5, np.max(qm_mag)+.5))\n",
    "fig.tight_layout()\n",
    "\n",
    "ax = axs[1]\n",
    "ims = Xu[ind].reshape((-1,*patch_shape))\n",
    "grid = make_im_grid(ims, im_per_row=min(len(ims), 10))\n",
    "ax.set_xticks([]); ax.set_yticks([])\n",
    "ax.imshow(grid, cmap='Greys', vmin=0, vmax=1)\n",
    "\n",
    "\n",
    "## plot spatial transform as well\n",
    "\n",
    "fn = vmap(transform_to_matrix, (0, None, None), 0)\n",
    "A = fn(pytree_leaf(params, 'params/Xu/transform/T'),\n",
    "       transform_cls().T_type,\n",
    "       transform_cls().A_init_val)\n",
    "S = pytree_leaf(params, 'params/Xu/X')\n",
    "A = A[ind]; S = S[ind]\n",
    "\n",
    "fn = vmap(spatial_transform_details, (0, 0, None), 0)\n",
    "T, Gs = fn(A, S, patch_shape)\n",
    "fig, axs = plt.subplots(2, len(A), figsize=(3*len(A),3*2))\n",
    "for i in range(len(T)):\n",
    "    plt_spatial_transform(axs[:,i], Gs[i], S[i], T[i])\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-playing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## \n",
    "print(np.where(y_test==0)[0][:10])\n",
    "\n",
    "ind_digit0 = np.where(y_test==0)[0]\n",
    "ind_digit1 = np.where(y_test==1)[0]\n",
    "\n",
    "ind = 1\n",
    "n_ims = 20\n",
    "\n",
    "x_rot = rotated_ims(X_test[ind], n_ims=n_ims)\n",
    "# X = np.stack(X_test[ind_digit0[:10]])\n",
    "# X = np.stack(X_test[np.hstack((ind_digit0[:n_ims//2], ind_digit1[:n_ims//2]))])\n",
    "X = x_rot\n",
    "μf, σ2f = model.apply(params, X, full_cov=False, method=model.pred_f); μf = μf.squeeze()\n",
    "y_pred = np.array(digits)[np.argmax(μf,axis=-1)]\n",
    "α = gamma_to_lognormal_inv(μf, σ2f, approx_type='kl')\n",
    "\n",
    "if isinstance(model.lik_cls(), LikMulticlassSoftmax):\n",
    "    lik_test = LikMulticlassSoftmax(output_dim=output_dim, n_mc_samples=5000)\n",
    "    p_mc, Vp_mc = lik_test.apply({}, μf, σ2f, rngs={'lik_mc_samples': key}, method=lik_test.predictive_dist)\n",
    "elif isinstance(model.lik_cls(), LikMulticlassDirichlet):\n",
    "    lik_test = LikMulticlassDirichlet(output_dim=output_dim, n_mc_samples=5000)\n",
    "    p_mc, Vp_mc = lik_test.apply({}, μf, σ2f, rngs={'lik_mc_samples': key}, method=lik_test.predictive_dist)\n",
    "    α = gamma_to_lognormal_inv(μf, σ2f, approx_type='kl')\n",
    "    α0 = np.sum(α, axis=-1, keepdims=True)\n",
    "    p = α / α0\n",
    "    Vp = p*(1-p)\n",
    "\n",
    "gridspec_kw = {'width_ratios': [1],\n",
    "               'height_ratios': [4, 4, 4, 1]}\n",
    "fig, axs = plt.subplots(4, 1, gridspec_kw=gridspec_kw, figsize=(15, 15))\n",
    "cmap = plt.cm.get_cmap('Set1')\n",
    "colors = [cmap(0), cmap(1), cmap(2)]\n",
    "\n",
    "\n",
    "ax = axs[0]\n",
    "ax.set_xticks([])\n",
    "for i, d in enumerate(digits):\n",
    "    c = colors[i]\n",
    "    μ, std = μf[:,i], np.sqrt(σ2f[:,i])\n",
    "    ax.plot(np.arange(len(X)), μf[:,i], lw=2, color=c, label=d)\n",
    "    ax.plot(np.arange(len(X)), μ + 2*std, '--', c=c)\n",
    "    ax.plot(np.arange(len(X)), μ - 2*std, '--', c=c)\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "ax.set_title('predictive posterior p(f*|X)')\n",
    "\n",
    "\n",
    "ax = axs[1]\n",
    "ax.set_xticks([])\n",
    "for i, d in enumerate(digits):\n",
    "    c = colors[i]\n",
    "    ax.plot(np.arange(len(X)), α[:,i], lw=2, color=c, label=d)\n",
    "ax.grid()\n",
    "ax.set_yticks(np.linspace(0, np.floor(np.max(α)*1.1), 5))\n",
    "ax.set_title('α')\n",
    "\n",
    "\n",
    "ax = axs[2]\n",
    "ax.set_xticks([])\n",
    "for i, d in enumerate(digits):\n",
    "    c = colors[i]\n",
    "    if isinstance(model.lik_cls(), LikMulticlassSoftmax) or \\\n",
    "        isinstance(model.lik_cls(), LikMulticlassDirichlet):\n",
    "        ax.plot(np.arange(len(X)), p[:,i], '-', c=c, lw=2)\n",
    "        ax.plot(np.arange(len(X)), p[:,i] + 2*np.sqrt(Vp[:,i]), '--', c=c)\n",
    "        ax.plot(np.arange(len(X)), p[:,i] - 2*np.sqrt(Vp[:,i]), '--', c=c)\n",
    "    else:\n",
    "        ax.plot(np.arange(len(X)), μf[:,i], c=c, lw=2)\n",
    "ax.set_title('E[p(y|f*)]')\n",
    "ax.grid()\n",
    "\n",
    "    \n",
    "ax = axs[3]\n",
    "ax.set_xticks([]); ax.set_yticks([])\n",
    "ax.imshow(np.hstack([x for x in X]), cmap='Greys')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-baghdad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mvn_marginal_variational_details(Kff, Kuf, mf,\n",
    "                             Luu, mu, μq, Lq, full_cov=False):\n",
    "    α = solve_triangular(Luu, Kuf, lower=True)\n",
    "    β = solve_triangular(Luu.T, α, lower=False)\n",
    "    γ = Lq.T@β\n",
    "    if full_cov:\n",
    "        Σf = Kff - α.T@α + γ.T@γ\n",
    "    else:\n",
    "        Σf = Kff - \\\n",
    "            np.sum(np.square(α), axis=0) + \\\n",
    "            np.sum(np.square(γ), axis=0)\n",
    "    # for multiple-output\n",
    "    μq = μq.reshape(-1, 1)\n",
    "    mu = mu.reshape(-1, 1)\n",
    "    mf = mf.reshape(-1, 1)\n",
    "    A = β.T; δ = (μq-mu)\n",
    "    μf = mf + A@δ\n",
    "    return μf, Σf, A, δ, mf\n",
    "\n",
    "def pred_f_details(self, Xs, full_cov=True):\n",
    "    k = self.k\n",
    "    Xu = self.Xu()               # (M,...)\n",
    "    μq, Lq = self.q.μ, self.q.L  # (P, M) & (P, M, M)\n",
    "    if μq.shape[0] == 1:\n",
    "        μq, Lq = μq.squeeze(0), Lq.squeeze(0)\n",
    "\n",
    "    ms = self.mean_fn(Xs)\n",
    "    mu = self.mean_fn(Xu)\n",
    "\n",
    "    Kss = k.Kff(Xs, full_cov=full_cov)\n",
    "    Kus = k.Kuf(Xu, Xs)\n",
    "    Kuu = k.Kuu(Xu)\n",
    "    Luu = cholesky_jitter_vmap(Kuu, jitter=5e-5)  # (P, M, M)\n",
    "\n",
    "    if isinstance(k, CovMultipleOutputIndependent):\n",
    "        mvn_marginal_variational_fn = vmap(\n",
    "            mvn_marginal_variational_details, (0, 0, 1, 0, 1, 0, 0, None), -1)  # along P-dim\n",
    "    else:\n",
    "        mvn_marginal_variational_fn = mvn_marginal_variational_details\n",
    "\n",
    "    μf, Σf, A, δ, mf = mvn_marginal_variational_fn(Kss, Kus, ms,\n",
    "                                         Luu, mu, μq, Lq, full_cov)\n",
    "    # (N, D), (N, D), (N, M, D), (M, D), (N, D)\n",
    "    N, D = Σf.shape; M = len(Xu)\n",
    "    if not full_cov:\n",
    "        μf = μf.reshape((N,D))\n",
    "        A = A.reshape((N,M,D))\n",
    "        δ = δ.reshape((M,D))\n",
    "        mf = mf.reshape((N,D))\n",
    "    return μf, Σf, A, δ, mf\n",
    "\n",
    "x = X_test[:3]\n",
    "μ, σ2, A, δ, mf = model.apply(params, x, full_cov=False, method=pred_f_details)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e79b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cmap = plt.cm.get_cmap('Set1')\n",
    "colors = [cmap(0), cmap(1), cmap(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M, D = A.shape  # (#test, #inducing, #classes)\n",
    "fig, axs = plt.subplots(2,N,figsize=(30,15), sharey=True)\n",
    "[axi.set_xticks([]) for axi in axs.ravel()];\n",
    "\n",
    "for n in range(N):\n",
    "    ax = axs[0,n]\n",
    "    for i, d in enumerate(digits):\n",
    "        ax.plot(A[n,:,i], np.arange(M), '--', c=colors[i], label=f'{d}')\n",
    "        \n",
    "\n",
    "for n in range(N):\n",
    "    ax = axs[1,n]\n",
    "    for i, d in enumerate(digits):\n",
    "        ax.plot(A[n,:,i]*δ[:,i], np.arange(M), '--', c=colors[i], label=f'{d}')\n",
    "        \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-story",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "Aδ = (A*δ)\n",
    "\n",
    "ind = np.argsort(np.abs(Aδ[n,:,0]))[::-1][:5]\n",
    "print(list(zip(ind, Aδ[n,ind,0]))[:5])\n",
    "\n",
    "Xu = model.inducing_loc_cls().apply({'params': params['params']['Xu']}, rngs=rngs)\n",
    "\n",
    "fig, axs = plt.subplots(len(digits),1+len(ind),figsize=(5*(len(ind)+1),10), sharey=True)\n",
    "[axi.set_xticks([]) for axi in axs.ravel()]; [axi.set_yticks([]) for axi in axs.ravel()]\n",
    "    \n",
    "for di in range(len(digits)):\n",
    "    # original image\n",
    "    ax = axs[di,0]\n",
    "    ax.imshow(X[n], cmap='Greys', vmin=0, vmax=1)\n",
    "    ax.set_xlabel(f'{np.sum(Aδ[n,:,di]):.2f}', fontsize=40, color='r')\n",
    "    ax.set_ylabel(f'C={digits[di]}', fontsize=40)\n",
    "    \n",
    "    # take top evidence\n",
    "    ind = np.argsort(np.abs(Aδ[n,:,di]))[::-1][:5]\n",
    "    for i in range(len(ind)):\n",
    "        ax = axs[di,i+1]\n",
    "        ax.imshow(Xu[ind[i]], cmap='Greys', vmin=0, vmax=1)\n",
    "        ax.set_xlabel(f'{Aδ[n,ind[i],di]:.2f}', fontsize=40)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-breath",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:misc_impl]",
   "language": "python",
   "name": "conda-env-misc_impl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
