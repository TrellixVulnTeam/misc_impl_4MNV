{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a2488e5",
   "metadata": {},
   "source": [
    "goal\n",
    "- [done] SVGP + dirichlet training on mnist\n",
    "    - recreate evidential DL example ... \n",
    "- [done] variational learning of supporting image patches !\n",
    "    - [done] impl STN ... \n",
    "    - observations\n",
    "        - perf okay if allows finetune encoder network \n",
    "        - not so much as evidence that is localized ... \n",
    "            - perhaps due to fact shared inducing locations ... \\\n",
    "                would want to retain all info (not localized) and \\\n",
    "                use variational mean to modulate evidence for class\n",
    "            - so to get localized info ... might want to do per-class inducing variables\n",
    "            - also might want to put STN to kernel hyperparam ...\\\n",
    "                and put product kernel over both image and affine trans matrix \n",
    "            - might also try just using one STN for entire thing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "715c3c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax/jaxlib:\t 0.2.14 (0, 1, 66)\n",
      "platform:\t gpu\n",
      "gpu counts:\t 1\n",
      "devices:\t [GpuDevice(id=0, process_index=0)]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import time\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import scipy\n",
    "import numpy as onp\n",
    "onp.set_printoptions(precision=3,suppress=True)\n",
    "\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "from jax import grad, jit, vmap, device_put, random\n",
    "from flax import linen as nn\n",
    "from jax.scipy.stats import dirichlet\n",
    "import optax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.tri as tri\n",
    "import matplotlib.patches as mpl_patches\n",
    "\n",
    "# https://matplotlib.org/3.1.1/gallery/style_sheets/style_sheets_reference.html\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 25\n",
    "mpl.rcParams['font.family'] = 'DejaVu Sans'\n",
    "mpl.rcParams['axes.linewidth'] = 3\n",
    "cmap = plt.cm.get_cmap('bwr')\n",
    "\n",
    "from tabulate import tabulate\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "from setup_convgp import *\n",
    "from plt_utils import *\n",
    "from gpax import *\n",
    "from dataset import *\n",
    "\n",
    "jax_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78acdceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) (60000, 1)\n",
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(1)\n",
    "# Y_subset = [0,1]\n",
    "Y_subset = [0,1,2,3,4,5,6,7,8,9]\n",
    "dataset_name = 'cifar10'\n",
    "X_train, y_train, X_test, y_test = load_mnist() # load_cifar10() # load_mnist() \n",
    "X_train, y_train = XY_subset(X_train, y_train, Y_subset)\n",
    "X_test, y_test = XY_subset(X_test, y_test, Y_subset)\n",
    "\n",
    "X_train = jax_to_gpu(np.asarray(X_train))\n",
    "y_train = jax_to_gpu(np.asarray(y_train))\n",
    "X_test = jax_to_gpu(np.asarray(X_test))\n",
    "y_test = jax_to_gpu(np.asarray(y_test))\n",
    "\n",
    "data_train = (X_train, y_train)\n",
    "data_test = (X_test, y_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(np.min(X_train), np.max(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "381def97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '[0]')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 1\n",
    "plt.imshow(X_train[n])\n",
    "plt.title(y_train[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-syntax",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.where(y_test==0)[0][:10])\n",
    "\n",
    "ind = 1\n",
    "n_ims = 20\n",
    "\n",
    "fig, axs = plt.subplots(1,n_ims,figsize=(2*n_ims,2))\n",
    "[axi.set_xticks([]) for axi in axs.ravel()]; [axi.set_yticks([]) for axi in axs.ravel()]\n",
    "\n",
    "x, y = X_test[ind], y_test[ind]\n",
    "\n",
    "x_rot = rotated_ims(x, n_ims=n_ims)\n",
    "for i in range(n_ims):\n",
    "    ax = axs[i]\n",
    "    ax.imshow(x_rot[i], cmap='Greys')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e796f54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batch_size: 64\n",
       "image_shape: !!python/tuple\n",
       "- 28\n",
       "- 28\n",
       "- 1\n",
       "learning_rate: 0.03\n",
       "n_epochs: 30"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ml_collections \n",
    "\n",
    "def get_config_mnist_supervised():\n",
    "    \n",
    "    config = ml_collections.ConfigDict()\n",
    "    \n",
    "    config.image_shape = (28, 28, 1)\n",
    "    \n",
    "    # optimization \n",
    "    config.learning_rate = .03\n",
    "    config.batch_size = 64\n",
    "    config.n_epochs = 30\n",
    "    \n",
    "    \n",
    "    return config\n",
    "\n",
    "\n",
    "config = get_config_mnist_supervised()\n",
    "config\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e7fe49ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0| 0.00%]\tLoss=2.289\taccuracy=0.156\t\n",
      "[  0| 9.91%]\tLoss=1.345\taccuracy=0.619\t\n",
      "[  0|19.83%]\tLoss=0.947\taccuracy=0.723\t\n",
      "[  0|29.74%]\tLoss=0.768\taccuracy=0.773\t\n",
      "[  0|39.66%]\tLoss=0.673\taccuracy=0.800\t\n",
      "[  0|49.57%]\tLoss=0.603\taccuracy=0.820\t\n",
      "[  0|59.49%]\tLoss=0.554\taccuracy=0.835\t\n",
      "[  0|69.40%]\tLoss=0.514\taccuracy=0.847\t\n",
      "[  0|79.32%]\tLoss=0.484\taccuracy=0.855\t\n",
      "[  0|89.23%]\tLoss=0.458\taccuracy=0.863\t\n",
      "[  0|99.15%]\tLoss=0.436\taccuracy=0.869\t\n",
      "[  0] test \tLoss=0.218\taccuracy=0.933\t\n",
      "[  1| 9.06%]\tLoss=0.220\taccuracy=0.931\t\n",
      "[  1|18.98%]\tLoss=0.210\taccuracy=0.935\t\n",
      "[  1|28.89%]\tLoss=0.208\taccuracy=0.937\t\n",
      "[  1|38.81%]\tLoss=0.202\taccuracy=0.939\t\n",
      "[  1|48.72%]\tLoss=0.197\taccuracy=0.940\t\n",
      "[  1|58.64%]\tLoss=0.192\taccuracy=0.942\t\n",
      "[  1|68.55%]\tLoss=0.190\taccuracy=0.943\t\n",
      "[  1|78.46%]\tLoss=0.186\taccuracy=0.944\t\n",
      "[  1|88.38%]\tLoss=0.183\taccuracy=0.945\t\n",
      "[  1|98.29%]\tLoss=0.180\taccuracy=0.946\t\n",
      "[  1] test \tLoss=0.236\taccuracy=0.922\t\n",
      "[  2| 8.21%]\tLoss=0.154\taccuracy=0.956\t\n",
      "[  2|18.12%]\tLoss=0.155\taccuracy=0.955\t\n",
      "[  2|28.04%]\tLoss=0.146\taccuracy=0.956\t\n",
      "[  2|37.95%]\tLoss=0.142\taccuracy=0.957\t\n",
      "[  2|47.87%]\tLoss=0.139\taccuracy=0.959\t\n",
      "[  2|57.78%]\tLoss=0.137\taccuracy=0.959\t\n",
      "[  2|67.70%]\tLoss=0.134\taccuracy=0.960\t\n",
      "[  2|77.61%]\tLoss=0.132\taccuracy=0.960\t\n",
      "[  2|87.53%]\tLoss=0.130\taccuracy=0.961\t\n",
      "[  2|97.44%]\tLoss=0.129\taccuracy=0.961\t\n",
      "[  2] test \tLoss=0.106\taccuracy=0.969\t\n",
      "[  3| 7.36%]\tLoss=0.094\taccuracy=0.972\t\n",
      "[  3|17.27%]\tLoss=0.106\taccuracy=0.967\t\n",
      "[  3|27.19%]\tLoss=0.105\taccuracy=0.968\t\n",
      "[  3|37.10%]\tLoss=0.105\taccuracy=0.968\t\n",
      "[  3|47.01%]\tLoss=0.101\taccuracy=0.969\t\n",
      "[  3|56.93%]\tLoss=0.101\taccuracy=0.969\t\n",
      "[  3|66.84%]\tLoss=0.102\taccuracy=0.969\t\n",
      "[  3|76.76%]\tLoss=0.101\taccuracy=0.970\t\n",
      "[  3|86.67%]\tLoss=0.101\taccuracy=0.970\t\n",
      "[  3|96.59%]\tLoss=0.101\taccuracy=0.970\t\n",
      "[  3] test \tLoss=0.117\taccuracy=0.962\t\n",
      "[  4| 6.50%]\tLoss=0.083\taccuracy=0.973\t\n",
      "[  4|16.42%]\tLoss=0.088\taccuracy=0.972\t\n",
      "[  4|26.33%]\tLoss=0.091\taccuracy=0.971\t\n",
      "[  4|36.25%]\tLoss=0.091\taccuracy=0.972\t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-203-66bb102f386c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_n_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrain_n_batches\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/vision/polina/scratch/wpq/github/misc_impl/gp/gpax.py\u001b[0m in \u001b[0;36mdata_stream\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m   2555\u001b[0m             \u001b[0mperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2556\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2557\u001b[0;31m                 \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2558\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2559\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/vision/polina/shared_software/miniconda3/envs/misc_impl/lib/python3.7/site-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_rewriting_take\u001b[0;34m(arr, idx)\u001b[0m\n\u001b[1;32m   4957\u001b[0m   \u001b[0;31m# All supported cases of indexing can be implemented as an XLA gather,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4958\u001b[0m   \u001b[0;31m# followed by an optional reverse and broadcast_in_dim.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4959\u001b[0;31m   \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4960\u001b[0m   \u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_split_index_for_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4961\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/vision/polina/shared_software/miniconda3/envs/misc_impl/lib/python3.7/site-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m   3033\u001b[0m   \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_user_dtype_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"asarray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3034\u001b[0m   \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonicalize_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3035\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/vision/polina/shared_software/miniconda3/envs/misc_impl/lib/python3.7/site-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin)\u001b[0m\n\u001b[1;32m   3018\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unexpected input type for array: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3020\u001b[0;31m   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_element_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweak_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3022\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mndmin\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/vision/polina/shared_software/miniconda3/envs/misc_impl/lib/python3.7/site-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_convert_element_type\u001b[0;34m(operand, new_dtype, weak_type)\u001b[0m\n\u001b[1;32m    429\u001b[0m   \u001b[0;31m# Don't canonicalize old_dtype because x64 context might cause\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m   \u001b[0;31m# un-canonicalized operands to be passed in.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m   \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m   \u001b[0mold_weak_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_weakly_typed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mresult_type\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from flax.training import train_state\n",
    "import functools\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    \"\"\" Keeps track of parameters, optimizer state, rng, \n",
    "            mutable states in training \n",
    "    \"\"\"\n",
    "    # `TrainState` attributes: \n",
    "    # step: int\n",
    "    # apply_fn: Callable = struct.field(pytree_node=False)\n",
    "    # params: core.FrozenDict[str, Any]\n",
    "    # tx: optax.GradientTransformation = struct.field(pytree_node=False)\n",
    "    # opt_state: optax.OptState\n",
    "    #\n",
    "    batch_stats: Any\n",
    "    rng: Any\n",
    "        \n",
    "    # jitted model's apply_fn for evaluation, use\n",
    "    #     `struct.field(pytree_node=False)` to avoid type checks\n",
    "    apply_fn_eval_jitted: Callable = struct.field(pytree_node=False,\n",
    "                                                  default=None)\n",
    "        \n",
    "    @property\n",
    "    def variables(self):\n",
    "        return freeze({'params': self.params,\n",
    "                       'batch_stats': self.batch_stats})\n",
    "\n",
    "    @property\n",
    "    def apply_fn_eval(self):\n",
    "        \"\"\" Note separate calls to this function \n",
    "                redo the jitting ... so is expensive\n",
    "            If state unchanged, then better write \n",
    "                ```\n",
    "                apply_fn_eval = state.get_apply_fn_eval()\n",
    "                [apply_fn_eval(X) for X in dataset]\n",
    "                ```\n",
    "        \"\"\"\n",
    "        if self.apply_fn_eval_jitted is None:\n",
    "            import inspect\n",
    "            apply_fn = self.apply_fn\n",
    "            sig = inspect.signature(self.apply_fn)\n",
    "            kwargs = {'train': False} \\\n",
    "                if 'train' in sig.parameters else {}\n",
    "            apply_fn = partial(apply_fn, **kwargs)\n",
    "            apply_fn = jax.jit(apply_fn)\n",
    "            # Use `object.__setattr__` to mutate \n",
    "            #     fields in @dataclass(frozen=True)\n",
    "            object.__setattr__(\n",
    "                self, 'apply_fn_eval_jitted', apply_fn)\n",
    "        return self.apply_fn_eval_jitted\n",
    "\n",
    "        \n",
    "@jax.jit\n",
    "def compute_metrics(logits, labels):\n",
    "    label_onehot = jax.nn.one_hot(labels.squeeze(), 10)\n",
    "    loss = np.mean(optax.softmax_cross_entropy(logits=logits,\n",
    "                                               labels=label_onehot))\n",
    "    pred = np.argmax(logits, -1).reshape(labels.shape)\n",
    "    accuracy = np.mean(pred == labels)\n",
    "    metrics = {'loss': loss, 'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    state_rng, train_rng = random.split(state.rng)\n",
    "    X, y = batch\n",
    "    def loss_fn(params):\n",
    "        y_onehot = jax.nn.one_hot(y, 10).squeeze()\n",
    "        logits, mutable_state = state.apply_fn(\n",
    "            {'params': params, 'batch_stats': state.batch_stats},\n",
    "            X, mutable=['batch_stats'])\n",
    "        # mutable_state: {'batch_stats': {...}}\n",
    "        loss = np.mean(optax.softmax_cross_entropy(logits=logits,\n",
    "                                                   labels=y_onehot))\n",
    "        aux = (logits, mutable_state)\n",
    "        return loss, aux\n",
    "    grad_fn = jax.value_and_grad(loss_fn,\n",
    "                                 has_aux=True)\n",
    "    (loss, aux), grads = grad_fn(state.params)\n",
    "    logits, mutable_state = aux\n",
    "    # `state.apply_gradients(grads)` does 3 things \n",
    "    #     - compute updates given gradient transformation `state.tx` & `grads`\n",
    "    #     - apply the updates using `optax.apply_updates`\n",
    "    #     - update `step`, `params`, `opt_state`, etc. attached to `state`\n",
    "    state = state.apply_gradients(\n",
    "        grads=grads,\n",
    "        batch_stats=mutable_state['batch_stats'],\n",
    "        rng=state_rng)\n",
    "    metrics = compute_metrics(logits, y)\n",
    "    log = {'loss': loss,\n",
    "           'accuracy': metrics['accuracy']}\n",
    "    return state, log\n",
    "\n",
    "\n",
    "\n",
    "def eval_model(state, data_test):\n",
    "    test_n_batches, test_batches = get_data_stream(\n",
    "        random.PRNGKey(0), 100, data_test)\n",
    "\n",
    "    logits = []\n",
    "    labels = []\n",
    "    for _ in range(test_n_batches):\n",
    "        batch = next(test_batches)\n",
    "        X, y = batch\n",
    "        logit = state.apply_fn_eval(state.variables,\n",
    "                                    X)\n",
    "        labels.append(y.reshape(-1, 1))\n",
    "        logits.append(logit)\n",
    "\n",
    "    logits = np.vstack(logits)\n",
    "    labels = np.vstack(labels)\n",
    "    metrics = compute_metrics(logits, labels)\n",
    "    metrics = jax.tree_map(lambda x: x.item(),\n",
    "                           metrics)\n",
    "    return metrics\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = CNNMnist(output_dim=10)\n",
    "\n",
    "x_init = np.ones((1, config.image_shape[0], config.image_shape[1], 1))\n",
    "variables = model.init(key, x_init)\n",
    "\n",
    "tx = optax.sgd(config.learning_rate,\n",
    "               nesterov=True)\n",
    "\n",
    "key, state_key = random.split(key)\n",
    "state = TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=variables['params'],\n",
    "    tx=tx,\n",
    "    batch_stats=variables['batch_stats'] if \\\n",
    "        'batch_stats' in variables else {},\n",
    "    rng=state_key)\n",
    "\n",
    "train_n_batches, train_batches = get_data_stream(\n",
    "    key, config.batch_size, data_train)\n",
    "\n",
    "for epoch in range(config.n_epochs):\n",
    "    logs = defaultdict(list)\n",
    "    for it in range(train_n_batches):\n",
    "        step = epoch*train_n_batches+it\n",
    "        batch = next(train_batches)\n",
    "        state, log = train_step(state, batch)\n",
    "        variables = state.variables\n",
    "        \n",
    "        for k, v in log.items():\n",
    "            logs[k].append(v)\n",
    "        if step%(train_n_batches//10)==0:\n",
    "            avg_metrics = {k: np.mean(np.array(v))\n",
    "                           for k, v in logs.items()}\n",
    "            print(f'[{epoch:3}|{100*it/train_n_batches:5.2f}%]\\t'\n",
    "                  f'Loss={avg_metrics[\"loss\"]:.3f}\\t'\n",
    "                  f'accuracy={avg_metrics[\"accuracy\"]:.3f}\\t')\n",
    "            \n",
    "    \n",
    "    metrics = eval_model(state, data_test)\n",
    "    print(f'[{epoch:3}] test \\t'\n",
    "          f'Loss={metrics[\"loss\"]:.3f}\\t'\n",
    "          f'accuracy={metrics[\"accuracy\"]:.3f}\\t')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_dim = len(Y_subset)\n",
    "\n",
    "    \n",
    "def model_def():\n",
    "    # return CNNMnist(output_dim=output_dim)\n",
    "    return CNNMnist(output_dim=output_dim)\n",
    "\n",
    "\n",
    "def compute_metrics(logits, labels):\n",
    "    loss = np.mean(optax.softmax_cross_entropy(\n",
    "        logits=logits, labels=jax.nn.one_hot(labels.squeeze(), output_dim)))\n",
    "    pred = np.argmax(logits, -1).reshape(labels.shape)\n",
    "    accuracy = np.mean(pred == labels)\n",
    "    metrics = {'loss': loss, 'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def logit_fn(params, X):\n",
    "    logits = model_def().apply(params, X)\n",
    "    return logits\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(opt, batch, key):\n",
    "    key, subkey = random.split(key)\n",
    "    X, y = batch\n",
    "    y_onehot = jax.nn.one_hot(y, output_dim).squeeze()\n",
    "    def loss_fn(params):\n",
    "        logits = model_def().apply(params, X)\n",
    "        loss = np.mean(optax.softmax_cross_entropy(\n",
    "            logits=logits, labels=y_onehot))\n",
    "        return loss, logits\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grad = grad_fn(opt.target)\n",
    "    opt = opt.apply_gradient(grad)\n",
    "    metrics = compute_metrics(logits, y)\n",
    "    log = {'loss': loss,\n",
    "           'accuracy': metrics['accuracy']}\n",
    "    return opt, log, key\n",
    "\n",
    "\n",
    "def eval_model(params, data_test, logit_fn=logit_fn):\n",
    "    test_n_batches, test_batches = get_data_stream(\n",
    "        random.PRNGKey(0), 100, data_test)\n",
    "\n",
    "    logits = []; labels = []\n",
    "    for _ in range(test_n_batches):\n",
    "        batch = next(test_batches)\n",
    "        X, y = batch\n",
    "        logit = logit_fn(params, X)\n",
    "        labels.append(y.reshape(-1, 1))\n",
    "        logits.append(logit)\n",
    "\n",
    "    logits = np.vstack(logits)\n",
    "    labels = np.vstack(labels)\n",
    "    metrics = compute_metrics(logits, labels)\n",
    "    metrics = jax.tree_map(lambda x: x.item(), metrics)\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-selection",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = model_def()\n",
    "params = model.init(key, X_train[:1])\n",
    "opt = flax_create_optimizer(params, 'Adam', {'learning_rate': .03})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-malawi",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bsz = 64\n",
    "train_n_batches, train_batches = get_data_stream(key, bsz, data_train)\n",
    "n_epochs = 30\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    logs = defaultdict(list)\n",
    "    for it in range(train_n_batches):\n",
    "        step = epoch*train_n_batches+it\n",
    "        batch = next(train_batches)\n",
    "        opt, log, key = train_step(opt, batch, key)\n",
    "        params = opt.target\n",
    "        for k, v in log.items():\n",
    "            logs[k].append(v)\n",
    "        if step%(train_n_batches//10)==0:\n",
    "            avg_metrics = {k: np.mean(np.array(v))\n",
    "                           for k, v in logs.items()}\n",
    "            print(f'[{epoch:3}|{100*it/train_n_batches:5.2f}%]\\t'\n",
    "                  f'Loss={avg_metrics[\"loss\"]:.3f}\\t'\n",
    "                  f'accuracy={avg_metrics[\"accuracy\"]:.3f}\\t')\n",
    "    \n",
    "    metrics = eval_model(params, data_test)\n",
    "    print(f'[{epoch:3}] test \\t'\n",
    "          f'Loss={metrics[\"loss\"]:.3f}\\t'\n",
    "          f'accuracy={metrics[\"accuracy\"]:.3f}\\t')\n",
    "    \n",
    "# 10 digits: acc=.973\n",
    "# cifar10: (epoch=40) acc=0.681\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-constraint",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "cnn_save_path = f'./{dataset_name}_cnn_params_{\",\".join([str(x) for x in Y_subset])}.pkl'\n",
    "pytree_save(opt.target, cnn_save_path)\n",
    "params = pytree_load(model_def().init(key, X_train[:1]), cnn_save_path)\n",
    "\n",
    "metrics = eval_model(params, data_test)\n",
    "print(f'Loss={metrics[\"loss\"]:.3f}\\t'\n",
    "      f'accuracy={metrics[\"accuracy\"]:.3f}\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8468176",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Variations of inducing points\n",
    "#     1. Application of differentiable transformation, defined via `transform_cls`\n",
    "#     2. Whether inducing points wrt patches or images.\n",
    "# \n",
    "\n",
    "def reinitialize_inducing(params, key, transform_cls, X_train, L=10):\n",
    "    \"\"\"Make full use of the set of inducing points.\n",
    "        - find L inducing points with smallest average μ magnitude\n",
    "        - re-initialize transformation parameters randomly\n",
    "        - update corresponding images in `Xu/X`\n",
    "    \"\"\"\n",
    "\n",
    "    ## find L inducing points with smallest average μ magnitude\n",
    "    im = pytree_leaf(params, 'params/Xu/X')\n",
    "    qμ = pytree_leaf(params, 'params/q/μ')\n",
    "    μconst = pytree_leaf(params, 'params/mean_fn/c')\n",
    "    qμ_mag = np.mean(np.abs(qμ - μconst[...,np.newaxis]), axis=0)\n",
    "    ind = np.argsort(qμ_mag)[:L]\n",
    "\n",
    "    ## re-initialize transformation parameters randomly\n",
    "\n",
    "    def reinitialize_T(T):\n",
    "        trans = transform_cls()\n",
    "        trans_params = {'params': pytree_leaf(params, 'params/Xu/transform')}\n",
    "        default_T_init_fn = trans.apply(trans_params, method=trans.default_T_init)[1]\n",
    "        return jax.ops.index_update(T, ind, default_T_init_fn(key, (L,)))\n",
    "\n",
    "    params = pytree_mutate_with_fn(params, 'params/Xu/transform/T', reinitialize_T)\n",
    "\n",
    "    ## re-initialize random images `Xu/X`\n",
    "\n",
    "    key, k2 = random.split(key)\n",
    "    def reinitialize_X(X):\n",
    "        Xind = np.take(X_train, random.randint(k2, (L,), 0, len(X_train)), axis=0)\n",
    "        return jax.ops.index_update(X, ind, Xind)\n",
    "    params = pytree_mutate_with_fn(params, 'params/Xu/X', reinitialize_X)\n",
    "    \n",
    "    return params, key\n",
    "\n",
    "params = opt.target\n",
    "params, key = reinitialize_inducing(params, key, transform_cls, X_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "S = pytree_leaf(params, 'params/Xu/X')[ind]\n",
    "fn = vmap(transform_to_matrix, (0, None, None), 0)\n",
    "A = fn(pytree_leaf(params, 'params/Xu/transform/T'),\n",
    "       transform_cls().T_type,\n",
    "       transform_cls().A_init_val)[ind]\n",
    "\n",
    "fn = vmap(spatial_transform_details, (0, 0, None), 0)\n",
    "T, Gs = fn(A, S, patch_shape)\n",
    "fig, axs = plt.subplots(2, len(A), figsize=(3*len(A),3*2))\n",
    "for i in range(len(T)):\n",
    "    plt_spatial_transform(axs[:,i], Gs[i], S[i], T[i])\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af85164b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_config_mnist():\n",
    "    \n",
    "    config_base = copy.deepcopy(get_config_base())\n",
    "    config = ml_collections.ConfigDict(config_base)\n",
    "    \n",
    "    config.image_shape = (28, 28, 1)\n",
    "    config.patch_shape = (10, 10)\n",
    "\n",
    "    config.output_dim = output_dim\n",
    "    \n",
    "    config.n_inducing = 40\n",
    "    config.inducing_init_fn = 'kmeans'\n",
    "    \n",
    "    config.T_type = 'transl'\n",
    "    config.use_loc_kernel = False\n",
    "    \n",
    "    return config\n",
    "\n",
    "\n",
    "def get_config_cifar10():\n",
    "    \n",
    "    config_base = copy.deepcopy(get_config_base())\n",
    "    config = ml_collections.ConfigDict(config_base)\n",
    "    \n",
    "    config.image_shape = (32, 32, 3)\n",
    "    config.patch_shape = (10, 10)\n",
    "\n",
    "    config.output_dim = output_dim\n",
    "    \n",
    "    config.n_inducing = 100\n",
    "    config.inducing_init_fn = 'random'\n",
    "    \n",
    "    config.T_type = 'transl'\n",
    "    config.use_loc_kernel = False\n",
    "    \n",
    "    return config\n",
    "\n",
    "\n",
    "# config = get_config_cifar10()\n",
    "config = get_config_mnist()\n",
    "print(config)\n",
    "\n",
    "patch_shape = config.patch_shape \n",
    "image_shape = config.image_shape\n",
    "n_inducing = config.n_inducing\n",
    "output_dim = config.output_dim\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "model_cls, k_cls, lik_cls, inducing_loc_cls, transform_cls = \\\n",
    "    get_model_cls(key, config, X_train)\n",
    "model = model_cls()\n",
    "params = model.get_init_params(model, key, X_shape=config.image_shape)\n",
    "print(model)\n",
    "pytree_keys(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73c323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(transform_cls(), SpatialTransform):\n",
    "    fig = plt_inducing_inputs_spatial_transform(params, model, patch_shape, max_show=min(10, n_inducing))\n",
    "#     plt_savefig(fig, './summary/assets/convgp_mnist_st_before.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-tracy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load pretrained weights & set initial values\n",
    "# if g_cls != LayerIdentity:\n",
    "#     g_path = 'params/k/kx/g' if isinstance(k_cls(), CovICM) else 'params/k/g'\n",
    "#     encoder_params = pytree_load({'params': pytree_leaf(params, g_path)}, cnn_save_path)\n",
    "#     encoder_params_kvs = pytree_get_kvs(encoder_params['params'])\n",
    "#     params = pytree_mutate(params, {f'params/{g_path}/{k}': v for k,v in encoder_params_kvs.items()})\n",
    "\n",
    "params = pytree_mutate(params, {f'params/k/ks_{i}/kg/kl/ls': softplus_inv(np.array([1.]))\n",
    "                                for i in range(output_dim)})\n",
    "\n",
    "kwd_notrain =  ['mean_fn', 'Xu/X'] # 'Xu/T/transform'\n",
    "kwd_notrain += [f'params/k/ks_{i}/kg/kl/σ2' for i in range(output_dim)]\n",
    "kwd_notrain += [f'params/k/ks_{i}/kg/kl/ls' for i in range(output_dim)]\n",
    "kwd_trainslow = [] # 'Xu/transform'\n",
    "opt = flax_create_multioptimizer(\n",
    "    params, 'Adam',\n",
    "    [{'learning_rate': 0.}, {'learning_rate': .01}, {'learning_rate': .03}],\n",
    "    [lambda p, v: pytree_path_contains_keywords(p, kwd_notrain),\n",
    "     lambda p, v: pytree_path_contains_keywords(p, kwd_trainslow),\n",
    "     lambda p, v: not pytree_path_contains_keywords(p, kwd_notrain+kwd_trainslow)])\n",
    "\n",
    "flax_check_multiopt(params, opt)\n",
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-associate",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "        \n",
    "@jax.jit\n",
    "def eval_step2(params, X):\n",
    "    Ey, Vy = model.apply(params, X, method=model.pred_y, rngs={'lik_mc_samples': key})\n",
    "    return Ey\n",
    "\n",
    "@jax.jit\n",
    "def train_step2(step, opt, batch, key):\n",
    "    key, subkey = random.split(key)\n",
    "    X, y = batch\n",
    "    y_onehot = jax.nn.one_hot(y.squeeze(), num_classes=output_dim)\n",
    "    def loss_fn(params):\n",
    "        fx = model.apply(params,\n",
    "                         (X, y_onehot),\n",
    "                         method=model.mll,\n",
    "                         rngs={'lik_mc_samples': subkey})\n",
    "        return -fx, {}\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    params = opt.target\n",
    "    (loss, aux), grad = grad_fn(params)\n",
    "    opt = opt.apply_gradient(grad)\n",
    "    log = {'loss': loss}\n",
    "    return opt, log, key\n",
    "\n",
    "\n",
    "bsz = 64\n",
    "train_n_batches, train_batches = get_data_stream(\n",
    "    key, bsz, data_train)\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start = time.time()\n",
    "    for it in range(train_n_batches):\n",
    "        step = epoch*train_n_batches+it\n",
    "        batch = next(train_batches)\n",
    "        opt, log, key = train_step2(step, opt, batch, key)\n",
    "        params = opt.target\n",
    "        if step%(train_n_batches//5)==0:\n",
    "            insert_str = '/kg' if isinstance(model.k_cls().k_cls(), CovConvolutional) else ''\n",
    "            log.update({\n",
    "               'k.ls': jax.nn.softplus(pytree_leaf(params, f'params/k/kx/ls')\n",
    "                                       if isinstance(model.k_cls(), CovICM) else \n",
    "                                       np.hstack([pytree_leaf(params, f'params/k/ks_{i}{insert_str}/kp/ls')\n",
    "                                                  for i in range(output_dim)])),\n",
    "               'k.σ2': jax.nn.softplus(pytree_leaf(params, f'params/k/kx/σ2')\n",
    "                                       if isinstance(model.k_cls(), CovICM) else \n",
    "                                       np.hstack([pytree_leaf(params, f'params/k/ks_{i}{insert_str}/kp/σ2')\n",
    "                                                  for i in range(output_dim)])),\n",
    "               'kl.ℓ': jax.nn.softplus(np.hstack([pytree_leaf(params, f'params/k/ks_{i}/kg/kl/ls')\n",
    "                                                  for i in range(output_dim)])\n",
    "                                       if  isinstance(model.k_cls(), CovMultipleOutputIndependent) and \\\n",
    "                                           not isinstance(model.k_cls().k_cls().kg_cls().kl_cls(), CovConstant) else np.array([np.nan]))})\n",
    "            \n",
    "            print(f'[{epoch:3}|{100*it/train_n_batches:5.2f}%]\\t'\n",
    "                  f'Loss={log[\"loss\"]:.3f}\\t'\n",
    "                  f'Time={time.time()-start:.3f}\\t'\n",
    "                  f'k.ls={log[\"k.ls\"][:3]}\\t'\n",
    "                  f'kl.ℓ = {log[\"kl.ℓ\"][:3]}\\t'\n",
    "#                   f'kl.ℓ = {log[\"kl.ℓ\"][:,0]}|{log[\"kl.ℓ\"][:,2]}|{log[\"kl.ℓ\"][:,3]}\\t'\n",
    "                  f'k.σ2={log[\"k.σ2\"][:3]}\\t')\n",
    "            start = time.time()\n",
    "\n",
    "\n",
    "    metrics = eval_model(params, data_test, logit_fn=eval_step2)\n",
    "    print(f'[{epoch:3}] test \\t'\n",
    "          f'Loss={metrics[\"loss\"]:.3f}\\t'\n",
    "          f'accuracy={metrics[\"accuracy\"]:.3f}\\t')\n",
    "\n",
    "parmas = opt.target\n",
    "m = model.bind(params, rngs={'lik_mc_samples': key})\n",
    "\n",
    "# \n",
    "# n_inducing=50, LikMulticlassDirichlet\n",
    "# CovConvolutional(patch_inducing_loc=False) acc=.7\n",
    "# CovConvolutional(patch_inducing_loc=True, patch_shape=(3,3)) acc=.83\n",
    "# CovConvolutional(patch_inducing_loc=True, patch_shape=(7,7)) acc=.95\n",
    "\n",
    "# n_inducing=50, LikMulticlassDirichlet, patch_shape=(7, 7), patch_inducing_loc=True\n",
    "\n",
    "# loc kernel `transl`\n",
    "# use_loc_kernel=False: acc=.95\n",
    "# use_loc_kernel=True:  acc=.96   (faster convergence. kl.ℓ = [6.1   2.025 1.815], degree of insensitivity)\n",
    "\n",
    "# loc kernel `transl+isot_scal`\n",
    "# use_loc_kernel=False: acc=.96\n",
    "# use_loc_kernel=True:  acc=.96\n",
    "\n",
    "# n_inducing=50, LikMulticlassDirichlet, patch_shape=(10,10), patch_inducing_loc=True\n",
    "# spatial transform=`transl+isot_scal`, kloc=CovSE, g_cls=LayerIdentity:      Loss=-0.760\taccuracy=0.976, kl.ℓ = [6.361 1.724 2.236]\n",
    "# spatial transform=`transl+isot_scal`, kloc=CovSE, g_cls=CNNMnistTrunk:      Loss=-0.843\taccuracy=0.996\tkl.ℓ = [4.867 2.849 2.592]\n",
    "# spatial transform=`transl`, kloc=CovSE, g_cls=CNNMnistTrunk:                Loss=-0.837\taccuracy=0.996\tkl.ℓ = [5.89  3.06  2.839]\n",
    "\n",
    "\n",
    "# output_dim=10\n",
    "# \n",
    "# spatial transform=`transl`, kloc=CovSE, g_cls=CNNMnistTrunk:                Loss=-0.556\taccuracy=0.973\tkl.ℓ = [5.304 4.297 2.305]\n",
    "# spatial transform=`transl`, kloc=CovSE, g_cls=CNNMnistTrunk, fix kl.ℓ=.5    Loss=-0.558\taccuracy=0.983\tkl.ℓ = [0.5 0.5 0.5]\n",
    "# spatial transform=`transl`, kloc=CovSE, g_cls=CNNMnistTrunk, 500 inducing   Loss=-0.570\taccuracy=0.984\tkl.ℓ = [0.5 0.5 0.5]\n",
    "\n",
    "# output_dim=3\n",
    "#\n",
    "# spatial transform=`transl`, kloc=CovSE, g_cls=CNNMnistTrunk, 30  inducing            Loss=-0.838\taccuracy=0.994\tkl.ℓ = [0.5 0.5 0.5]\n",
    "# spatial transform=`transl+iso_scal`, kloc=CovSE, g_cls=CNNMnistTrunk, 30  inducing   Loss=-0.814\taccuracy=0.991\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d74267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = model.bind(params, rngs={'lik_mc_samples': key})\n",
    "# tscal = m.Xu.transform.scal\n",
    "# plt.plot(np.arange(len(tscal)), tscal[:,0])\n",
    "# plt.grid()\n",
    "# print(np.array(patch_shape)/np.array(image_shape[:2]), '->', np.mean(tscal, axis=0))\n",
    "\n",
    "# # visualize patch location kernel \n",
    "# scal, transl = extract_patches_2d_scal_transl(image_shape,\n",
    "#                                               patch_shape)\n",
    "# scal = np.repeat(scal[np.newaxis,...], len(transl), axis=0)\n",
    "# XL = np.column_stack([scal, transl])\n",
    "# KL = m.k.ks[0].kg.kl(XL)\n",
    "# fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "# ax.imshow(KL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2c78c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Rank inducing patches based on variational μ for different classes\n",
    "\n",
    "m = model.bind(params, rngs={'lik_mc_samples': key})\n",
    "Xu = m.Xu()[0] if config.use_loc_kernel else m.Xu()\n",
    "qm = m.q.μ - m.mean_fn.c[...,np.newaxis]\n",
    "\n",
    "M = len(Xu)\n",
    "    \n",
    "\n",
    "gridspec_kw = {'width_ratios': [1,1], 'height_ratios': [1]*output_dim}\n",
    "fig, axs = plt.subplots(output_dim,2,figsize=(10*2,5*output_dim))\n",
    "n_top = n_inducing\n",
    "c = 0; ind = np.argsort(qm[c,:])\n",
    "ylim = (np.min(qm), np.max(qm))\n",
    "\n",
    "for c in range(output_dim):\n",
    "    ind = np.argsort(qm[c,:])[::-1]\n",
    "    # variational μ\n",
    "    ax = axs[c,0]\n",
    "    for co in range(output_dim):\n",
    "        ls = '+--' if co != c else '+-'\n",
    "        ax.plot(np.arange(n_inducing), qm[co,ind], ls, label=f'{Y_subset[co]}', lw=5)\n",
    "    ax.set_ylabel(f'qμ ({Y_subset[c]})', fontsize=35)\n",
    "    ax.grid()\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.legend(fontsize=20)\n",
    "    \n",
    "    # top weighted patches\n",
    "    ax = axs[c,1]\n",
    "    ims = Xu[ind].reshape((-1,*patch_shape))[:n_top]\n",
    "    grid = make_im_grid(ims, im_per_row=min(len(ims), 8))\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.imshow(grid, cmap='Greys', vmin=0, vmax=1)\n",
    "    \n",
    "fig.tight_layout()\n",
    "# plt_savefig(fig, './summary/assets/convgp_mnist_variational_mean_sorted.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ecc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank inducing patches based on average variational μ\n",
    "#     goal is to see if there are patches not used as evidence for any class \n",
    "#     sometime its patches where all classes have ... so cannot be used as discriminating features \n",
    "#\n",
    "\n",
    "Xu = m.Xu()[0] if config.use_loc_kernel else m.Xu()\n",
    "qm = pytree_leaf(params, 'params/q/μ')\n",
    "μconst = pytree_leaf(params, 'params/mean_fn/c')[...,np.newaxis]\n",
    "qm_mag = np.mean(np.abs(qm - μconst), axis=0)\n",
    "ind = np.argsort(qm_mag)[::-1]\n",
    "\n",
    "fig, axs = plt.subplots(2,1,figsize=(8,10))\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(np.arange(n_inducing), qm_mag[ind], '-')\n",
    "ax.set_title(f'qμ magnitude', fontsize=35)\n",
    "ax.grid()\n",
    "ax.set_ylim((-.5, np.max(qm_mag)+.5))\n",
    "fig.tight_layout()\n",
    "\n",
    "ax = axs[1]\n",
    "C = Xu[ind].shape[-1]\n",
    "ims = Xu[ind].reshape((-1,*patch_shape,C))\n",
    "grid = make_im_grid(ims, im_per_row=min(len(ims), 10))\n",
    "ax.set_xticks([]); ax.set_yticks([])\n",
    "ax.imshow(grid, cmap='Greys' if C == 1 else None, vmin=0, vmax=1)\n",
    "\n",
    "\n",
    "## plot spatial transform as well\n",
    "\n",
    "if isinstance(transform_cls(), SpatialTransform):\n",
    "    ind = np.arange(min(5, n_inducing))\n",
    "    \n",
    "    m = model.bind(params)\n",
    "    A = m.Xu.transform.T\n",
    "    S = pytree_leaf(params, 'params/Xu/X')\n",
    "    A = A[ind]; S = S[ind]\n",
    "\n",
    "    fn = vmap(spatial_transform_details, (0, 0, None), 0)\n",
    "    T, Gs = fn(A, S, patch_shape)\n",
    "    fig, axs = plt.subplots(2, len(A), figsize=(3*len(A),3*2))\n",
    "    for i in range(len(T)):\n",
    "        plt_spatial_transform(axs[:,i], Gs[i], S[i], T[i])\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig.tight_layout()\n",
    "#     plt_savefig(fig, './summary/assets/convgp_mnist_st_after.png')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff4455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_all = torchvision.datasets.MNIST('./data', train=True, download=True)\n",
    "# X_all = jax_to_gpu(np.asarray(dataset_all.data[...,np.newaxis]) / 255.)\n",
    "# y_all = jax_to_gpu(np.asarray(dataset_all.targets[...,np.newaxis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-playing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## \n",
    "\n",
    "print(np.where(y_test==0)[0][:10])\n",
    "\n",
    "ind_digit0 = np.where(y_test==0)[0]\n",
    "ind_digit1 = np.where(y_test==1)[0]\n",
    "\n",
    "ind = 4\n",
    "n_ims = 20\n",
    "\n",
    "x_rot = X_test[ind]; x_rot = X_all[ind]\n",
    "x_rot = rotated_ims(x_rot, n_ims=n_ims)\n",
    "# X = np.stack(X_test[ind_digit0[:10]])\n",
    "# X = np.stack(X_test[np.hstack((ind_digit0[:n_ims//2], ind_digit1[:n_ims//2]))])\n",
    "X = x_rot\n",
    "μf, σ2o, σ2p, σ2f, _, _, _ = model.apply(params, X, output_gh=True, method=SVGP_pred_f_details)\n",
    "μf = μf.squeeze()\n",
    "\n",
    "y_pred = np.array(Y_subset)[np.argmax(μf,axis=-1)]\n",
    "# α = gamma_to_lognormal_inv(μf, σ2f, approx_type='kl')\n",
    "\n",
    "if isinstance(model.lik_cls(), LikMulticlassSoftmax):\n",
    "    lik_test = LikMulticlassSoftmax(output_dim=output_dim, n_mc_samples=5000)\n",
    "    p_mc, Vp_mc = lik_test.apply({}, μf, σ2f, rngs={'lik_mc_samples': key}, method=lik_test.predictive_dist)\n",
    "elif isinstance(model.lik_cls(), LikMulticlassDirichlet):\n",
    "    lik_test = LikMulticlassDirichlet(output_dim=output_dim, n_mc_samples=5000)\n",
    "    p_mc, Vp_mc = lik_test.apply({}, μf, σ2f, rngs={'lik_mc_samples': key}, method=lik_test.predictive_dist)\n",
    "    α = gamma_to_lognormal_inv(μf, σ2f, approx_type='kl')\n",
    "    α0 = np.sum(α, axis=-1, keepdims=True)\n",
    "    p = α / α0\n",
    "    Vp = p*(1-p)\n",
    "\n",
    "gridspec_kw = {'width_ratios': [1],\n",
    "               'height_ratios': [4, 4, 4, 1]}\n",
    "fig, axs = plt.subplots(4, 1, gridspec_kw=gridspec_kw, figsize=(15, 15))\n",
    "cmap = plt.cm.get_cmap('Set1')\n",
    "colors = [cmap(0), cmap(1), cmap(2)]\n",
    "\n",
    "\n",
    "ax = axs[0]\n",
    "ax.set_xticks([])\n",
    "for i, d in enumerate(Y_subset):\n",
    "    c = colors[i]\n",
    "    μ, std = μf[:,i], np.sqrt(σ2f[:,i])\n",
    "    ax.plot(np.arange(len(X)), μf[:,i], lw=2, color=c, label=d)\n",
    "    ax.plot(np.arange(len(X)), μ + 2*std, '--', c=c)\n",
    "    ax.plot(np.arange(len(X)), μ - 2*std, '--', c=c)\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "ax.set_ylabel('posterior p(f*|X)')\n",
    "\n",
    "\n",
    "ax = axs[1]\n",
    "ax.set_xticks([])\n",
    "for i, d in enumerate(Y_subset):\n",
    "    c = colors[i]\n",
    "    ax.plot(np.arange(len(X)), α[:,i], lw=2, color=c, label=d)\n",
    "ax.grid()\n",
    "ax.set_yticks(np.linspace(0, np.floor(np.max(α)*1.1), 5))\n",
    "ax.set_ylabel('α')\n",
    "\n",
    "\n",
    "ax = axs[2]\n",
    "ax.set_xticks([])\n",
    "# c = 1\n",
    "# ax.plot(np.arange(len(X)), σ2f[:,c], label='Σf=Σp+Σo')\n",
    "# ax.plot(np.arange(len(X)), σ2p[:,c], label='Σp(data)')\n",
    "\n",
    "for i, d in enumerate(Y_subset):\n",
    "    c = colors[i]\n",
    "#     ax.plot(np.arange(len(X)), σ2f[:,i], color=c, label=f'Σp[{d}]') # signap vary together for different classes !\n",
    "#     ax.plot(np.arange(len(X)), σ2f[:,c], label=f'Σf[{d}]')\n",
    "    ax.plot(np.arange(len(X)), σ2o[:,i], color=c, label=f'Σo[{d}]')\n",
    "# ax.plot(np.arange(len(X)), np.mean(σ2o, axis=-1), color='k', label='avgΣo')\n",
    "ax.plot(np.arange(len(X)), np.mean(σ2p, axis=-1), color='k', label='avgΣp')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "\n",
    "# for i, d in enumerate(Y_subset):\n",
    "#     c = colors[i]\n",
    "#     if isinstance(model.lik_cls(), LikMulticlassSoftmax) or \\\n",
    "#         isinstance(model.lik_cls(), LikMulticlassDirichlet):\n",
    "#         ax.plot(np.arange(len(X)), p[:,i], '-', c=c, lw=2)\n",
    "#         ax.plot(np.arange(len(X)), p[:,i] + 2*np.sqrt(Vp[:,i]), '--', c=c)\n",
    "#         ax.plot(np.arange(len(X)), p[:,i] - 2*np.sqrt(Vp[:,i]), '--', c=c)\n",
    "#     else:\n",
    "#         ax.plot(np.arange(len(X)), μf[:,i], c=c, lw=2)\n",
    "# ax.set_ylabel('E[p(y|f*)]')\n",
    "# ax.grid()\n",
    "\n",
    "    \n",
    "ax = axs[3]\n",
    "ax.set_xticks([]); ax.set_yticks([])\n",
    "ax.imshow(np.hstack([x for x in X]), cmap='Greys')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-baghdad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1,1\n",
    "# 6,5\n",
    "# 7\n",
    "xi = 4\n",
    "x = X_test[xi:xi+1]; y = y_test[xi][0]\n",
    "print(x.shape, y.shape)\n",
    "μ, σ2, AA, δ, mf = model.apply(params, x, method=SVGP_pred_f_details)\n",
    "print(AA.shape)\n",
    "plt.imshow(x[0])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e79b558",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plt_func_with_inducing_patches(ax, y, Z, ind=None, labels=None, ylabel=''):\n",
    "    \"\"\" y:   (M, D)\n",
    "        Z:   (M, h, w, 1)\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        labels = np.arange(y.shape[1])\n",
    "    if ind is not None:\n",
    "        y = y[ind]\n",
    "        Z = Z[ind]\n",
    "        M = len(ind)\n",
    "    else:\n",
    "        M = y.shape[0]\n",
    "\n",
    "    for i, d in enumerate(labels):\n",
    "        ax.plot(np.arange(M), y[:,i], '+--', markersize=20,label=f'{d}',linewidth=3,markeredgewidth=5)\n",
    "    ax.grid()\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlim((0-.5,(M-1)*1+.5))\n",
    "    ax.set_xticks(np.arange(M))\n",
    "    ax.tick_params(labelbottom=False)\n",
    "    \n",
    "    \n",
    "def plt_inducing_patches(ax, Z, ind=None):\n",
    "    if ind is not None:\n",
    "        Z = Z[ind]\n",
    "        M = len(ind)\n",
    "    else:\n",
    "        M = len(Z)\n",
    "\n",
    "    # top weighted patches\n",
    "    grid = make_im_grid(Z, im_per_row=M)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.imshow(grid, cmap='Greys', vmin=0, vmax=1)\n",
    "    \n",
    "\n",
    "Xu = m.Xu()[0] if config.use_loc_kernel else m.Xu()\n",
    "n = 0\n",
    "c = int(y)\n",
    "N, M, D = AA.shape \n",
    "A = AA[n]\n",
    "\n",
    "Aδ = A*δ\n",
    "# A: (N, M, D)\n",
    "# A:  (M, D)\n",
    "# δ:  (M, D)\n",
    "# Aδ: (M, D)\n",
    "\n",
    "firstn = 10 ; hp = 1.8\n",
    "#; firstn = n_inducing; hp = 5\n",
    "\n",
    "ind = np.argsort(Aδ[:,c])[::-1][:firstn]\n",
    "\n",
    "\n",
    "gridspec_kw = {'height_ratios': [hp,hp,hp,1], }\n",
    "fig, axs = plt.subplots(4,1,figsize=(15,12), gridspec_kw=gridspec_kw)\n",
    "\n",
    "cmap = plt.cm.get_cmap('Set1')\n",
    "colors = [cmap(0), cmap(1), cmap(2)]\n",
    "\n",
    "# ind = np.argsort(A[:,c])[::-1][:firstn] #; ind = np.hstack((ind[:10],ind[-10:]))\n",
    "plt_func_with_inducing_patches(axs[0], A, Xu, ind, labels=Y_subset, ylabel='A')\n",
    "axs[0].legend(loc='upper right')\n",
    "#plt_inducing_patches(axs[1], Xu, ind)\n",
    "\n",
    "\n",
    "# ind = np.argsort(δ[:,c])[::-1][:firstn] #; ind = np.hstack((ind[:10],ind[-10:]))\n",
    "plt_func_with_inducing_patches(axs[1], δ, Xu, ind, labels=Y_subset, ylabel='(μᵤ-mᵤ)')\n",
    "#plt_inducing_patches(axs[3], Xu, ind)\n",
    "\n",
    "# ind = np.argsort(Aδ[:,c])[::-1][:firstn] #; ind = np.hstack((ind[:10],ind[-10:]))\n",
    "plt_func_with_inducing_patches(axs[2], Aδ, Xu, ind, labels=Y_subset, ylabel='A*(μᵤ-mᵤ)')\n",
    "plt_inducing_patches(axs[3], Xu, ind)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "# plt_savefig(fig, f'./summary/assets/convgp_mnist_inference_weights_{int(y)}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40c2402",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def plt_heatmap_and_inducing_patch(fig, axs, x, z, colors, start_ind,\n",
    "                                   colorbar=True, sort_by_color=True,\n",
    "                                   normalize_color=False, heatmap=True, mpl_norm=None):\n",
    "    h, w, C = z.shape\n",
    "\n",
    "    if normalize_color:\n",
    "        colors = jax.nn.softmax(colors)\n",
    "    \n",
    "    if sort_by_color and not heatmap:\n",
    "        ind = np.argsort(colors)\n",
    "        colors = colors[ind]\n",
    "        start_ind = start_ind[ind]\n",
    "\n",
    "    ax = axs[0]\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.imshow(x.squeeze(), cmap='Greys')\n",
    "    \n",
    "    \n",
    "    if heatmap:\n",
    "        color_dim = np.sqrt(colors.size).astype(np.int32)\n",
    "        heatmap = jax.image.resize(colors.reshape(color_dim, color_dim), \n",
    "                                   x.squeeze().shape[:2],\n",
    "                                   method='bilinear')\n",
    "#         heatmap = np.repeat(heatmap[...,np.newaxis], C, axis=-1)\n",
    "        im = ax.imshow(heatmap, cmap='Reds', alpha=.5, norm=mpl_norm)\n",
    "        if colorbar:\n",
    "            cbar = fig.colorbar(im, cax=plt_scaled_colobar_ax(ax))\n",
    "    else:\n",
    "        if mpl_norm is None:\n",
    "            mpl_norm = mpl.colors.Normalize()\n",
    "        rects = [mpl_patches.Rectangle(xy[::-1], h, w)\n",
    "                 for xy in start_ind]\n",
    "        collection = mpl.collections.PatchCollection(rects,\n",
    "                                                     linewidth=mpl_norm(colors)*5+1,\n",
    "                                                     alpha=.8,\n",
    "                                                     facecolor='none',\n",
    "                                                     ls='--',\n",
    "                                                     norm=mpl_norm,\n",
    "                                                     cmap=plt.get_cmap('Reds'))\n",
    "        collection.set_array(colors)\n",
    "        ax.add_collection(collection)\n",
    "        if colorbar:\n",
    "            cbar = fig.colorbar(collection, cax=plt_scaled_colobar_ax(ax))\n",
    "\n",
    "#     ax = axs[1]\n",
    "#     ax.set_xticks([]); ax.set_yticks([])\n",
    "#     ax.imshow(z, cmap='Greys')\n",
    "    \n",
    "\n",
    "## For inducing patches with large magnitude for A*δ,\n",
    "#      plot similarty of these patches to patches in test image ... \n",
    "#      to show where the model is looking at\n",
    "# \n",
    "# from plt_utils import *\n",
    "\n",
    "start_ind = CNNMnistTrunk.get_start_ind(image_shape)\n",
    "\n",
    "c = int(y)\n",
    "# first several patches with larges evidence\n",
    "# orderby = np.abs(Aδ[:,c])\n",
    "orderby = Aδ[:,c]\n",
    "ind = np.argsort(orderby)[::-1][:10]\n",
    "# first several patches with largest coefficients\n",
    "# ind = np.argsort(np.abs(A[:,c]))[::-1][:10]\n",
    "# first several patches with largest evidence\n",
    "# ind = np.argsort(np.abs(δ[:,c]))[::-1][:10]\n",
    "\n",
    "# (D, M, 1)\n",
    "Kg = m.k.ks[c].kg.Kuf(m.k.slice_and_map_inducing(m.Xu()),\n",
    "                      m.k.slice_and_map(x))\n",
    "Kg = Kg.squeeze() # (#inducing, #patches) for colors \n",
    "K = np.mean(Kg, (1,)) # (#inducing, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## spatial transforms\n",
    "trans = m.Xu.transform.T\n",
    "S = pytree_leaf(params, 'params/Xu/X')\n",
    "trans = trans[ind]; S = S[ind]\n",
    "fn = vmap(spatial_transform_details, (0, 0, None), 0)\n",
    "T, Gs = fn(trans, S, patch_shape)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2,len(ind),figsize=(7*len(ind),7*2))\n",
    "cmap = plt.get_cmap('bwr')\n",
    "normalize_color = True\n",
    "if normalize_color:  \n",
    "    mpl_norm = mpl.colors.Normalize()\n",
    "else:\n",
    "    cbar_vmin, cbar_vmax = np.min(Kg[ind,:]), np.max(Kg[ind,:])\n",
    "    mpl_norm = mpl.colors.Normalize(vmin=cbar_vmin,\n",
    "                                    vmax=cbar_vmax)\n",
    "\n",
    "for i, Mi in enumerate(ind):\n",
    "    ev = Aδ[Mi,c]\n",
    "#     axs[2,i].set_xlabel(f'{ev:.2f}={A[Mi,c]:.2f}*{δ[Mi,c]:.2f}', color=cmap(.9) if ev>0 else cmap(0.1),\n",
    "#                         fontsize=35)\n",
    "    plt_heatmap_and_inducing_patch(fig, axs[:,i], x, Xu[Mi], Kg[Mi,:]-np.min(Kg[Mi,:]), start_ind, mpl_norm=mpl_norm, normalize_color=normalize_color)\n",
    "    plt_spatial_transform(axs[1,i], Gs[i], S[i], T[i], overlay=True)\n",
    "\n",
    "fig.tight_layout()\n",
    "# plt_savefig(fig, f'./summary/assets/convgp_mnist_inference_heatmap_{int(y)}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d68ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0ad1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:misc_impl]",
   "language": "python",
   "name": "conda-env-misc_impl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
