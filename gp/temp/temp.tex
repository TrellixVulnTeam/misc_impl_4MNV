\documentclass[11pt]{article}
\input{../../preamble_local.tex}
\addbibresource{GP.bib}
\addbibresource{multitask.bib}


\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
    \node[shape=circle, draw, inner sep=1pt, 
        minimum height=12pt] (char) {\vphantom{1g}#1};}}
\renewcommand\bf{\ensuremath{\mathbf{f}}} 



\begin{document}

\section{FITC}

\subsection{FITC}


\begin{align}
    q_{\text{FITC}}(\bf_*\mid \by)
        &\sim \sN\left(
            Q_{*\bf} ( Q_{\bf\bf} + \Lambda )^{-1} \by,
            K_{**} - Q_{*\bf} ( Q_{\bf\bf} + \Lambda )^{-1} Q_{\bf*}
        \right) 
        \label{eq:fitc_predictive_intuitive}
        \\
        &\sim \sN\left(
            K_{*\bu}\Sigma K_{\bu\bf} \Lambda^{-1} \by,
            K_{**} - Q_{**} + K_{*\bu}\Sigma K_{\bu*}
        \right)
        \label{eq:fitc_predictive_computation}
\end{align}
where $\Lambda = \text{diag}\pb{ K_{\bf\bf} - Q_{\bf\bf} + \sigma_n^2 I }$ and $\Sigma = (K_{\bu\bu} + K_{\bu\bf}\Lambda^{-1}K_{\bf\bu})^{-1}$. Here Equation (\ref{eq:fitc_predictive_intuitive}) follows from natural derivation and the Equation (\ref{eq:fitc_predictive_computation}) is more computationally attractive. To see the equivalence, we first apply Woodbury inversion formula,
\begin{align}
    (\Lambda + Q_{\bf\bf})^{-1}
        = \Lambda^{-1} - \Lambda^{-1} K_{\bf\bu} (K_{\bu\bu} + K_{\bu\bf} \Lambda^{-1} K_{\bf\bu} )^{-1} K_{\bu\bf} \Lambda^{-1}
    \label{eq:fitc_woodbury_inverse}
\end{align}
By definition of $\Sigma$, we have $(\Lambda + Q_{\bf\bf})^{-1} = \Lambda^{-1} - \Lambda^{-1} K_{\bf\bu} \Sigma K_{\bu\bf} \Lambda^{-1}$. Therefore,
\begin{align*}
    \mu_{\bf_*} 
        &= Q_{*\bf} ( Q_{\bf\bf} + \Lambda )^{-1} \by  \\
        &= K_{*\bu} K_{\bu\bu}^{-1} K_{\bu*} ( \Lambda^{-1} - \Lambda^{-1} K_{\bf\bu} \Sigma K_{\bu\bf} \Lambda^{-1} ) \by 
            \tag{Woodbury with defn $\Sigma$} \\
        &= K_{*\bu} K_{\bu\bu}^{-1} ( \Sigma^{-1} - K_{\bu\bf} \Lambda^{-1} K_{\bf\bu} ) \Sigma K_{\bu\bf} \Lambda^{-1} \by 
            \tag{pull out $\Sigma K_{\bu\bf}\Lambda^{-1}$}
        \\
        &= K_{*\bu} K_{\bu\bu}^{-1} K_{\bu\bu} \Sigma K_{\bu\bf} \Lambda^{-1} \by
            \tag{defn $\Sigma$} \\
        &= K_{*\bu}\Sigma K_{\bu\bf} \Lambda^{-1} \by \\
    \cov_{\bf_*}
        &= K_{**} - Q_{*\bf} ( Q_{\bf\bf} + \Lambda )^{-1} Q_{\bf*} \\ 
        &= K_{**} - K_{*\bu}\Sigma K_{\bu\bf} \Lambda^{-1} K_{\bf\bu} K_{\bu\bu}^{-1} K_{\bu*} 
            \tag{results from $\mu_{\bf_*}$} \\
        &= K_{**} - K_{*\bu}\Sigma (\Sigma^{-1} - K_{\bu\bu}) K_{\bu\bu}^{-1} K_{\bu*} 
            \tag{defn $\Sigma$} \\
        &= K_{**} - Q_{**} + K_{*\bu}\Sigma K_{\bu*}
\end{align*}


\subsection{Implementation Details}

To optimize for inducing variables, we need a computationally stable way to compute $\log p(\by\mid X)$. More specifically, we need a way to compute $\by^T (\Lambda + Q_{\bf\bf})^{-1} \by$ and $\log | \Lambda + Q_{\bf\bf} |$. Referencing (\href{https://github.com/GPflow/GPflow/blob/develop/gpflow/models/sgpr.py#L263}{gpflow}, \href{https://bwengals.github.io/pymc3-fitcvfe-implementation-notes.html}{a blog}),
\begin{align*}
    (\Lambda + Q_{\bf\bf})^{-1}
        &= (\Lambda + V^TV)^{-1}
            \tag{\texttt{chol} $K_{\bu\bu} = L_{\bu\bu} L_{\bu\bu}^T$, \texttt{backsolve} $V = L_{\bu\bu}^{-1}K_{\bu\bf}$, $Q_{\bf\bf} = V^TV$} \\ 
        &= \Lambda^{-1} - \Lambda^{-1} V^T (I + V\Lambda^{-1}V^T)^{-1} V \Lambda^{-1}
            \tag{Woodbury} \\
        &= \Lambda^{-1} - \Lambda^{-1} V^T L_B^{-T} L_B^{-1} V \Lambda^{-1}
            \tag{$B:= I + V\Lambda^{-1}V^T$, \texttt{chol} $B = L_{B}L_{B}^T$} \\
    \by^T (\Lambda + Q_{\bf\bf})^{-1} \by
        &= \by^T \Lambda^{-1} \by - \bgamma^T\bgamma
            \tag{\texttt{backsolve} $\bgamma = L_B^{-1} V\Lambda^{-1} \by $}  \\
    \log \det (\Lambda + Q_{\bf\bf})
        &= \log \det(I + V\Lambda^{-1}V^T) + \log \det(\Lambda)
            \tag{ Matrix inversion lemma, $Q_{\bf\bf} = V^TV$} \\ 
        &= \log \det(B) + \log \det(\Lambda)  
            \tag{defn $B$} \\
        &= 2\sum_{i=1}^m \log \pb{L_B}_{ii} + \sum_{i=1}^n \log \pb{\Lambda}_{ii}
\end{align*}
Put everything together, we have an expression that involves mostly $\sO(m^3)$ \texttt{chol} and $\sO(nm^2)$ \texttt{backsolve},
\begin{align}
    \log p(\by\mid X)
        = - \frac{1}{2} \by^T \Lambda^{-1} \by + \frac{1}{2} \bgamma^T\bgamma - \sum_{i=1}^m \pb{L_B}_{ii} -\frac{1}{2} \sum_{i=1}^n \pb{\Lambda}_{ii} - \frac{n}{2} \log(2\pi)
\end{align}
We can also compute the predictive distribution in Equation (\ref{eq:fitc_predictive_computation}) as follows,
\begin{align*}
    \Sigma
        &= (K_{\bu\bu} + K_{\bu\bf}\Lambda^{-1}K_{\bf\bu})^{-1} \\
        &= (L_{\bu\bu} L_{\bu\bu}^T + L_{\bu\bu} L_{\bu\bu}^{-1} K_{\bu\bf}\Lambda^{-1}K_{\bf\bu} L_{\bu\bu}^{-T} L_{\bu\bu}^T ) 
            \tag{$K_{\bu\bu} = L_{\bu\bu} L_{\bu\bu}^T$} \\
        &= (L_{\bu\bu} B L_{\bu\bu}^T)^{-1}
            \tag{$B = I + L_{\bu\bu}^TL_{\bu\bf} \Lambda^{-1} K_{\bf\bu} L_{\bu\bu}^{-T}$} \\
        &= L_{\bu\bu}^{-T} L_{B}^{-T} L_{B}^{-1} L^{-1} \\
    \mu_{\bf_*}
        &= K_{*\bu}\Sigma K_{\bu\bf} \Lambda^{-1} \by \\ 
        &= K_{*\bu}  L_{\bu\bu}^{-T} L_{B}^{-T} L_{B}^{-1} L^{-1} K_{\bu\bf} \Lambda^{-1} \by 
            \tag{ substitute $\Sigma$} \\
        &= K_{*\bu}  L_{\bu\bu}^{-T} L_{B}^{-T} \bgamma
            \tag{ $\bgamma = L_B^{-1}L_{\bu\bu}^{-1} K_{\bu\bf} \Lambda^{-1}\by $} \\
        &= \bomega^T L_{B}^{-T} \bgamma 
            \tag{\texttt{backsolve} $\bomega = L_{\bu\bu}^{-1} K_{\bu *}$ implies $Q_{**}=\bomega^T\bomega$} \\
    \cov_{\bf_*}
        &= K_{**} - Q_{**} + K_{*\bu}\Sigma K_{\bu*} \\
        &= K_{**} - Q_{**} + K_{*\bu} L_{\bu\bu}^{-1} L_B^{-1} L_B^{-1} L_{\bu\bu}^{-1} K_{\bu *} 
            \tag{substitute $\Sigma$} \\
        &= K_{**} - \bomega^T \bomega + \bnu^T \bnu 
            \tag{\texttt{backsolve} $\bnu = L_B^{-1} \bomega$}
\end{align*}
Note that there are two additional \texttt{backsolve} to compute the predictive distribution.







\end{document}