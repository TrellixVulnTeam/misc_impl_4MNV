{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-publisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Reference:\n",
    "#     gpflow: https://gpflow.readthedocs.io/en/master/notebooks/advanced/gps_for_big_data.html\n",
    "#             https://github.com/GPflow/GPflow/blob/develop/gpflow/models/sgpr.py#L263\n",
    "#     julia:  https://github.com/STOR-i/GaussianProcesses.jl/blob/master/src/sparse/fully_indep_train_conditional.jl\n",
    "#     ladax:  https://github.com/danieljtait/ladax\n",
    "#\n",
    "\n",
    "import sys\n",
    "sys.path.append('../kernel')\n",
    "\n",
    "import numpy as onp\n",
    "import numpy.random as npr\n",
    "onp.set_printoptions(precision=3,suppress=True)\n",
    "\n",
    "import jax\n",
    "from jax import device_put, random\n",
    "import jax.numpy as np\n",
    "import jax.numpy.linalg as linalg\n",
    "from jax.scipy.linalg import cho_solve, solve_triangular\n",
    "\n",
    "from typing import Any, Callable, Sequence, Optional, Tuple\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax import optim, struct\n",
    "from flax.core import freeze, unfreeze\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "import torch\n",
    "print(torch.cuda.is_available(), jax.devices())\n",
    "\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "# https://matplotlib.org/3.1.1/gallery/style_sheets/style_sheets_reference.html\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 25\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "cmap = plt.cm.get_cmap('bwr')\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "import sys\n",
    "sys.path.append('../kernel')\n",
    "from jaxkern import (cov_se, cov_rq, cov_pe, LookupKernel, normalize_K, mtgp_k)\n",
    "\n",
    "from plt_utils import plt_savefig, plt_scaled_colobar_ax\n",
    "from gp import gp_regression_chol, run_sgd\n",
    "from gpax import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters \n",
    "\n",
    "xlim = (-1, 1)\n",
    "ylim = (-2, 2)\n",
    "n_train = 200\n",
    "n_test = 200\n",
    "σn = .5\n",
    "logsn = np.log(σn)\n",
    "lr = .01\n",
    "num_steps = 20\n",
    "\n",
    "\n",
    "## Data\n",
    "\n",
    "def f_gen(x):\n",
    "    return np.sin(x * 3 * 3.14) + \\\n",
    "           0.3 * np.cos(x * 9 * 3.14) + \\\n",
    "           0.5 * np.sin(x * 7 * 3.14)\n",
    "\n",
    "## Plotting\n",
    "\n",
    "npr.seed(0)\n",
    "key = jax.random.PRNGKey(0)\n",
    "X_train = np.expand_dims(npr.uniform(xlim[0]+.25, xlim[1]-.25, size=n_train), 1)\n",
    "y_train = f_gen(X_train) + σn * npr.rand(n_train, 1)\n",
    "data = (X_train, y_train)\n",
    "X_test  = np.expand_dims(np.linspace(*xlim, n_test), 1)\n",
    "\n",
    "X_train = device_put(X_train)\n",
    "y_train = device_put(y_train)\n",
    "X_test = device_put(X_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,5))\n",
    "ax.plot(X_train, y_train, 'x', alpha=1)\n",
    "ax.grid()\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlim(xlim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-action",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class SVGP(nn.Module, GPModel):\n",
    "    mean_fn_cls: Callable\n",
    "    k_cls: Callable\n",
    "    lik_cls: Callable\n",
    "    n_data: int\n",
    "    output_dim: int\n",
    "    Xu_initial: np.ndarray\n",
    "\n",
    "    def setup(self):\n",
    "        self.d = self.Xu_initial.shape[1]\n",
    "        self.n_inducing = self.Xu_initial.shape[0]\n",
    "        self.mean_fn = self.mean_fn_cls()\n",
    "        self.k = self.k_cls()\n",
    "        self.lik = self.lik_cls()\n",
    "        self.Xu = self.param('Xu', lambda k, s: self.Xu_initial,\n",
    "                             (self.n_inducing, self.d))\n",
    "\n",
    "        # len(Xu)=100, then 400 outputs ...\n",
    "        # In https://arxiv.org/pdf/1705.09862.pdf\n",
    "        #     variational mvn also parameterized by some kronecker structure !\n",
    "        # For now just assume its a large mvn where Σ models both input&output\n",
    "        self.q = VariationalMultivariateNormal(self.output_dim*self.n_inducing)\n",
    "\n",
    "    @classmethod\n",
    "    def get_init_params(self, model, key):\n",
    "        n = 1\n",
    "        Xs = np.ones((n, model.Xu_initial.shape[-1]))\n",
    "        ys = np.ones((n, model.output_dim))\n",
    "        params = model.init(key, (Xs, ys), method=model.mll)\n",
    "        return params\n",
    "\n",
    "    def mll(self, data):\n",
    "        X, y = data\n",
    "        k = self.k\n",
    "        Xu, μq, Lq = self.Xu, self.q.μ, self.q.L\n",
    "\n",
    "        mf = self.mean_fn(X)\n",
    "        mu = self.mean_fn(Xu)\n",
    "\n",
    "        Kff = k(X, full_cov=False)\n",
    "        Kuf = k(Xu, X)\n",
    "        Kuu = k(Xu)\n",
    "        Luu = cholesky_jitter(Kuu, jitter=5e-5)\n",
    "\n",
    "        α = self.n_data/len(X) \\\n",
    "            if self.n_data is not None else 1.\n",
    "\n",
    "        μqf, σ2qf = mvn_marginal_variational(Kff, Kuf, mf,\n",
    "                                             Luu, mu, μq, Lq, full_cov=False)\n",
    "        if isinstance(self.lik, LikMultipleNormal):\n",
    "            elbo_lik = α*self.lik.variational_log_prob(y, μqf, σ2qf, X[:, -1])\n",
    "        else:\n",
    "            elbo_lik = α*self.lik.variational_log_prob(y, μqf, σ2qf)\n",
    "        elbo_nkl = -kl_mvn_tril(μq, Lq, mu, Luu)\n",
    "        elbo = elbo_lik + elbo_nkl\n",
    "\n",
    "        return elbo, {'elbo_lik': elbo_lik,\n",
    "                      'elbo_nkl': elbo_nkl}\n",
    "\n",
    "    def pred_f(self, Xs, full_cov=True):\n",
    "        k = self.k\n",
    "        Xu, μq, Lq = self.Xu, self.q.μ, self.q.L\n",
    "\n",
    "        ms = self.mean_fn(Xs)\n",
    "        mu = self.mean_fn(Xu)\n",
    "\n",
    "        Kss = k(Xs, full_cov=full_cov)\n",
    "        Kus = k(Xu, Xs)\n",
    "        Kuu = k(Xu)\n",
    "        Luu = cholesky_jitter(Kuu, jitter=5e-5)\n",
    "\n",
    "        μf, Σf = mvn_marginal_variational(Kss, Kus, ms,\n",
    "                                          Luu, mu, μq, Lq, full_cov=full_cov)\n",
    "        return μf, Σf\n",
    "\n",
    "n_inducing=5\n",
    "key = jax.random.PRNGKey(0)\n",
    "model = SVGP(mean_fn_cls=MeanConstant,\n",
    "             k_cls=CovSE,\n",
    "             lik_cls=LikNormal,\n",
    "             n_data=n_train,\n",
    "             output_dim=1,\n",
    "             Xu_initial=X_train[:n_inducing])\n",
    "\n",
    "params = SVGP.get_init_params(model, jax.random.PRNGKey(0))\n",
    "bsz = 20\n",
    "datas = (X_train[:bsz], y_train[:bsz])\n",
    "model.apply(params, datas, method=model.mll)\n",
    "params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-tragedy",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "bsz = 20\n",
    "n_inducing = 50\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20,10), sharey=True)\n",
    "\n",
    "\n",
    "mean_fn_cls = partial(MeanConstant, init_val_m=0.)\n",
    "lik_cls = LikNormal\n",
    "k_cls = CovSE\n",
    "\n",
    "def get_model(i):\n",
    "    if i == 0:\n",
    "        return 'GPR', GPR(mean_fn_cls=mean_fn_cls,\n",
    "                          k_cls=k_cls,\n",
    "                          lik_cls=lik_cls,\n",
    "                          data=data)\n",
    "    if i == 1:\n",
    "        return f'GPR+FITC (m={n_inducing})', GPRFITC(data, n_inducing)\n",
    "    if i == 2:\n",
    "        return f'GPR+VFE (m={n_inducing})', VFE(data, n_inducing)\n",
    "    if i == 3:\n",
    "        return f'SVGP (m={n_inducing})', SVGP(mean_fn_cls=mean_fn_cls,\n",
    "                                              k_cls=k_cls,\n",
    "                                              lik_cls=lik_cls,\n",
    "                                              n_data=n_train,\n",
    "                                              output_dim=1,\n",
    "                                               Xu_initial=X_train[:n_inducing])\n",
    "    \n",
    "    \n",
    "def log_func(i, f, params):\n",
    "    if i%20==0:\n",
    "        print(f'[{i:3}]\\tLoss={f(params):.3f}\\t'\n",
    "              f'lik.σn={jax.nn.softplus(params[\"params\"][\"lik\"][\"σ2\"][0]):.3f}\\t'\n",
    "              f'k.ls={jax.nn.softplus(params[\"params\"][\"k\"][\"ls\"][0]):.3f}\\t'\n",
    "              f'k.σ2={jax.nn.softplus(params[\"params\"][\"k\"][\"σ2\"][0]):.3f}\\t'\n",
    "              + (f'Xu[:2]={params[\"params\"][\"Xu\"][:2,0]}' if 'Xu' in params[\"params\"] else ''))\n",
    "\n",
    "\n",
    "n_batches, batches = get_data_stream(\n",
    "    key, bsz, X_train, y_train)\n",
    "\n",
    "for i in [0,1,2,3]:\n",
    "    \n",
    "    name, model = get_model(i)\n",
    "    params = model.get_init_params(model, key)\n",
    "    if i != 0:\n",
    "        Xu_initial = params['params']['Xu']\n",
    "    \n",
    "    if i < 3:\n",
    "        nmll = lambda params: -model.apply(params, method=model.mll)\n",
    "        nmll = jax.jit(nmll)\n",
    "        num_steps = 400\n",
    "        params = pytree_mutate(params, {'params/k/ls': softplus_inv(np.array([0.5]))})\n",
    "        params = flax_run_optim(nmll, params, num_steps=num_steps, log_func=log_func,\n",
    "                                optimizer='GradientDescent', optimizer_kwargs={'learning_rate': .001})\n",
    "        mll = model.apply(params, method=model.mll)\n",
    "    else:\n",
    "        ######################################################\n",
    "        params = pytree_mutate(params, {'params/lik/σ2': softplus_inv(np.array([1.])),\n",
    "                                        'params/k/ls': softplus_inv(np.array([0.5]))})\n",
    "\n",
    "        @jax.jit\n",
    "        def train_step(step, opt, batch):\n",
    "            def f(params):\n",
    "                fx, aux = model.apply(params, batch, method=model.mll)\n",
    "                return -fx, aux\n",
    "            fg_fn = jax.value_and_grad(f, has_aux=True)\n",
    "            (fx, aux), grad = fg_fn(opt.target)\n",
    "            opt = opt.apply_gradient(grad)\n",
    "            log = {'step': step,\n",
    "                   'loss': fx,\n",
    "                   'lik.σ2': jax.nn.softplus(opt.target[\"params\"][\"lik\"][\"σ2\"][0]),\n",
    "                   'k.ls': jax.nn.softplus(opt.target[\"params\"][\"k\"][\"ls\"][0]),\n",
    "                   'k.σ2': jax.nn.softplus(opt.target[\"params\"][\"k\"][\"σ2\"][0]),\n",
    "                   'elbo_lik': aux['elbo_lik'],\n",
    "                   'elbo_nkl': aux['elbo_nkl']}\n",
    "            return opt, log\n",
    "        \n",
    "        num_steps = 1500\n",
    "        opt = flax_create_optimizer(params, 'Adam', {'learning_rate': .002})\n",
    "        for j in range(num_steps):\n",
    "            for k in range(n_batches):\n",
    "                step = j*n_batches+k\n",
    "                batch = next(batches)\n",
    "                opt, log = train_step(step, opt, batch)\n",
    "                if step%(20*n_batches)==0:\n",
    "                    print(f'[{log[\"step\"]:3}]\\t'\n",
    "                          f'Loss={log[\"loss\"]:.3f}\\t'\n",
    "                          f'lik.σ2={log[\"lik.σ2\"]:.3f}\\t'\n",
    "                          f'k.ls={log[\"k.ls\"]:.3f}\\t'\n",
    "                          f'k.σ2={log[\"k.σ2\"]:.3f}\\t'\n",
    "                          f'L=lik+nkl={log[\"elbo_lik\"]:.1f}+{log[\"elbo_nkl\"]:.1f}')\n",
    "\n",
    "        params = opt.target\n",
    "        mll, aux = model.apply(params, data, method=model.mll)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    μ, σ2 = model.apply(params, X_test, full_cov=False, method=model.pred_y)\n",
    "    std = np.sqrt(σ2).squeeze(); μ = μ.squeeze()\n",
    "\n",
    "    ax = axs[i//2, i%2]\n",
    "    ax.plot(X_test, μ, color='k')\n",
    "    ax.fill_between(X_test.squeeze(), μ-2*std, μ+2*std, alpha=.2, color=cmap(0))\n",
    "    ax.scatter(X_train, y_train, marker='x', color='r', s=50, alpha=.4)\n",
    "    if i != 0:\n",
    "        Xu = params['params']['Xu']\n",
    "        ax.plot(Xu_initial, np.ones_like(Xu)*ylim[1]-.04, \"k|\", mew=2, label=\"Inducing locations\")\n",
    "        ax.plot(Xu, np.ones_like(Xu)*ylim[0]+.04, \"k|\", mew=2, label=\"Inducing locations\")\n",
    "    ax.grid()\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_title(f'{name}: {\"mll\" if i <3 else \"elbo\"}={-mll:.2f}')\n",
    "    \n",
    "\n",
    "# fig.tight_layout()\n",
    "# plt_savefig(fig, 'summary/assets/plt_apx_comparison.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "μ.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mi_gp]",
   "language": "python",
   "name": "conda-env-mi_gp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
