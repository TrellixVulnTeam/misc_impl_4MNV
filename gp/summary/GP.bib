
@article{wilsonStochasticVariationalDeep2016,
	title = {Stochastic Variational Deep Kernel Learning},
	url = {https://arxiv.org/abs/1611.00336v2},
	abstract = {Deep kernel learning combines the non-parametric flexibility of kernel
methods with the inductive biases of deep learning architectures. We propose a
novel deep kernel learning model and stochastic variational inference procedure
which generalizes deep kernel learning approaches to enable classification,
multi-task learning, additive covariance structures, and stochastic gradient
training. Specifically, we apply additive base kernels to subsets of output
features from deep neural architectures, and jointly learn the parameters of
the base kernels and deep network through a Gaussian process marginal
likelihood objective. Within this framework, we derive an efficient form of
stochastic variational inference which leverages local kernel interpolation,
inducing points, and structure exploiting algebra. We show improved performance
over stand alone deep networks, {SVMs}, and state of the art scalable Gaussian
processes on several classification benchmarks, including an airline delay
dataset containing 6 million training points, {CIFAR}, and {ImageNet}.},
	author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
	urldate = {2020-12-28},
	date = {2016-11-01},
	langid = {english},
	keywords = {good, tbr},
	file = {Wilson et al_2016_Stochastic Variational Deep Kernel Learning.pdf:/Users/wpq/Dropbox (MIT)/zotero/Wilson et al_2016_Stochastic Variational Deep Kernel Learning.pdf:application/pdf}
}

@article{snelsonVariableNoiseDimensionality2012,
	title = {Variable noise and dimensionality reduction for sparse Gaussian processes},
	abstract = {The sparse pseudo-input Gaussian process ({SPGP}) is a new approximation method for speeding up {GP} regression in the case of a large number of data points N . The approximation is controlled by the gradient optimization of a small set of M ‘pseudoinputs’, thereby reducing complexity from O(N 3) to O(M 2N ). One limitation of the {SPGP} is that this optimization space becomes impractically big for high dimensional data sets. This paper addresses this limitation by performing automatic dimensionality reduction. A projection of the input space to a low dimensional space is learned in a supervised manner, alongside the pseudoinputs, which now live in this reduced space. The paper also investigates the suitability of the {SPGP} for modeling data with inputdependent noise. A further extension of the model is made to make it even more powerful in this regard – we learn an uncertainty parameter for each pseudo-input. The combination of sparsity, reduced dimension, and input-dependent noise makes it possible to apply {GPs} to much larger and more complex data sets than was previously practical. We demonstrate the beneﬁts of these methods on several synthetic and real world problems.},
	pages = {8},
	author = {Snelson, Edward and Ghahramani, Zoubin},
	date = {2012},
	langid = {english},
	keywords = {tbr},
	file = {Snelson_Ghahramani_2012_Variable noise and dimensionality reduction for sparse Gaussian processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Snelson_Ghahramani_2012_Variable noise and dimensionality reduction for sparse Gaussian processes.pdf:application/pdf}
}

@article{wilsonCopulaProcesses2010,
	title = {Copula Processes},
	abstract = {We deﬁne a copula process which describes the dependencies between arbitrarily many random variables independently of their marginal distributions. As an example, we develop a stochastic volatility model, Gaussian Copula Process Volatility ({GCPV}), to predict the latent standard deviations of a sequence of random variables. To make predictions we use Bayesian inference, with the Laplace approximation, and with Markov chain Monte Carlo as an alternative. We ﬁnd our model can outperform {GARCH} on simulated and ﬁnancial data. And unlike {GARCH}, {GCPV} can easily handle missing data, incorporate covariates other than time, and model a rich class of covariance structures.},
	pages = {9},
	author = {Wilson, Andrew and Ghahramani, Zoubin},
	date = {2010},
	langid = {english},
	keywords = {tbr},
	file = {Wilson and Ghahramani - Copula Processes.pdf:/Users/wpq/Zotero/storage/XNKS4LDR/Wilson and Ghahramani - Copula Processes.pdf:application/pdf}
}

@article{wilsonGaussianProcessRegression2012,
	title = {Gaussian Process Regression Networks},
	abstract = {We introduce a new regression framework, Gaussian process regression networks ({GPRN}), which combines the structural properties of Bayesian neural networks with the nonparametric ﬂexibility of Gaussian processes. {GPRN} accommodates input (predictor) dependent signal and noise correlations between multiple output (response) variables, input dependent length-scales and amplitudes, and heavy-tailed predictive distributions. We derive both elliptical slice sampling and variational Bayes inference procedures for {GPRN}. We apply {GPRN} as a multiple output regression and multivariate volatility model, demonstrating substantially improved performance over eight popular multiple output (multi-task) Gaussian process models and three multivariate volatility models on real datasets, including a 1000 dimensional gene expression dataset.},
	pages = {8},
	author = {Wilson, Andrew Gordon and Knowles, David A and Ghahramani, Zoubin},
	date = {2012},
	langid = {english},
	keywords = {tbr},
	file = {Wilson et al. - Gaussian Process Regression Networks.pdf:/Users/wpq/Zotero/storage/BR8RW2F6/Wilson et al. - Gaussian Process Regression Networks.pdf:application/pdf}
}

@article{wilsonGeneralisedWishartProcesses2011,
	title = {Generalised Wishart Processes},
	abstract = {We introduce a new stochastic process called the generalised Wishart process ({GWP}). It is a collection of positive semi-deﬁnite random matrices indexed by any arbitrary input variable. We use this process as a prior over dynamic (e.g. time varying) covariance matrices Σ(t). The {GWP} captures a diverse class of covariance dynamics, naturally handles missing data, scales nicely with dimension, has easily interpretable parameters, and can use input variables that include covariates other than time. We describe how to construct the {GWP}, introduce general procedures for inference and prediction, and show that it outperforms its main competitor, multivariate {GARCH}, even on ﬁnancial data that especially suits {GARCH}.},
	pages = {9},
	author = {Wilson, Andrew Gordon and Ghahramani, Zoubin},
	date = {2011},
	langid = {english},
	keywords = {tbr},
	file = {Wilson and Ghahramani - Generalised Wishart Processes.pdf:/Users/wpq/Zotero/storage/RYSG5673/Wilson and Ghahramani - Generalised Wishart Processes.pdf:application/pdf}
}

@article{wilsonGaussianProcessKernels2013a,
	title = {Gaussian Process Kernels for Pattern Discovery and Extrapolation},
	url = {http://arxiv.org/abs/1302.4245},
	abstract = {Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation. We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation. These kernels are derived by modelling a spectral density -- the Fourier transform of a kernel -- with a Gaussian mixture. The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic. We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric {CO}2 trends and airline passenger data. We also show that we can reconstruct standard covariances within our framework.},
	journaltitle = {{arXiv}:1302.4245 [cs, stat]},
	author = {Wilson, Andrew Gordon and Adams, Ryan Prescott},
	urldate = {2021-01-05},
	date = {2013-12-31},
	eprinttype = {arxiv},
	eprint = {1302.4245},
	note = {tex.ids: {wilsonGaussianProcessKernels}2013},
	keywords = {tbr},
	file = {Wilson_Adams_2013_Gaussian Process Kernels for Pattern Discovery and Extrapolation.pdf:/Users/wpq/Dropbox (MIT)/zotero/Wilson_Adams_2013_Gaussian Process Kernels for Pattern Discovery and Extrapolation.pdf:application/pdf}
}

@article{vanderwilkConvolutionalGaussianProcesses2017,
	title = {Convolutional Gaussian Processes},
	url = {http://arxiv.org/abs/1709.01894},
	abstract = {We present a practical way of introducing convolutional structure into Gaussian processes, making them more suited to high-dimensional inputs like images. The main contribution of our work is the construction of an inter-domain inducing point approximation that is well-tailored to the convolutional kernel. This allows us to gain the generalisation benefit of a convolutional kernel, together with fast but accurate posterior inference. We investigate several variations of the convolutional kernel, and apply it to {MNIST} and {CIFAR}-10, which have both been known to be challenging for Gaussian processes. We also show how the marginal likelihood can be used to find an optimal weighting between convolutional and {RBF} kernels to further improve performance. We hope that this illustration of the usefulness of a marginal likelihood will help automate discovering architectures in larger models.},
	journaltitle = {{arXiv}:1709.01894 [cs, stat]},
	author = {van der Wilk, Mark and Rasmussen, Carl Edward and Hensman, James},
	urldate = {2021-01-15},
	date = {2017-09-06},
	eprinttype = {arxiv},
	eprint = {1709.01894},
	keywords = {tbr},
	file = {van der Wilk et al_2017_Convolutional Gaussian Processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/van der Wilk et al_2017_Convolutional Gaussian Processes.pdf:application/pdf}
}

@article{futomaLearningDetectSepsis2017,
	title = {Learning to Detect Sepsis with a Multitask Gaussian Process {RNN} Classifier},
	abstract = {We present a scalable end-to-end classiﬁer that uses streaming physiological and medication data to accurately predict the onset of sepsis, a life-threatening complication from infections that has high mortality and morbidity. Our proposed framework models the multivariate trajectories of continuous-valued physiological time series using multitask Gaussian processes, seamlessly accounting for the high uncertainty, frequent missingness, and irregular sampling rates typically associated with real clinical data. The Gaussian process is directly connected to a black-box classiﬁer that predicts whether a patient will become septic, chosen in our case to be a recurrent neural network to account for the extreme variability in the length of patient encounters. We show how to scale the computations associated with the Gaussian process in a manner so that the entire system can be discriminatively trained end-to-end using backpropagation. In a large cohort of heterogeneous inpatient encounters at our university health system we ﬁnd that it outperforms several baselines at predicting sepsis, and yields 19.4\% and 55.5\% improved areas under the Receiver Operating Characteristic and Precision Recall curves as compared to the {NEWS} score currently used by our hospital.},
	pages = {9},
	author = {Futoma, Joseph and Hariharan, Sanjay and Heller, Katherine},
	date = {2017},
	langid = {english},
	keywords = {read},
	file = {Futoma et al. - Learning to Detect Sepsis with a Multitask Gaussia.pdf:/Users/wpq/Zotero/storage/AYENSMZR/Futoma et al. - Learning to Detect Sepsis with a Multitask Gaussia.pdf:application/pdf}
}

@article{snelsonSparseGaussianProcesses2005,
	title = {Sparse Gaussian Processes using Pseudo-inputs},
	volume = {18},
	url = {https://proceedings.neurips.cc/paper/2005/hash/4491777b1aa8b5b32c2e8666dbe1a495-Abstract.html},
	pages = {1257--1264},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Snelson, Edward and Ghahramani, Zoubin},
	urldate = {2021-02-05},
	date = {2005},
	langid = {english},
	keywords = {tbr},
	file = {Snelson_Ghahramani_2005_Sparse Gaussian Processes using Pseudo-inputs.pdf:/Users/wpq/Dropbox (MIT)/zotero/Snelson_Ghahramani_2005_Sparse Gaussian Processes using Pseudo-inputs.pdf:application/pdf}
}

@article{quinonero-candelaUnifyingViewSparse2005,
	title = {A Unifying View of Sparse Approximate Gaussian Process Regression},
	volume = {6},
	issn = {1532-4435},
	abstract = {We provide a new unifying view, including all existing proper probabilistic sparse approximations for Gaussian process regression. Our approach relies on expressing the effective prior which the methods are using. This allows new insights to be gained, and highlights the relationship between existing methods. It also allows for a clear theoretically justified ranking of the closeness of the known approximations to the corresponding full {GPs}. Finally we point directly to designs of new better sparse approximations, combining the best of the existing strategies, within attractive computational constraints.},
	pages = {1939--1959},
	journaltitle = {The Journal of Machine Learning Research},
	shortjournal = {J. Mach. Learn. Res.},
	author = {Quiñonero-Candela, Joaquin and Rasmussen, Carl Edward},
	date = {2005-12-01},
	keywords = {tbr},
	file = {Quinonero-Candela_Rasmussen_2005_A Unifying View of Sparse Approximate Gaussian Process Regression.pdf:/Users/wpq/Dropbox (MIT)/zotero/Quinonero-Candela_Rasmussen_2005_A Unifying View of Sparse Approximate Gaussian Process Regression.pdf:application/pdf}
}

@inproceedings{damianouDeepGaussianProcesses2013,
	title = {Deep Gaussian Processes},
	url = {http://proceedings.mlr.press/v31/damianou13a.html},
	abstract = {In this paper we introduce deep Gaussian process ({GP}) models. Deep {GPs} are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate {GP}. The inpu...},
	eventtitle = {Artificial Intelligence and Statistics},
	pages = {207--215},
	booktitle = {Artificial Intelligence and Statistics},
	publisher = {{PMLR}},
	author = {Damianou, Andreas and Lawrence, Neil D.},
	urldate = {2021-02-05},
	date = {2013-04-29},
	langid = {english},
	note = {{ISSN}: 1938-7228},
	keywords = {tbr},
	file = {Damianou_Lawrence_2013_Deep Gaussian Processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Damianou_Lawrence_2013_Deep Gaussian Processes.pdf:application/pdf}
}

@book{carledwardrasmussenGaussianProcessMachine2006,
	title = {Gaussian Process for Machine Learning},
	publisher = {{MIT} Press},
	author = {{Carl Edward Rasmussen} and Christopher, Williams},
	date = {2006},
	keywords = {good, read},
	file = {Carl Edward Rasmussen_Christopher_2006_Gaussian Process for Machine Learning.pdf:/Users/wpq/Dropbox (MIT)/zotero/Carl Edward Rasmussen_Christopher_2006_Gaussian Process for Machine Learning.pdf:application/pdf}
}

@article{hintonUsingDeepBelief2007,
	title = {Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes},
	volume = {20},
	url = {https://papers.nips.cc/paper/2007/hash/4b6538a44a1dfdc2b83477cd76dee98e-Abstract.html},
	pages = {1249--1256},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Hinton, Geoffrey E. and Salakhutdinov, Russ R.},
	urldate = {2021-02-05},
	date = {2007},
	langid = {english},
	keywords = {tbr},
	file = {Hinton_Salakhutdinov_2007_Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Hinton_Salakhutdinov_2007_Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes.pdf:application/pdf}
}

@article{wilsonEfficientlySamplingFunctions2020,
	title = {Efficiently Sampling Functions from Gaussian Process Posteriors},
	url = {http://arxiv.org/abs/2002.09309},
	abstract = {Gaussian processes are the gold standard for many real-world modeling problems, especially in cases where a model's success hinges upon its ability to faithfully represent predictive uncertainty. These problems typically exist as parts of larger frameworks, wherein quantities of interest are ultimately defined by integrating over posterior distributions. These quantities are frequently intractable, motivating the use of Monte Carlo methods. Despite substantial progress in scaling up Gaussian processes to large training sets, methods for accurately generating draws from their posterior distributions still scale cubically in the number of test locations. We identify a decomposition of Gaussian processes that naturally lends itself to scalable sampling by separating out the prior from the data. Building off of this factorization, we propose an easy-to-use and general-purpose approach for fast posterior sampling, which seamlessly pairs with sparse approximations to afford scalability both during training and at test time. In a series of experiments designed to test competing sampling schemes' statistical properties and practical ramifications, we demonstrate how decoupled sample paths accurately represent Gaussian process posteriors at a fraction of the usual cost.},
	journaltitle = {{arXiv}:2002.09309 [cs, stat]},
	author = {Wilson, James T. and Borovitskiy, Viacheslav and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc Peter},
	urldate = {2021-02-12},
	date = {2020-08-16},
	eprinttype = {arxiv},
	eprint = {2002.09309},
	keywords = {tbr},
	file = {Wilson et al_2020_Efficiently Sampling Functions from Gaussian Process Posteriors.pdf:/Users/wpq/Dropbox (MIT)/zotero/Wilson et al_2020_Efficiently Sampling Functions from Gaussian Process Posteriors.pdf:application/pdf}
}

@article{alaaBayesianInferenceIndividualized2017,
	title = {Bayesian Inference of Individualized Treatment Effects using Multi-task Gaussian Processes},
	url = {https://arxiv.org/abs/1704.02801v2},
	abstract = {Predicated on the increasing abundance of electronic health records, we
investi- gate the problem of inferring individualized treatment effects using
observational data. Stemming from the potential outcomes model, we propose a
novel multi- task learning framework in which factual and counterfactual
outcomes are mod- eled as the outputs of a function in a vector-valued
reproducing kernel Hilbert space ({vvRKHS}). We develop a nonparametric Bayesian
method for learning the treatment effects using a multi-task Gaussian process
({GP}) with a linear coregion- alization kernel as a prior over the {vvRKHS}. The
Bayesian approach allows us to compute individualized measures of confidence in
our estimates via pointwise credible intervals, which are crucial for realizing
the full potential of precision medicine. The impact of selection bias is
alleviated via a risk-based empirical Bayes method for adapting the multi-task
{GP} prior, which jointly minimizes the empirical error in factual outcomes and
the uncertainty in (unobserved) counter- factual outcomes. We conduct
experiments on observational datasets for an inter- ventional social program
applied to premature infants, and a left ventricular assist device applied to
cardiac patients wait-listed for a heart transplant. In both experi- ments, we
show that our method significantly outperforms the state-of-the-art.},
	author = {Alaa, Ahmed M. and van der Schaar, Mihaela},
	urldate = {2021-02-16},
	date = {2017-04-10},
	langid = {english},
	keywords = {tbr},
	file = {Alaa_van der Schaar_2017_Bayesian Inference of Individualized Treatment Effects using Multi-task Gaussian Processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Alaa_van der Schaar_2017_Bayesian Inference of Individualized Treatment Effects using Multi-task Gaussian Processes.pdf:application/pdf}
}

@article{swerskyMultiTaskBayesianOptimization2013a,
	title = {Multi-Task Bayesian Optimization},
	abstract = {Bayesian optimization has recently been proposed as a framework for automatically tuning the hyperparameters of machine learning models and has been shown to yield state-of-the-art performance with impressive ease and efﬁciency. In this paper, we explore whether it is possible to transfer the knowledge gained from previous optimizations to new tasks in order to ﬁnd optimal hyperparameter settings more efﬁciently. Our approach is based on extending multi-task Gaussian processes to the framework of Bayesian optimization. We show that this method signiﬁcantly speeds up the optimization process when compared to the standard single-task approach. We further propose a straightforward extension of our algorithm in order to jointly minimize the average error across multiple tasks and demonstrate how this can be used to greatly speed up k-fold cross-validation. Lastly, we propose an adaptation of a recently developed acquisition function, entropy search, to the cost-sensitive, multi-task setting. We demonstrate the utility of this new acquisition function by leveraging a small dataset to explore hyperparameter settings for a large dataset. Our algorithm dynamically chooses which dataset to query in order to yield the most information per unit cost.},
	pages = {10},
	author = {Swersky, Kevin and Snoek, Jasper and Adams, Ryan P},
	date = {2013},
	langid = {english},
	keywords = {tbr},
	file = {Swersky et al. - Multi-Task Bayesian Optimization.pdf:/Users/wpq/Zotero/storage/W7DGRVLW/Swersky et al. - Multi-Task Bayesian Optimization.pdf:application/pdf}
}

@article{alvarezKernelsVectorValuedFunctions2012,
	title = {Kernels for Vector-Valued Functions: a Review},
	url = {http://arxiv.org/abs/1106.6251},
	shorttitle = {Kernels for Vector-Valued Functions},
	abstract = {Kernel methods are among the most popular techniques in machine learning. From a frequentist/discriminative perspective they play a central role in regularization theory as they provide a natural choice for the hypotheses space and the regularization functional through the notion of reproducing kernel Hilbert spaces. From a Bayesian/generative perspective they are the key in the context of Gaussian processes, where the kernel function is also known as the covariance function. Traditionally, kernel methods have been used in supervised learning problem with scalar outputs and indeed there has been a considerable amount of work devoted to designing and learning kernels. More recently there has been an increasing interest in methods that deal with multiple outputs, motivated partly by frameworks like multitask learning. In this paper, we review different methods to design or learn valid kernel functions for multiple outputs, paying particular attention to the connection between probabilistic and functional methods.},
	journaltitle = {{arXiv}:1106.6251 [cs, math, stat]},
	author = {Alvarez, Mauricio A. and Rosasco, Lorenzo and Lawrence, Neil D.},
	urldate = {2021-02-16},
	date = {2012-04-16},
	eprinttype = {arxiv},
	eprint = {1106.6251},
	keywords = {tbr},
	file = {Alvarez et al_2012_Kernels for Vector-Valued Functions - a Review.pdf:/Users/wpq/Dropbox (MIT)/zotero/Alvarez et al_2012_Kernels for Vector-Valued Functions - a Review.pdf:application/pdf}
}

@article{zhangMaximumlikelihoodEstimationMultivariate2007,
	title = {Maximum-likelihood estimation for multivariate spatial linear coregionalization models},
	volume = {18},
	issn = {11804009, 1099095X},
	url = {http://doi.wiley.com/10.1002/env.807},
	doi = {10.1002/env.807},
	abstract = {A multivariate spatial linear coregionalization model is considered that incorporates the Mate´rn class of covariograms. An {EM} algorithm is developed for maximum-likelihood estimation that has a few desirable properties and is capable of handling high-dimensional data. Most estimates in the {EM} algorithm are updated through closed form expressions and these estimates automatically satisfy necessary constraints. The model and algorithm are illustrated through a real example. Copyright \# 2006 John Wiley \& Sons, Ltd.},
	pages = {125--139},
	number = {2},
	journaltitle = {Environmetrics},
	shortjournal = {Environmetrics},
	author = {Zhang, Hao},
	urldate = {2021-02-16},
	date = {2007-03},
	langid = {english},
	keywords = {tbr},
	file = {Zhang_2007_Maximum-likelihood estimation for multivariate spatial linear coregionalization models.pdf:/Users/wpq/Dropbox (MIT)/zotero/Zhang_2007_Maximum-likelihood estimation for multivariate spatial linear coregionalization models.pdf:application/pdf}
}

@incollection{gerigSpatiallyVaryingRegistration2014,
	location = {Cham},
	title = {Spatially Varying Registration Using Gaussian Processes},
	volume = {8674},
	isbn = {978-3-319-10469-0 978-3-319-10470-6},
	url = {http://link.springer.com/10.1007/978-3-319-10470-6_52},
	abstract = {In this paper we propose a new approach for spatially-varying registration using Gaussian process priors. The method is based on the idea of spectral tempering, i.e. the spectrum of the Gaussian process is modiﬁed depending on a user deﬁned tempering function. The result is a non-stationary Gaussian process, which induces diﬀerent amount of smoothness in diﬀerent areas. In contrast to most other schemes for spatially-varying registration, our approach does not require any change in the registration algorithm itself, but only aﬀects the prior model. Thus we can obtain spatially-varying versions of any registration method whose deformation prior can be formulated in terms of a Gaussian process. This includes for example most spline-based models, but also statistical shape or deformation models. We present results for the problem of atlas based skull-registration of cone beam {CT} images. These datasets are diﬃcult to register as they contain a large amount of noise around the teeth. We show that with our method we can become robust against noise, but still obtain accurate correspondence where the data is clean.},
	pages = {413--420},
	booktitle = {Medical Image Computing and Computer-Assisted Intervention – {MICCAI} 2014},
	publisher = {Springer International Publishing},
	author = {Gerig, Thomas and Shahim, Kamal and Reyes, Mauricio and Vetter, Thomas and Lüthi, Marcel},
	editor = {Golland, Polina and Hata, Nobuhiko and Barillot, Christian and Hornegger, Joachim and Howe, Robert},
	urldate = {2021-02-18},
	date = {2014},
	langid = {english},
	doi = {10.1007/978-3-319-10470-6_52},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {tbr},
	file = {Gerig et al_2014_Spatially Varying Registration Using Gaussian Processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gerig et al_2014_Spatially Varying Registration Using Gaussian Processes.pdf:application/pdf}
}

@article{lawrenceProbabilisticNonlinearPrincipal2005,
	title = {Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models},
	volume = {6},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v6/lawrence05a.html},
	pages = {1783--1816},
	number = {60},
	journaltitle = {Journal of Machine Learning Research},
	author = {Lawrence, Neil},
	urldate = {2021-02-19},
	date = {2005},
	keywords = {good, read},
	file = {Lawrence_2005_Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models.pdf:/Users/wpq/Dropbox (MIT)/zotero/Lawrence_2005_Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models.pdf:application/pdf}
}

@article{damianouVariationalInferenceLatent2016,
	title = {Variational Inference for Latent Variables and Uncertain Inputs in Gaussian Processes},
	volume = {17},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v17/damianou16a.html},
	pages = {1--62},
	number = {42},
	journaltitle = {Journal of Machine Learning Research},
	author = {Damianou, Andreas C. and Titsias, Michalis K. and Lawrence, Neil D.},
	urldate = {2021-02-19},
	date = {2016},
	keywords = {tbr},
	file = {Damianou et al_2016_Variational Inference for Latent Variables and Uncertain Inputs in Gaussian Processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Damianou et al_2016_Variational Inference for Latent Variables and Uncertain Inputs in Gaussian Processes.pdf:application/pdf}
}

@article{damianouSemidescribedSemisupervisedLearning2015,
	title = {Semi-described and semi-supervised learning with Gaussian processes},
	url = {http://arxiv.org/abs/1509.01168},
	abstract = {Propagating input uncertainty through non-linear Gaussian process ({GP}) mappings is intractable. This hinders the task of training {GPs} using uncertain and partially observed inputs. In this paper we refer to this task as "semi-described learning". We then introduce a {GP} framework that solves both, the semi-described and the semi-supervised learning problems (where missing values occur in the outputs). Auto-regressive state space simulation is also recognised as a special case of semi-described learning. To achieve our goal we develop variational methods for handling semi-described inputs in {GPs}, and couple them with algorithms that allow for imputing the missing values while treating the uncertainty in a principled, Bayesian manner. Extensive experiments on simulated and real-world data study the problems of iterative forecasting and regression/classification with missing values. The results suggest that the principled propagation of uncertainty stemming from our framework can significantly improve performance in these tasks.},
	journaltitle = {{arXiv}:1509.01168 [cs, math, stat]},
	author = {Damianou, Andreas and Lawrence, Neil D.},
	urldate = {2021-02-19},
	date = {2015-09-03},
	eprinttype = {arxiv},
	eprint = {1509.01168},
	keywords = {tbr},
	file = {Damianou_Lawrence_2015_Semi-described and semi-supervised learning with Gaussian processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Damianou_Lawrence_2015_Semi-described and semi-supervised learning with Gaussian processes.pdf:application/pdf}
}

@article{ghassemiMultivariateTimeseriesModeling2015,
	title = {A Multivariate Timeseries Modeling Approach to Severity of Illness Assessment and Forecasting in {ICU} with Sparse, Heterogeneous Clinical Data},
	volume = {2015},
	issn = {2159-5399},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4864016/},
	abstract = {The ability to determine patient acuity (or severity of illness) has immediate practical use for clinicians. We evaluate the use of multivariate timeseries modeling with the multi-task Gaussian process ({GP}) models using noisy, incomplete, sparse, heterogeneous and unevenly-sampled clinical data, including both physiological signals and clinical notes. The learned multi-task {GP} ({MTGP}) hyperparameters are then used to assess and forecast patient acuity. Experiments were conducted with two real clinical data sets acquired from {ICU} patients: firstly, estimating cerebrovascular pressure reactivity, an important indicator of secondary damage for traumatic brain injury patients, by learning the interactions between intracranial pressure and mean arterial blood pressure signals, and secondly, mortality prediction using clinical progress notes. In both cases, {MTGPs} provided improved results: an {MTGP} model provided better results than single-task {GP} models for signal interpolation and forecasting (0.91 vs 0.69 {RMSE}), and the use of {MTGP} hyperparameters obtained improved results when used as additional classification features (0.812 vs 0.788 {AUC}).},
	pages = {446--453},
	journaltitle = {Proceedings of the ... {AAAI} Conference on Artificial Intelligence. {AAAI} Conference on Artificial Intelligence},
	shortjournal = {Proc Conf {AAAI} Artif Intell},
	author = {Ghassemi, Marzyeh and Pimentel, Marco A.F. and Naumann, Tristan and Brennan, Thomas and Clifton, David A. and Szolovits, Peter and Feng, Mengling},
	urldate = {2021-02-20},
	date = {2015-01},
	pmid = {27182460},
	pmcid = {PMC4864016},
	keywords = {read},
	file = {Ghassemi et al_2015_A Multivariate Timeseries Modeling Approach to Severity of Illness Assessment and Forecasting in ICU with Sparse, Heterogeneous Clinical Data.pdf:/Users/wpq/Dropbox (MIT)/zotero/Ghassemi et al_2015_A Multivariate Timeseries Modeling Approach to Severity of Illness Assessment and Forecasting in ICU with Sparse, Heterogeneous Clinical Data.pdf:application/pdf}
}