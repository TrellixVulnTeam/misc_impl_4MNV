
@article{wilsonStochasticVariationalDeep2016,
	title = {Stochastic Variational Deep Kernel Learning},
	url = {https://arxiv.org/abs/1611.00336v2},
	abstract = {Deep kernel learning combines the non-parametric flexibility of kernel
methods with the inductive biases of deep learning architectures. We propose a
novel deep kernel learning model and stochastic variational inference procedure
which generalizes deep kernel learning approaches to enable classification,
multi-task learning, additive covariance structures, and stochastic gradient
training. Specifically, we apply additive base kernels to subsets of output
features from deep neural architectures, and jointly learn the parameters of
the base kernels and deep network through a Gaussian process marginal
likelihood objective. Within this framework, we derive an efficient form of
stochastic variational inference which leverages local kernel interpolation,
inducing points, and structure exploiting algebra. We show improved performance
over stand alone deep networks, {SVMs}, and state of the art scalable Gaussian
processes on several classification benchmarks, including an airline delay
dataset containing 6 million training points, {CIFAR}, and {ImageNet}.},
	author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
	urldate = {2020-12-28},
	date = {2016-11-01},
	langid = {english},
	keywords = {good, tbr},
	file = {Wilson et al_2016_Stochastic Variational Deep Kernel Learning.pdf:/Users/wpq/Dropbox (MIT)/zotero/Wilson et al_2016_Stochastic Variational Deep Kernel Learning.pdf:application/pdf}
}

@article{snelsonVariableNoiseDimensionality2012,
	title = {Variable noise and dimensionality reduction for sparse Gaussian processes},
	abstract = {The sparse pseudo-input Gaussian process ({SPGP}) is a new approximation method for speeding up {GP} regression in the case of a large number of data points N . The approximation is controlled by the gradient optimization of a small set of M ‘pseudoinputs’, thereby reducing complexity from O(N 3) to O(M 2N ). One limitation of the {SPGP} is that this optimization space becomes impractically big for high dimensional data sets. This paper addresses this limitation by performing automatic dimensionality reduction. A projection of the input space to a low dimensional space is learned in a supervised manner, alongside the pseudoinputs, which now live in this reduced space. The paper also investigates the suitability of the {SPGP} for modeling data with inputdependent noise. A further extension of the model is made to make it even more powerful in this regard – we learn an uncertainty parameter for each pseudo-input. The combination of sparsity, reduced dimension, and input-dependent noise makes it possible to apply {GPs} to much larger and more complex data sets than was previously practical. We demonstrate the beneﬁts of these methods on several synthetic and real world problems.},
	pages = {8},
	author = {Snelson, Edward and Ghahramani, Zoubin},
	date = {2012},
	langid = {english},
	keywords = {tbr},
	file = {Snelson_Ghahramani_2012_Variable noise and dimensionality reduction for sparse Gaussian processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Snelson_Ghahramani_2012_Variable noise and dimensionality reduction for sparse Gaussian processes.pdf:application/pdf}
}

@article{wilsonCopulaProcesses2010,
	title = {Copula Processes},
	abstract = {We deﬁne a copula process which describes the dependencies between arbitrarily many random variables independently of their marginal distributions. As an example, we develop a stochastic volatility model, Gaussian Copula Process Volatility ({GCPV}), to predict the latent standard deviations of a sequence of random variables. To make predictions we use Bayesian inference, with the Laplace approximation, and with Markov chain Monte Carlo as an alternative. We ﬁnd our model can outperform {GARCH} on simulated and ﬁnancial data. And unlike {GARCH}, {GCPV} can easily handle missing data, incorporate covariates other than time, and model a rich class of covariance structures.},
	pages = {9},
	author = {Wilson, Andrew and Ghahramani, Zoubin},
	date = {2010},
	langid = {english},
	keywords = {tbr},
	file = {Wilson and Ghahramani - Copula Processes.pdf:/Users/wpq/Zotero/storage/XNKS4LDR/Wilson and Ghahramani - Copula Processes.pdf:application/pdf}
}

@article{wilsonGaussianProcessRegression2012,
	title = {Gaussian Process Regression Networks},
	abstract = {We introduce a new regression framework, Gaussian process regression networks ({GPRN}), which combines the structural properties of Bayesian neural networks with the nonparametric ﬂexibility of Gaussian processes. {GPRN} accommodates input (predictor) dependent signal and noise correlations between multiple output (response) variables, input dependent length-scales and amplitudes, and heavy-tailed predictive distributions. We derive both elliptical slice sampling and variational Bayes inference procedures for {GPRN}. We apply {GPRN} as a multiple output regression and multivariate volatility model, demonstrating substantially improved performance over eight popular multiple output (multi-task) Gaussian process models and three multivariate volatility models on real datasets, including a 1000 dimensional gene expression dataset.},
	pages = {8},
	author = {Wilson, Andrew Gordon and Knowles, David A and Ghahramani, Zoubin},
	date = {2012},
	langid = {english},
	keywords = {read},
	file = {Wilson et al. - Gaussian Process Regression Networks.pdf:/Users/wpq/Zotero/storage/BR8RW2F6/Wilson et al. - Gaussian Process Regression Networks.pdf:application/pdf}
}

@article{wilsonGeneralisedWishartProcesses2011,
	title = {Generalised Wishart Processes},
	abstract = {We introduce a new stochastic process called the generalised Wishart process ({GWP}). It is a collection of positive semi-deﬁnite random matrices indexed by any arbitrary input variable. We use this process as a prior over dynamic (e.g. time varying) covariance matrices Σ(t). The {GWP} captures a diverse class of covariance dynamics, naturally handles missing data, scales nicely with dimension, has easily interpretable parameters, and can use input variables that include covariates other than time. We describe how to construct the {GWP}, introduce general procedures for inference and prediction, and show that it outperforms its main competitor, multivariate {GARCH}, even on ﬁnancial data that especially suits {GARCH}.},
	pages = {9},
	author = {Wilson, Andrew Gordon and Ghahramani, Zoubin},
	date = {2011},
	langid = {english},
	keywords = {tbr},
	file = {Wilson and Ghahramani - Generalised Wishart Processes.pdf:/Users/wpq/Zotero/storage/RYSG5673/Wilson and Ghahramani - Generalised Wishart Processes.pdf:application/pdf}
}

@article{vanderwilkConvolutionalGaussianProcesses2017,
	title = {Convolutional Gaussian Processes},
	url = {http://arxiv.org/abs/1709.01894},
	abstract = {We present a practical way of introducing convolutional structure into Gaussian processes, making them more suited to high-dimensional inputs like images. The main contribution of our work is the construction of an inter-domain inducing point approximation that is well-tailored to the convolutional kernel. This allows us to gain the generalisation benefit of a convolutional kernel, together with fast but accurate posterior inference. We investigate several variations of the convolutional kernel, and apply it to {MNIST} and {CIFAR}-10, which have both been known to be challenging for Gaussian processes. We also show how the marginal likelihood can be used to find an optimal weighting between convolutional and {RBF} kernels to further improve performance. We hope that this illustration of the usefulness of a marginal likelihood will help automate discovering architectures in larger models.},
	journaltitle = {{arXiv}:1709.01894 [cs, stat]},
	author = {van der Wilk, Mark and Rasmussen, Carl Edward and Hensman, James},
	urldate = {2021-01-15},
	date = {2017-09-06},
	eprinttype = {arxiv},
	eprint = {1709.01894},
	keywords = {read},
	file = {van der Wilk et al_2017_Convolutional Gaussian Processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/van der Wilk et al_2017_Convolutional Gaussian Processes.pdf:application/pdf}
}

@article{futomaLearningDetectSepsis2017,
	title = {Learning to Detect Sepsis with a Multitask Gaussian Process {RNN} Classifier},
	abstract = {We present a scalable end-to-end classiﬁer that uses streaming physiological and medication data to accurately predict the onset of sepsis, a life-threatening complication from infections that has high mortality and morbidity. Our proposed framework models the multivariate trajectories of continuous-valued physiological time series using multitask Gaussian processes, seamlessly accounting for the high uncertainty, frequent missingness, and irregular sampling rates typically associated with real clinical data. The Gaussian process is directly connected to a black-box classiﬁer that predicts whether a patient will become septic, chosen in our case to be a recurrent neural network to account for the extreme variability in the length of patient encounters. We show how to scale the computations associated with the Gaussian process in a manner so that the entire system can be discriminatively trained end-to-end using backpropagation. In a large cohort of heterogeneous inpatient encounters at our university health system we ﬁnd that it outperforms several baselines at predicting sepsis, and yields 19.4\% and 55.5\% improved areas under the Receiver Operating Characteristic and Precision Recall curves as compared to the {NEWS} score currently used by our hospital.},
	pages = {9},
	author = {Futoma, Joseph and Hariharan, Sanjay and Heller, Katherine},
	date = {2017},
	langid = {english},
	keywords = {read},
	file = {Futoma et al. - Learning to Detect Sepsis with a Multitask Gaussia.pdf:/Users/wpq/Zotero/storage/AYENSMZR/Futoma et al. - Learning to Detect Sepsis with a Multitask Gaussia.pdf:application/pdf}
}

@article{snelsonSparseGaussianProcesses2005,
	title = {Sparse Gaussian Processes using Pseudo-inputs},
	volume = {18},
	url = {https://proceedings.neurips.cc/paper/2005/hash/4491777b1aa8b5b32c2e8666dbe1a495-Abstract.html},
	pages = {1257--1264},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Snelson, Edward and Ghahramani, Zoubin},
	urldate = {2021-02-05},
	date = {2005},
	langid = {english},
	keywords = {read},
	file = {Snelson_Ghahramani_2005_Sparse Gaussian Processes using Pseudo-inputs.pdf:/Users/wpq/Dropbox (MIT)/zotero/Snelson_Ghahramani_2005_Sparse Gaussian Processes using Pseudo-inputs.pdf:application/pdf}
}

@article{quinonero-candelaUnifyingViewSparse2005,
	title = {A Unifying View of Sparse Approximate Gaussian Process Regression},
	volume = {6},
	issn = {1532-4435},
	abstract = {We provide a new unifying view, including all existing proper probabilistic sparse approximations for Gaussian process regression. Our approach relies on expressing the effective prior which the methods are using. This allows new insights to be gained, and highlights the relationship between existing methods. It also allows for a clear theoretically justified ranking of the closeness of the known approximations to the corresponding full {GPs}. Finally we point directly to designs of new better sparse approximations, combining the best of the existing strategies, within attractive computational constraints.},
	pages = {1939--1959},
	journaltitle = {The Journal of Machine Learning Research},
	shortjournal = {J. Mach. Learn. Res.},
	author = {Quiñonero-Candela, Joaquin and Rasmussen, Carl Edward},
	date = {2005-12-01},
	keywords = {good, read},
	file = {Quinonero-Candela_Rasmussen_2005_A Unifying View of Sparse Approximate Gaussian Process Regression.pdf:/Users/wpq/Dropbox (MIT)/zotero/Quinonero-Candela_Rasmussen_2005_A Unifying View of Sparse Approximate Gaussian Process Regression.pdf:application/pdf}
}

@inproceedings{damianouDeepGaussianProcesses2013,
	title = {Deep Gaussian Processes},
	url = {http://proceedings.mlr.press/v31/damianou13a.html},
	abstract = {In this paper we introduce deep Gaussian process ({GP}) models. Deep {GPs} are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate {GP}. The inpu...},
	eventtitle = {Artificial Intelligence and Statistics},
	pages = {207--215},
	booktitle = {Artificial Intelligence and Statistics},
	publisher = {{PMLR}},
	author = {Damianou, Andreas and Lawrence, Neil D.},
	urldate = {2021-02-05},
	date = {2013-04-29},
	langid = {english},
	note = {{ISSN}: 1938-7228},
	keywords = {tbr},
	file = {Damianou_Lawrence_2013_Deep Gaussian Processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Damianou_Lawrence_2013_Deep Gaussian Processes.pdf:application/pdf}
}

@book{carledwardrasmussenGaussianProcessMachine2006,
	title = {Gaussian Process for Machine Learning},
	publisher = {{MIT} Press},
	author = {{Carl Edward Rasmussen} and Christopher, Williams},
	date = {2006},
	keywords = {read},
	file = {Carl Edward Rasmussen_Christopher_2006_Gaussian Process for Machine Learning.pdf:/Users/wpq/Dropbox (MIT)/zotero/Carl Edward Rasmussen_Christopher_2006_Gaussian Process for Machine Learning.pdf:application/pdf}
}

@article{hintonUsingDeepBelief2007,
	title = {Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes},
	volume = {20},
	url = {https://papers.nips.cc/paper/2007/hash/4b6538a44a1dfdc2b83477cd76dee98e-Abstract.html},
	pages = {1249--1256},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Hinton, Geoffrey E. and Salakhutdinov, Russ R.},
	urldate = {2021-02-05},
	date = {2007},
	langid = {english},
	keywords = {read},
	file = {Hinton_Salakhutdinov_2007_Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Hinton_Salakhutdinov_2007_Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes.pdf:application/pdf}
}

@article{wilsonEfficientlySamplingFunctions2020,
	title = {Efficiently Sampling Functions from Gaussian Process Posteriors},
	url = {http://arxiv.org/abs/2002.09309},
	abstract = {Gaussian processes are the gold standard for many real-world modeling problems, especially in cases where a model's success hinges upon its ability to faithfully represent predictive uncertainty. These problems typically exist as parts of larger frameworks, wherein quantities of interest are ultimately defined by integrating over posterior distributions. These quantities are frequently intractable, motivating the use of Monte Carlo methods. Despite substantial progress in scaling up Gaussian processes to large training sets, methods for accurately generating draws from their posterior distributions still scale cubically in the number of test locations. We identify a decomposition of Gaussian processes that naturally lends itself to scalable sampling by separating out the prior from the data. Building off of this factorization, we propose an easy-to-use and general-purpose approach for fast posterior sampling, which seamlessly pairs with sparse approximations to afford scalability both during training and at test time. In a series of experiments designed to test competing sampling schemes' statistical properties and practical ramifications, we demonstrate how decoupled sample paths accurately represent Gaussian process posteriors at a fraction of the usual cost.},
	journaltitle = {{arXiv}:2002.09309 [cs, stat]},
	author = {Wilson, James T. and Borovitskiy, Viacheslav and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc Peter},
	urldate = {2021-02-12},
	date = {2020-08-16},
	eprinttype = {arxiv},
	eprint = {2002.09309},
	keywords = {tbr},
	file = {Wilson et al_2020_Efficiently Sampling Functions from Gaussian Process Posteriors.pdf:/Users/wpq/Dropbox (MIT)/zotero/Wilson et al_2020_Efficiently Sampling Functions from Gaussian Process Posteriors.pdf:application/pdf}
}

@article{alaaBayesianInferenceIndividualized2017,
	title = {Bayesian Inference of Individualized Treatment Effects using Multi-task Gaussian Processes},
	url = {https://arxiv.org/abs/1704.02801v2},
	abstract = {Predicated on the increasing abundance of electronic health records, we
investi- gate the problem of inferring individualized treatment effects using
observational data. Stemming from the potential outcomes model, we propose a
novel multi- task learning framework in which factual and counterfactual
outcomes are mod- eled as the outputs of a function in a vector-valued
reproducing kernel Hilbert space ({vvRKHS}). We develop a nonparametric Bayesian
method for learning the treatment effects using a multi-task Gaussian process
({GP}) with a linear coregion- alization kernel as a prior over the {vvRKHS}. The
Bayesian approach allows us to compute individualized measures of confidence in
our estimates via pointwise credible intervals, which are crucial for realizing
the full potential of precision medicine. The impact of selection bias is
alleviated via a risk-based empirical Bayes method for adapting the multi-task
{GP} prior, which jointly minimizes the empirical error in factual outcomes and
the uncertainty in (unobserved) counter- factual outcomes. We conduct
experiments on observational datasets for an inter- ventional social program
applied to premature infants, and a left ventricular assist device applied to
cardiac patients wait-listed for a heart transplant. In both experi- ments, we
show that our method significantly outperforms the state-of-the-art.},
	author = {Alaa, Ahmed M. and van der Schaar, Mihaela},
	urldate = {2021-02-16},
	date = {2017-04-10},
	langid = {english},
	keywords = {tbr},
	file = {Alaa_van der Schaar_2017_Bayesian Inference of Individualized Treatment Effects using Multi-task Gaussian Processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Alaa_van der Schaar_2017_Bayesian Inference of Individualized Treatment Effects using Multi-task Gaussian Processes.pdf:application/pdf}
}

@article{swerskyMultiTaskBayesianOptimization2013a,
	title = {Multi-Task Bayesian Optimization},
	abstract = {Bayesian optimization has recently been proposed as a framework for automatically tuning the hyperparameters of machine learning models and has been shown to yield state-of-the-art performance with impressive ease and efﬁciency. In this paper, we explore whether it is possible to transfer the knowledge gained from previous optimizations to new tasks in order to ﬁnd optimal hyperparameter settings more efﬁciently. Our approach is based on extending multi-task Gaussian processes to the framework of Bayesian optimization. We show that this method signiﬁcantly speeds up the optimization process when compared to the standard single-task approach. We further propose a straightforward extension of our algorithm in order to jointly minimize the average error across multiple tasks and demonstrate how this can be used to greatly speed up k-fold cross-validation. Lastly, we propose an adaptation of a recently developed acquisition function, entropy search, to the cost-sensitive, multi-task setting. We demonstrate the utility of this new acquisition function by leveraging a small dataset to explore hyperparameter settings for a large dataset. Our algorithm dynamically chooses which dataset to query in order to yield the most information per unit cost.},
	pages = {10},
	author = {Swersky, Kevin and Snoek, Jasper and Adams, Ryan P},
	date = {2013},
	langid = {english},
	note = {tex.ids= {swerskyMultiTaskBayesianOptimization}2013},
	keywords = {tbr},
	file = {Swersky et al. - Multi-Task Bayesian Optimization.pdf:/Users/wpq/Zotero/storage/UX4D8FQJ/Swersky et al. - Multi-Task Bayesian Optimization.pdf:application/pdf;Swersky et al. - Multi-Task Bayesian Optimization.pdf:/Users/wpq/Zotero/storage/W7DGRVLW/Swersky et al. - Multi-Task Bayesian Optimization.pdf:application/pdf}
}

@article{alvarezKernelsVectorValuedFunctions2012,
	title = {Kernels for Vector-Valued Functions: a Review},
	url = {http://arxiv.org/abs/1106.6251},
	shorttitle = {Kernels for Vector-Valued Functions},
	abstract = {Kernel methods are among the most popular techniques in machine learning. From a frequentist/discriminative perspective they play a central role in regularization theory as they provide a natural choice for the hypotheses space and the regularization functional through the notion of reproducing kernel Hilbert spaces. From a Bayesian/generative perspective they are the key in the context of Gaussian processes, where the kernel function is also known as the covariance function. Traditionally, kernel methods have been used in supervised learning problem with scalar outputs and indeed there has been a considerable amount of work devoted to designing and learning kernels. More recently there has been an increasing interest in methods that deal with multiple outputs, motivated partly by frameworks like multitask learning. In this paper, we review different methods to design or learn valid kernel functions for multiple outputs, paying particular attention to the connection between probabilistic and functional methods.},
	journaltitle = {{arXiv}:1106.6251 [cs, math, stat]},
	author = {Alvarez, Mauricio A. and Rosasco, Lorenzo and Lawrence, Neil D.},
	urldate = {2021-02-16},
	date = {2012-04-16},
	eprinttype = {arxiv},
	eprint = {1106.6251},
	keywords = {tbr},
	file = {Alvarez et al_2012_Kernels for Vector-Valued Functions - a Review.pdf:/Users/wpq/Dropbox (MIT)/zotero/Alvarez et al_2012_Kernels for Vector-Valued Functions - a Review.pdf:application/pdf}
}

@article{zhangMaximumlikelihoodEstimationMultivariate2007,
	title = {Maximum-likelihood estimation for multivariate spatial linear coregionalization models},
	volume = {18},
	issn = {11804009, 1099095X},
	url = {http://doi.wiley.com/10.1002/env.807},
	doi = {10.1002/env.807},
	abstract = {A multivariate spatial linear coregionalization model is considered that incorporates the Mate´rn class of covariograms. An {EM} algorithm is developed for maximum-likelihood estimation that has a few desirable properties and is capable of handling high-dimensional data. Most estimates in the {EM} algorithm are updated through closed form expressions and these estimates automatically satisfy necessary constraints. The model and algorithm are illustrated through a real example. Copyright \# 2006 John Wiley \& Sons, Ltd.},
	pages = {125--139},
	number = {2},
	journaltitle = {Environmetrics},
	shortjournal = {Environmetrics},
	author = {Zhang, Hao},
	urldate = {2021-02-16},
	date = {2007-03},
	langid = {english},
	keywords = {tbr},
	file = {Zhang_2007_Maximum-likelihood estimation for multivariate spatial linear coregionalization models.pdf:/Users/wpq/Dropbox (MIT)/zotero/Zhang_2007_Maximum-likelihood estimation for multivariate spatial linear coregionalization models.pdf:application/pdf}
}

@incollection{gerigSpatiallyVaryingRegistration2014,
	location = {Cham},
	title = {Spatially Varying Registration Using Gaussian Processes},
	volume = {8674},
	isbn = {978-3-319-10469-0 978-3-319-10470-6},
	url = {http://link.springer.com/10.1007/978-3-319-10470-6_52},
	abstract = {In this paper we propose a new approach for spatially-varying registration using Gaussian process priors. The method is based on the idea of spectral tempering, i.e. the spectrum of the Gaussian process is modiﬁed depending on a user deﬁned tempering function. The result is a non-stationary Gaussian process, which induces diﬀerent amount of smoothness in diﬀerent areas. In contrast to most other schemes for spatially-varying registration, our approach does not require any change in the registration algorithm itself, but only aﬀects the prior model. Thus we can obtain spatially-varying versions of any registration method whose deformation prior can be formulated in terms of a Gaussian process. This includes for example most spline-based models, but also statistical shape or deformation models. We present results for the problem of atlas based skull-registration of cone beam {CT} images. These datasets are diﬃcult to register as they contain a large amount of noise around the teeth. We show that with our method we can become robust against noise, but still obtain accurate correspondence where the data is clean.},
	pages = {413--420},
	booktitle = {Medical Image Computing and Computer-Assisted Intervention – {MICCAI} 2014},
	publisher = {Springer International Publishing},
	author = {Gerig, Thomas and Shahim, Kamal and Reyes, Mauricio and Vetter, Thomas and Lüthi, Marcel},
	editor = {Golland, Polina and Hata, Nobuhiko and Barillot, Christian and Hornegger, Joachim and Howe, Robert},
	urldate = {2021-02-18},
	date = {2014},
	langid = {english},
	doi = {10.1007/978-3-319-10470-6_52},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {tbr},
	file = {Gerig et al_2014_Spatially Varying Registration Using Gaussian Processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gerig et al_2014_Spatially Varying Registration Using Gaussian Processes.pdf:application/pdf}
}

@article{lawrenceProbabilisticNonlinearPrincipal2005,
	title = {Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models},
	volume = {6},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v6/lawrence05a.html},
	pages = {1783--1816},
	number = {60},
	journaltitle = {Journal of Machine Learning Research},
	author = {Lawrence, Neil},
	urldate = {2021-02-19},
	date = {2005},
	keywords = {good, read},
	file = {Lawrence_2005_Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models.pdf:/Users/wpq/Dropbox (MIT)/zotero/Lawrence_2005_Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models.pdf:application/pdf}
}

@article{damianouVariationalInferenceLatent2016,
	title = {Variational Inference for Latent Variables and Uncertain Inputs in Gaussian Processes},
	volume = {17},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v17/damianou16a.html},
	pages = {1--62},
	number = {42},
	journaltitle = {Journal of Machine Learning Research},
	author = {Damianou, Andreas C. and Titsias, Michalis K. and Lawrence, Neil D.},
	urldate = {2021-02-19},
	date = {2016},
	keywords = {tbr},
	file = {Damianou et al_2016_Variational Inference for Latent Variables and Uncertain Inputs in Gaussian Processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Damianou et al_2016_Variational Inference for Latent Variables and Uncertain Inputs in Gaussian Processes.pdf:application/pdf}
}

@article{damianouSemidescribedSemisupervisedLearning2015,
	title = {Semi-described and semi-supervised learning with Gaussian processes},
	url = {http://arxiv.org/abs/1509.01168},
	abstract = {Propagating input uncertainty through non-linear Gaussian process ({GP}) mappings is intractable. This hinders the task of training {GPs} using uncertain and partially observed inputs. In this paper we refer to this task as "semi-described learning". We then introduce a {GP} framework that solves both, the semi-described and the semi-supervised learning problems (where missing values occur in the outputs). Auto-regressive state space simulation is also recognised as a special case of semi-described learning. To achieve our goal we develop variational methods for handling semi-described inputs in {GPs}, and couple them with algorithms that allow for imputing the missing values while treating the uncertainty in a principled, Bayesian manner. Extensive experiments on simulated and real-world data study the problems of iterative forecasting and regression/classification with missing values. The results suggest that the principled propagation of uncertainty stemming from our framework can significantly improve performance in these tasks.},
	journaltitle = {{arXiv}:1509.01168 [cs, math, stat]},
	author = {Damianou, Andreas and Lawrence, Neil D.},
	urldate = {2021-02-19},
	date = {2015-09-03},
	eprinttype = {arxiv},
	eprint = {1509.01168},
	keywords = {tbr},
	file = {Damianou_Lawrence_2015_Semi-described and semi-supervised learning with Gaussian processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Damianou_Lawrence_2015_Semi-described and semi-supervised learning with Gaussian processes.pdf:application/pdf}
}

@article{ghassemiMultivariateTimeseriesModeling2015,
	title = {A Multivariate Timeseries Modeling Approach to Severity of Illness Assessment and Forecasting in {ICU} with Sparse, Heterogeneous Clinical Data},
	volume = {2015},
	issn = {2159-5399},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4864016/},
	abstract = {The ability to determine patient acuity (or severity of illness) has immediate practical use for clinicians. We evaluate the use of multivariate timeseries modeling with the multi-task Gaussian process ({GP}) models using noisy, incomplete, sparse, heterogeneous and unevenly-sampled clinical data, including both physiological signals and clinical notes. The learned multi-task {GP} ({MTGP}) hyperparameters are then used to assess and forecast patient acuity. Experiments were conducted with two real clinical data sets acquired from {ICU} patients: firstly, estimating cerebrovascular pressure reactivity, an important indicator of secondary damage for traumatic brain injury patients, by learning the interactions between intracranial pressure and mean arterial blood pressure signals, and secondly, mortality prediction using clinical progress notes. In both cases, {MTGPs} provided improved results: an {MTGP} model provided better results than single-task {GP} models for signal interpolation and forecasting (0.91 vs 0.69 {RMSE}), and the use of {MTGP} hyperparameters obtained improved results when used as additional classification features (0.812 vs 0.788 {AUC}).},
	pages = {446--453},
	journaltitle = {Proceedings of the ... {AAAI} Conference on Artificial Intelligence. {AAAI} Conference on Artificial Intelligence},
	shortjournal = {Proc Conf {AAAI} Artif Intell},
	author = {Ghassemi, Marzyeh and Pimentel, Marco A.F. and Naumann, Tristan and Brennan, Thomas and Clifton, David A. and Szolovits, Peter and Feng, Mengling},
	urldate = {2021-02-20},
	date = {2015-01},
	pmid = {27182460},
	pmcid = {PMC4864016},
	keywords = {read},
	file = {Ghassemi et al_2015_A Multivariate Timeseries Modeling Approach to Severity of Illness Assessment and Forecasting in ICU with Sparse, Heterogeneous Clinical Data.pdf:/Users/wpq/Dropbox (MIT)/zotero/Ghassemi et al_2015_A Multivariate Timeseries Modeling Approach to Severity of Illness Assessment and Forecasting in ICU with Sparse, Heterogeneous Clinical Data.pdf:application/pdf}
}

@inproceedings{titsiasVariationalLearningInducing2009,
	title = {Variational Learning of Inducing Variables in Sparse Gaussian Processes},
	url = {http://proceedings.mlr.press/v5/titsias09a.html},
	abstract = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximat...},
	eventtitle = {Artificial Intelligence and Statistics},
	pages = {567--574},
	booktitle = {Artificial Intelligence and Statistics},
	publisher = {{PMLR}},
	author = {Titsias, Michalis},
	urldate = {2021-02-21},
	date = {2009-04-15},
	langid = {english},
	note = {{ISSN}: 1938-7228},
	keywords = {good, read},
	file = {Titsias_2009_Variational Learning of Inducing Variables in Sparse Gaussian Processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Titsias_2009_Variational Learning of Inducing Variables in Sparse Gaussian Processes.pdf:application/pdf}
}

@article{vanderwilkFrameworkInterdomainMultioutput2020,
	title = {A Framework for Interdomain and Multioutput Gaussian Processes},
	url = {http://arxiv.org/abs/2003.01115},
	abstract = {One obstacle to the use of Gaussian processes ({GPs}) in large-scale problems, and as a component in deep learning system, is the need for bespoke derivations and implementations for small variations in the model or inference. In order to improve the utility of {GPs} we need a modular system that allows rapid implementation and testing, as seen in the neural network community. We present a mathematical and software framework for scalable approximate inference in {GPs}, which combines interdomain approximations and multiple outputs. Our framework, implemented in {GPflow}, provides a unified interface for many existing multioutput models, as well as more recent convolutional structures. This simplifies the creation of deep models with {GPs}, and we hope that this work will encourage more interest in this approach.},
	journaltitle = {{arXiv}:2003.01115 [cs, stat]},
	author = {van der Wilk, Mark and Dutordoir, Vincent and John, S. T. and Artemev, Artem and Adam, Vincent and Hensman, James},
	urldate = {2021-02-21},
	date = {2020-03-02},
	eprinttype = {arxiv},
	eprint = {2003.01115},
	keywords = {tbr},
	file = {van der Wilk et al_2020_A Framework for Interdomain and Multioutput Gaussian Processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/van der Wilk et al_2020_A Framework for Interdomain and Multioutput Gaussian Processes.pdf:application/pdf}
}

@article{rakitschItAllNoise2013,
	title = {It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper/2013/hash/59c33016884a62116be975a9bb8257e3-Abstract.html},
	shorttitle = {It is all in the noise},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Rakitsch, Barbara and Lippert, Christoph and Borgwardt, Karsten and Stegle, Oliver},
	urldate = {2021-02-22},
	date = {2013},
	langid = {english},
	keywords = {tbr},
	file = {Rakitsch et al_2013_It is all in the noise - Efficient multi-task Gaussian process inference with structured residuals.pdf:/Users/wpq/Dropbox (MIT)/zotero/Rakitsch et al_2013_It is all in the noise - Efficient multi-task Gaussian process inference with structured residuals.pdf:application/pdf}
}

@article{dallaireApproximateInferenceGaussian2011,
	title = {An approximate inference with Gaussian process to latent functions from uncertain data},
	volume = {74},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231211000440},
	doi = {10.1016/j.neucom.2010.09.024},
	series = {Adaptive Incremental Learning in Neural Networks},
	abstract = {Most formulations of supervised learning are often based on the assumption that only the outputs data are uncertain. However, this assumption might be too strong for some learning tasks. This paper investigates the use of Gaussian processes to infer latent functions from a set of uncertain input–output examples. By assuming Gaussian distributions with known variances over the inputs–outputs and using the expectation of the covariance function, it is possible to analytically compute the expected covariance matrix of the data to obtain a posterior distribution over functions. The method is evaluated on a synthetic problem and on a more realistic one, which consist in learning the dynamics of a cart–pole balancing task. The results indicate an improvement of the mean squared error and the likelihood of the posterior Gaussian process when the data uncertainty is significant.},
	pages = {1945--1955},
	number = {11},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Dallaire, Patrick and Besse, Camille and Chaib-draa, Brahim},
	urldate = {2021-02-23},
	date = {2011-05-01},
	langid = {english},
	keywords = {tbr},
	file = {Dallaire et al_2011_An approximate inference with Gaussian process to latent functions from uncertain data.pdf:/Users/wpq/Dropbox (MIT)/zotero/Dallaire et al_2011_An approximate inference with Gaussian process to latent functions from uncertain data.pdf:application/pdf}
}

@article{girardGaussianProcessPriors2002,
	title = {Gaussian Process Priors with Uncertain Inputs Application to Multiple-Step Ahead Time Series Forecasting},
	pages = {8},
	author = {Girard, Agathe and Rasmussen, Carl Edward and Candela, Joaquin Quiñonero and Murray-Smith, Roderick},
	date = {2002},
	langid = {english},
	keywords = {good, read},
	file = {Girard et al_2002_Gaussian Process Priors with Uncertain Inputs Application to Multiple-Step Ahead Time Series Forecasting.pdf:/Users/wpq/Dropbox (MIT)/zotero/Girard et al_2002_Gaussian Process Priors with Uncertain Inputs Application to Multiple-Step Ahead Time Series Forecasting.pdf:application/pdf}
}

@article{rasmussenEvaluationGaussianProcess1996,
	title = {Evaluation of Gaussian Process and Other Methods for Non-linear Regression},
	abstract = {This thesis develops two Bayesian learning methods relying on Gaussian processes and a rigorous statistical approach for evaluating such methods. In these experimental designs the sources of uncertainty in the estimated generalisation performances due to both variation in training and test sets are accounted for. The framework allows for estimation of generalisation performance as well as statistical tests of signiﬁcance for pairwise comparisons. Two experimental designs are recommended and supported by the {DELVE} software environment.},
	pages = {138},
	author = {Rasmussen, Carl Edward},
	date = {1996},
	langid = {english},
	keywords = {tbr},
	file = {Rasmussen - EVALUATION OF GAUSSIAN PROCESSES AND OTHER METHODS.pdf:/Users/wpq/Zotero/storage/3FGBC8G9/Rasmussen - EVALUATION OF GAUSSIAN PROCESSES AND OTHER METHODS.pdf:application/pdf}
}

@inproceedings{titsiasBayesianGaussianProcess2010,
	title = {Bayesian Gaussian Process Latent Variable Model},
	url = {http://proceedings.mlr.press/v9/titsias10a.html},
	abstract = {We introduce a variational inference framework for training the Gaussian process latent variable model and thus performing Bayesian nonlinear dimensionality reduction. This method allows us to vari...},
	eventtitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	pages = {844--851},
	booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	publisher = {{JMLR} Workshop and Conference Proceedings},
	author = {Titsias, Michalis and Lawrence, Neil D.},
	urldate = {2021-02-23},
	date = {2010-03-31},
	langid = {english},
	note = {{ISSN}: 1938-7228},
	keywords = {good, tbr},
	file = {Titsias_Lawrence_2010_Bayesian Gaussian Process Latent Variable Model.pdf:/Users/wpq/Dropbox (MIT)/zotero/Titsias_Lawrence_2010_Bayesian Gaussian Process Latent Variable Model.pdf:application/pdf}
}

@article{mchutchonGaussianProcessTraining2011,
	title = {Gaussian Process Training with Input Noise},
	abstract = {In standard Gaussian Process regression input locations are assumed to be noise free. We present a simple yet effective {GP} model for training on input points corrupted by i.i.d. Gaussian noise. To make computations tractable we use a local linear expansion about each input point. This allows the input noise to be recast as output noise proportional to the squared gradient of the {GP} posterior mean. The input noise variances are inferred from the data as extra hyperparameters. They are trained alongside other hyperparameters by the usual method of maximisation of the marginal likelihood. Training uses an iterative scheme, which alternates between optimising the hyperparameters and calculating the posterior gradient. Analytic predictive moments can then be found for Gaussian distributed test points. We compare our model to others over a range of different regression problems and show that it improves over current methods.},
	pages = {9},
	author = {Mchutchon, Andrew and Rasmussen, Carl E},
	date = {2011},
	langid = {english},
	keywords = {tbr},
	file = {Mchutchon and Rasmussen - Gaussian Process Training with Input Noise.pdf:/Users/wpq/Zotero/storage/63TDYBMP/Mchutchon and Rasmussen - Gaussian Process Training with Input Noise.pdf:application/pdf}
}

@article{girardLearningGaussianProcess2003,
	title = {Learning a Gaussian Process Model with Uncertain Inputs},
	abstract = {Learning with uncertain inputs is well-known to be a difﬁcult task. In order to achieve this analytically using a Gaussian Process prior model, we expand the original process around the input mean (Delta method), assuming the random input is normally distributed. We thus derive a new process whose covariance function accounts for the randomness of the input. We illustrate the effectiveness of the proposed model on a simple static simulation example and on the modelling of a nonlinear noisy time-series.},
	pages = {10},
	author = {Girard, Agathe and Murray-Smith, Roderick},
	date = {2003},
	langid = {english},
	keywords = {tbr},
	file = {Girard and Murray-Smith - Learning a Gaussian Process Model with Uncertain I.pdf:/Users/wpq/Zotero/storage/6ZW5JLCF/Girard and Murray-Smith - Learning a Gaussian Process Model with Uncertain I.pdf:application/pdf}
}

@article{hensmanScalableVariationalGaussian2014,
	title = {Scalable Variational Gaussian Process Classification},
	url = {http://arxiv.org/abs/1411.2005},
	abstract = {Gaussian process classification is a popular method with a number of appealing properties. We show how to scale the model within a variational inducing point framework, outperforming the state of the art on benchmark datasets. Importantly, the variational formulation can be exploited to allow classification in problems with millions of data points, as we demonstrate in experiments.},
	journaltitle = {{arXiv}:1411.2005 [stat]},
	author = {Hensman, James and Matthews, Alex and Ghahramani, Zoubin},
	urldate = {2021-02-23},
	date = {2014-11-07},
	eprinttype = {arxiv},
	eprint = {1411.2005},
	keywords = {tbr},
	file = {Hensman et al_2014_Scalable Variational Gaussian Process Classification.pdf:/Users/wpq/Dropbox (MIT)/zotero/Hensman et al_2014_Scalable Variational Gaussian Process Classification.pdf:application/pdf}
}

@article{wilsonGaussianProcessKernels2013,
	title = {Gaussian Process Kernels for Pattern Discovery and Extrapolation},
	abstract = {Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation. We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation. These kernels are derived by modelling a spectral density – the Fourier transform of a kernel – with a Gaussian mixture. The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic. We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric {CO}2 trends and airline passenger data. We also show that it is possible to reconstruct several popular standard covariances within our framework.},
	pages = {9},
	author = {Wilson, Andrew Gordon and Adams, Ryan Prescott},
	date = {2013},
	langid = {english},
	keywords = {read},
	file = {Wilson_Adams_2013_Gaussian Process Kernels for Pattern Discovery and Extrapolation.pdf:/Users/wpq/Dropbox (MIT)/zotero/Wilson_Adams_2013_Gaussian Process Kernels for Pattern Discovery and Extrapolation3.pdf:application/pdf}
}

@inproceedings{calandraManifoldGaussianProcesses2016,
	title = {Manifold Gaussian Processes for regression},
	doi = {10.1109/IJCNN.2016.7727626},
	abstract = {Off-the-shelf Gaussian Process ({GP}) covariance functions encode smoothness assumptions on the structure of the function to be modeled. To model complex and non-differentiable functions, these smoothness assumptions are often too restrictive. One way to alleviate this limitation is to find a different representation of the data by introducing a feature space. This feature space is often learned in an unsupervised way, which might lead to data representations that are not useful for the overall regression task. In this paper, we propose Manifold Gaussian Processes, a novel supervised method that jointly learns a transformation of the data into a feature space and a {GP} regression from the feature space to observed space. The Manifold {GP} is a full {GP} and allows to learn data representations, which are useful for the overall regression task. As a proof-of-concept, we evaluate our approach on complex non-smooth functions where standard {GPs} perform poorly, such as step functions and robotics tasks with contacts.},
	eventtitle = {2016 International Joint Conference on Neural Networks ({IJCNN})},
	pages = {3338--3345},
	booktitle = {2016 International Joint Conference on Neural Networks ({IJCNN})},
	author = {Calandra, R. and Peters, J. and Rasmussen, C. E. and Deisenroth, M. P.},
	date = {2016-07},
	note = {{ISSN}: 2161-4407},
	keywords = {tbr},
	file = {Calandra et al_2016_Manifold Gaussian Processes for regression.pdf:/Users/wpq/Dropbox (MIT)/zotero/Calandra et al_2016_Manifold Gaussian Processes for regression.pdf:application/pdf}
}

@article{wilsonKernelInterpolationScalable2015,
	title = {Kernel Interpolation for Scalable Structured Gaussian Processes ({KISS}-{GP})},
	abstract = {We introduce a new structured kernel interpolation ({SKI}) framework, which generalises and uniﬁes inducing point methods for scalable Gaussian processes ({GPs}). {SKI} methods produce kernel approximations for fast computations through kernel interpolation. The {SKI} framework clariﬁes how the quality of an inducing point approach depends on the number of inducing (aka interpolation) points, interpolation strategy, and {GP} covariance kernel. {SKI} also provides a mechanism to create new scalable kernel methods, through choosing different kernel interpolation strategies. Using {SKI}, with local cubic kernel interpolation, we introduce {KISSGP}, which is 1) more scalable than inducing point alternatives, 2) naturally enables Kronecker and Toeplitz algebra for substantial additional gains in scalability, without requiring any grid data, and 3) can be used for fast and expressive kernel learning. {KISS}-{GP} costs O(n) time and storage for {GP} inference. We evaluate {KISS}-{GP} for kernel matrix approximation, kernel learning, and natural sound modelling.},
	pages = {10},
	author = {Wilson, Andrew Gordon and Nickisch, Hannes},
	date = {2015},
	langid = {english},
	keywords = {tbr},
	file = {Wilson and Nickisch - Kernel Interpolation for Scalable Structured Gauss.pdf:/Users/wpq/Zotero/storage/DMZHHWTN/Wilson and Nickisch - Kernel Interpolation for Scalable Structured Gauss.pdf:application/pdf}
}

@article{grochowStyleBasedInverseKinematics2004,
	title = {Style-Based Inverse Kinematics},
	abstract = {This paper presents an inverse kinematics system based on a learned model of human poses. Given a set of constraints, our system can produce the most likely pose satisfying those constraints, in realtime. Training the model on different input data leads to different styles of {IK}. The model is represented as a probability distribution over the space of all possible poses. This means that our {IK} system can generate any pose, but prefers poses that are most similar to the space of poses in the training data. We represent the probability with a novel model called a Scaled Gaussian Process Latent Variable Model. The parameters of the model are all learned automatically; no manual tuning is required for the learning component of the system. We additionally describe a novel procedure for interpolating between styles.},
	pages = {10},
	author = {Grochow, Keith and Martin, Steven L and Hertzmann, Aaron and Popovic, Zoran},
	date = {2004},
	langid = {english},
	keywords = {tbr},
	file = {Grochow et al_2004_Style-Based Inverse Kinematics.pdf:/Users/wpq/Dropbox (MIT)/zotero/Grochow et al_2004_Style-Based Inverse Kinematics.pdf:application/pdf}
}

@article{vanderwilkLearningInvariancesUsing2018,
	title = {Learning Invariances using the Marginal Likelihood},
	url = {http://arxiv.org/abs/1808.05563},
	abstract = {Generalising well in supervised learning tasks relies on correctly extrapolating the training data to a large region of the input space. One way to achieve this is to constrain the predictions to be invariant to transformations on the input that are known to be irrelevant (e.g. translation). Commonly, this is done through data augmentation, where the training set is enlarged by applying hand-crafted transformations to the inputs. We argue that invariances should instead be incorporated in the model structure, and learned using the marginal likelihood, which correctly rewards the reduced complexity of invariant models. We demonstrate this for Gaussian process models, due to the ease with which their marginal likelihood can be estimated. Our main contribution is a variational inference scheme for Gaussian processes containing invariances described by a sampling procedure. We learn the sampling procedure by back-propagating through it to maximise the marginal likelihood.},
	journaltitle = {{arXiv}:1808.05563 [cs, stat]},
	author = {van der Wilk, Mark and Bauer, Matthias and John, S. T. and Hensman, James},
	urldate = {2021-02-25},
	date = {2018-08-16},
	eprinttype = {arxiv},
	eprint = {1808.05563},
	keywords = {tbr},
	file = {van der Wilk et al_2018_Learning Invariances using the Marginal Likelihood.pdf:/Users/wpq/Dropbox (MIT)/zotero/van der Wilk et al_2018_Learning Invariances using the Marginal Likelihood.pdf:application/pdf}
}

@article{ustyuzhaninovCompositionalUncertaintyDeep2020,
	title = {Compositional uncertainty in deep Gaussian processes},
	url = {http://arxiv.org/abs/1909.07698},
	abstract = {Gaussian processes ({GPs}) are nonparametric priors over functions. Fitting a {GP} implies computing a posterior distribution of functions consistent with the observed data. Similarly, deep Gaussian processes ({DGPs}) should allow us to compute a posterior distribution of compositions of multiple functions giving rise to the observations. However, exact Bayesian inference is intractable for {DGPs}, motivating the use of various approximations. We show that the application of simplifying mean-field assumptions across the hierarchy leads to the layers of a {DGP} collapsing to near-deterministic transformations. We argue that such an inference scheme is suboptimal, not taking advantage of the potential of the model to discover the compositional structure in the data. To address this issue, we examine alternative variational inference schemes allowing for dependencies across different layers and discuss their advantages and limitations.},
	journaltitle = {{arXiv}:1909.07698 [cs, stat]},
	author = {Ustyuzhaninov, Ivan and Kazlauskaite, Ieva and Kaiser, Markus and Bodin, Erik and Campbell, Neill D. F. and Ek, Carl Henrik},
	urldate = {2021-02-25},
	date = {2020-02-25},
	eprinttype = {arxiv},
	eprint = {1909.07698},
	keywords = {tbr},
	file = {Ustyuzhaninov et al_2020_Compositional uncertainty in deep Gaussian processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Ustyuzhaninov et al_2020_Compositional uncertainty in deep Gaussian processes.pdf:application/pdf}
}

@article{jankowiakParametricGaussianProcess2020,
	title = {Parametric Gaussian Process Regressors},
	url = {http://arxiv.org/abs/1910.07123},
	abstract = {The combination of inducing point methods with stochastic variational inference has enabled approximate Gaussian Process ({GP}) inference on large datasets. Unfortunately, the resulting predictive distributions often exhibit substantially underestimated uncertainties. Notably, in the regression case the predictive variance is typically dominated by observation noise, yielding uncertainty estimates that make little use of the input-dependent function uncertainty that makes {GP} priors attractive. In this work we propose two simple methods for scalable {GP} regression that address this issue and thus yield substantially improved predictive uncertainties. The first applies variational inference to {FITC} (Fully Independent Training Conditional; Snelson et.{\textasciitilde}al.{\textasciitilde}2006). The second bypasses posterior approximations and instead directly targets the posterior predictive distribution. In an extensive empirical comparison with a number of alternative methods for scalable {GP} regression, we find that the resulting predictive distributions exhibit significantly better calibrated uncertainties and higher log likelihoods--often by as much as half a nat per datapoint.},
	journaltitle = {{arXiv}:1910.07123 [cs, stat]},
	author = {Jankowiak, Martin and Pleiss, Geoff and Gardner, Jacob R.},
	urldate = {2021-02-26},
	date = {2020-12-26},
	eprinttype = {arxiv},
	eprint = {1910.07123},
	keywords = {read},
	file = {Jankowiak et al_2020_Parametric Gaussian Process Regressors.pdf:/Users/wpq/Dropbox (MIT)/zotero/Jankowiak et al_2020_Parametric Gaussian Process Regressors.pdf:application/pdf}
}

@inproceedings{miliosDirichletbasedGaussianProcesses2018a,
	location = {Red Hook, {NY}, {USA}},
	title = {Dirichlet-based Gaussian processes for large-scale calibrated classification},
	series = {{NIPS}'18},
	abstract = {This paper studies the problem of deriving fast and accurate classification algorithms with uncertainty quantification. Gaussian process classification provides a principled approach, but the corresponding computational burden is hardly sustainable in large-scale problems and devising efficient alternatives is a challenge. In this work, we investigate if and how Gaussian process regression directly applied to classification labels can be used to tackle this question. While in this case training is remarkably faster, predictions need to be calibrated for classification and uncertainty estimation. To this aim, we propose a novel regression approach where the labels are obtained through the interpretation of classification labels as the coefficients of a degenerate Dirichlet distribution. Extensive experimental results show that the proposed approach provides essentially the same accuracy and uncertainty quantification as Gaussian process classification while requiring only a fraction of computational resources.},
	pages = {6008--6018},
	booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates Inc.},
	author = {Milios, Dimitrios and Camoriano, Raffaello and Michiardi, Pietro and Rosasco, Lorenzo and Filippone, Maurizio},
	urldate = {2021-02-25},
	date = {2018-12-03},
	keywords = {read},
	file = {Milios et al_2018_Dirichlet-based Gaussian processes for large-scale calibrated classification.pdf:/Users/wpq/Dropbox (MIT)/zotero/Milios et al_2018_Dirichlet-based Gaussian processes for large-scale calibrated classification.pdf:application/pdf}
}

@article{kussAssessingApproximateInference2005,
	title = {Assessing Approximate Inference for Binary Gaussian Process Classiﬁcation},
	abstract = {Gaussian process priors can be used to deﬁne ﬂexible, probabilistic classiﬁcation models. Unfortunately exact Bayesian inference is analytically intractable and various approximation techniques have been proposed. In this work we review and compare Laplace’s method and Expectation Propagation for approximate Bayesian inference in the binary Gaussian process classiﬁcation model. We present a comprehensive comparison of the approximations, their predictive performance and marginal likelihood estimates to results obtained by {MCMC} sampling. We explain theoretically and corroborate empirically the advantages of Expectation Propagation compared to Laplace’s method.},
	pages = {26},
	author = {Kuss, Malte and Rasmussen, Carl Edward},
	date = {2005},
	langid = {english},
	keywords = {tbr},
	file = {Kuss and Rasmussen - Assessing Approximate Inference for Binary Gaussia.pdf:/Users/wpq/Zotero/storage/9TBH65RB/Kuss and Rasmussen - Assessing Approximate Inference for Binary Gaussia.pdf:application/pdf}
}

@article{al-shedivatLearningScalableDeep2017,
	title = {Learning Scalable Deep Kernels with Recurrent Structure},
	url = {http://arxiv.org/abs/1610.08936},
	abstract = {Many applications in speech, robotics, finance, and biology deal with sequential data, where ordering matters and recurrent structures are common. However, this structure cannot be easily captured by standard kernel functions. To model such structure, we propose expressive closed-form kernel functions for Gaussian processes. The resulting model, {GP}-{LSTM}, fully encapsulates the inductive biases of long short-term memory ({LSTM}) recurrent networks, while retaining the non-parametric probabilistic advantages of Gaussian processes. We learn the properties of the proposed kernels by optimizing the Gaussian process marginal likelihood using a new provably convergent semi-stochastic gradient procedure and exploit the structure of these kernels for scalable training and prediction. This approach provides a practical representation for Bayesian {LSTMs}. We demonstrate state-of-the-art performance on several benchmarks, and thoroughly investigate a consequential autonomous driving application, where the predictive uncertainties provided by {GP}-{LSTM} are uniquely valuable.},
	journaltitle = {{arXiv}:1610.08936 [cs, stat]},
	author = {Al-Shedivat, Maruan and Wilson, Andrew Gordon and Saatchi, Yunus and Hu, Zhiting and Xing, Eric P.},
	urldate = {2021-03-07},
	date = {2017-10-04},
	eprinttype = {arxiv},
	eprint = {1610.08936},
	keywords = {tbr},
	file = {Al-Shedivat et al_2017_Learning Scalable Deep Kernels with Recurrent Structure.pdf:/Users/wpq/Dropbox (MIT)/zotero/Al-Shedivat et al_2017_Learning Scalable Deep Kernels with Recurrent Structure.pdf:application/pdf}
}

@article{lazaro-gredillaSparseSpectrumGaussian2010,
	title = {Sparse Spectrum Gaussian Process Regression},
	abstract = {We present a new sparse Gaussian Process ({GP}) model for regression. The key novel idea is to sparsify the spectral representation of the {GP}. This leads to a simple, practical algorithm for regression tasks. We compare the achievable trade-offs between predictive accuracy and computational requirements, and show that these are typically superior to existing state-of-the-art sparse approximations. We discuss both the weight space and function space representations, and note that the new construction implies priors over functions which are always stationary, and can approximate any covariance function in this class.},
	pages = {17},
	author = {Lázaro-Gredilla, Miguel and Quiñonero-Candela, Joaquin and Rasmussen, Carl Edward and Figueiras-Vidal, Aníbal R},
	date = {2010},
	langid = {english},
	keywords = {tbr},
	file = {Lázaro-Gredilla et al. - Sparse Spectrum Gaussian Process Regression.pdf:/Users/wpq/Zotero/storage/VUSS9FMH/Lázaro-Gredilla et al. - Sparse Spectrum Gaussian Process Regression.pdf:application/pdf}
}

@article{hensmanGaussianProcessesBig2013,
	title = {Gaussian Processes for Big Data},
	url = {http://arxiv.org/abs/1309.6835},
	abstract = {We introduce stochastic variational inference for Gaussian process models. This enables the application of Gaussian process ({GP}) models to data sets containing millions of data points. We show how {GPs} can be vari- ationally decomposed to depend on a set of globally relevant inducing variables which factorize the model in the necessary manner to perform variational inference. Our ap- proach is readily extended to models with non-Gaussian likelihoods and latent variable models based around Gaussian processes. We demonstrate the approach on a simple toy problem and two real world data sets.},
	journaltitle = {{arXiv}:1309.6835 [cs, stat]},
	author = {Hensman, James and Fusi, Nicolo and Lawrence, Neil D.},
	urldate = {2021-03-07},
	date = {2013-09-26},
	eprinttype = {arxiv},
	eprint = {1309.6835},
	keywords = {good, read},
	file = {Hensman et al_2013_Gaussian Processes for Big Data.pdf:/Users/wpq/Dropbox (MIT)/zotero/Hensman et al_2013_Gaussian Processes for Big Data.pdf:application/pdf}
}

@article{buiUnifyingFrameworkGaussian2017,
	title = {A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation},
	url = {http://arxiv.org/abs/1605.07066},
	abstract = {Gaussian processes ({GPs}) are flexible distributions over functions that enable high-level assumptions about unknown functions to be encoded in a parsimonious, flexible and general way. Although elegant, the application of {GPs} is limited by computational and analytical intractabilities that arise when data are sufficiently numerous or when employing non-Gaussian models. Consequently, a wealth of {GP} approximation schemes have been developed over the last 15 years to address these key limitations. Many of these schemes employ a small set of pseudo data points to summarise the actual data. In this paper, we develop a new pseudo-point approximation framework using Power Expectation Propagation (Power {EP}) that unifies a large number of these pseudo-point approximations. Unlike much of the previous venerable work in this area, the new framework is built on standard methods for approximate inference (variational free-energy, {EP} and Power {EP} methods) rather than employing approximations to the probabilistic generative model itself. In this way, all of approximation is performed at `inference time' rather than at `modelling time' resolving awkward philosophical and empirical questions that trouble previous approaches. Crucially, we demonstrate that the new framework includes new pseudo-point approximation methods that outperform current approaches on regression and classification tasks.},
	journaltitle = {{arXiv}:1605.07066 [cs, stat]},
	author = {Bui, Thang D. and Yan, Josiah and Turner, Richard E.},
	urldate = {2021-03-07},
	date = {2017-10-05},
	eprinttype = {arxiv},
	eprint = {1605.07066},
	keywords = {tbr},
	file = {Bui et al_2017_A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation.pdf:/Users/wpq/Dropbox (MIT)/zotero/Bui et al_2017_A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation.pdf:application/pdf}
}

@article{tranVariationalGaussianProcess2016,
	title = {The Variational Gaussian Process},
	url = {http://arxiv.org/abs/1511.06499},
	abstract = {Variational inference is a powerful tool for approximate inference, and it has been recently applied for representation learning with deep generative models. We develop the variational Gaussian process ({VGP}), a Bayesian nonparametric variational family, which adapts its shape to match complex posterior distributions. The {VGP} generates approximate posterior samples by generating latent inputs and warping them through random non-linear mappings; the distribution over random mappings is learned during inference, enabling the transformed outputs to adapt to varying complexity. We prove a universal approximation theorem for the {VGP}, demonstrating its representative power for learning any model. For inference we present a variational objective inspired by auto-encoders and perform black box inference over a wide class of models. The {VGP} achieves new state-of-the-art results for unsupervised learning, inferring models such as the deep latent Gaussian model and the recently proposed {DRAW}.},
	journaltitle = {{arXiv}:1511.06499 [cs, stat]},
	author = {Tran, Dustin and Ranganath, Rajesh and Blei, David M.},
	urldate = {2021-03-07},
	date = {2016-04-17},
	eprinttype = {arxiv},
	eprint = {1511.06499},
	keywords = {tbr},
	file = {Tran et al_2016_The Variational Gaussian Process.pdf:/Users/wpq/Dropbox (MIT)/zotero/Tran et al_2016_The Variational Gaussian Process.pdf:application/pdf}
}

@article{leibfriedTutorialSparseGaussian,
	title = {A Tutorial on Sparse Gaussian Processes and Variational Inference},
	abstract = {Gaussian processes ({GPs}) provide a mathematically elegant framework for Bayesian inference and they can offer principled uncertainty estimates for a large range of problems. For example, if we consider certain regression problems with Gaussian likelihoods, a {GP} model enjoys a posterior in closed form. However, identifying the posterior {GP} scales cubically with the number of training examples and furthermore requires to store all training examples in memory. In order to overcome these practical obstacles, sparse {GPs} have been proposed that approximate the true posterior {GP} with a set of pseudo-training examples (a.k.a. inducing inputs or inducing points). Importantly, the number of pseudo-training examples is user-deﬁned and enables control over computational and memory complexity. In the general case, sparse {GPs} do not enjoy closed-form solutions and one has to resort to approximate inference. In this context, a convenient choice for approximate inference is variational inference ({VI}), where the problem of Bayesian inference is cast as an optimization problem—namely, to maximize a lower bound of the logarithm of the marginal likelihood. This paves the way for a powerful and versatile framework, where pseudo-training examples are treated as optimization arguments of the approximate posterior that are jointly identiﬁed together with hyperparameters of the generative model (i.e. prior and likelihood) in the course of training. The framework can naturally handle a wide scope of supervised learning problems, ranging from regression with heteroscedastic and non-Gaussian likelihoods to classiﬁcation problems with discrete labels, but also multilabel problems (where the regression or classiﬁcation targets are multidimensional). The purpose of this tutorial is to provide access to the basic matter for readers without prior knowledge in both {GPs} and {VI}. It turns out that a proper exposition to the subject enables also convenient access to more recent advances in the ﬁeld of {GPs} (like importance-weighted {VI} as well as inderdomain, multioutput and deep {GPs}) that can serve as an inspiration for exploring new research ideas.},
	pages = {45},
	author = {Leibfried, Felix and Dutordoir, Vincent and John, {ST} and Durrande, Nicolas},
	langid = {english},
	keywords = {tbr},
	file = {Leibfried et al. - A Tutorial on Sparse Gaussian Processes and Variat.pdf:/Users/wpq/Zotero/storage/JDV3DMQS/Leibfried et al. - A Tutorial on Sparse Gaussian Processes and Variat.pdf:application/pdf}
}

@article{galVariationalInferenceSparse2014,
	title = {Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models - a Gentle Tutorial},
	url = {http://arxiv.org/abs/1402.1412},
	abstract = {In this tutorial we explain the inference procedures developed for the sparse Gaussian process ({GP}) regression and Gaussian process latent variable model ({GPLVM}). Due to page limit the derivation given in Titsias (2009) and Titsias \& Lawrence (2010) is brief, hence getting a full picture of it requires collecting results from several different sources and a substantial amount of algebra to fill-in the gaps. Our main goal is thus to collect all the results and full derivations into one place to help speed up understanding this work. In doing so we present a re-parametrisation of the inference that allows it to be carried out in parallel. A secondary goal for this document is, therefore, to accompany our paper and open-source implementation of the parallel inference scheme for the models. We hope that this document will bridge the gap between the equations as implemented in code and those published in the original papers, in order to make it easier to extend existing work. We assume prior knowledge of Gaussian processes and variational inference, but we also include references for further reading where appropriate.},
	journaltitle = {{arXiv}:1402.1412 [stat]},
	author = {Gal, Yarin and van der Wilk, Mark},
	urldate = {2021-03-08},
	date = {2014-09-29},
	eprinttype = {arxiv},
	eprint = {1402.1412},
	keywords = {read},
	file = {Gal_van der Wilk_2014_Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models - a Gentle Tutorial.pdf:/Users/wpq/Dropbox (MIT)/zotero/Gal_van der Wilk_2014_Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models - a Gentle Tutorial.pdf:application/pdf}
}

@article{titsiasVariationalModelSelection2009,
	title = {Variational Model Selection for Sparse Gaussian Process Regression},
	abstract = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs are deﬁned to be variational parameters which are selected by minimizing the Kullback-Leibler divergence between the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.},
	pages = {20},
	author = {Titsias, Michalis K},
	date = {2009},
	langid = {english},
	keywords = {read},
	file = {Titsias_2009_Variational Model Selection for Sparse Gaussian Process Regression.pdf:/Users/wpq/Dropbox (MIT)/zotero/Titsias_2009_Variational Model Selection for Sparse Gaussian Process Regression.pdf:application/pdf}
}

@article{matthewsSparseVariationalMethods2015,
	title = {On Sparse variational methods and the Kullback-Leibler divergence between stochastic processes},
	url = {http://arxiv.org/abs/1504.07027},
	abstract = {The variational framework for learning inducing variables (Titsias, 2009a) has had a large impact on the Gaussian process literature. The framework may be interpreted as minimizing a rigorously defined Kullback-Leibler divergence between the approximating and posterior processes. To our knowledge this connection has thus far gone unremarked in the literature. In this paper we give a substantial generalization of the literature on this topic. We give a new proof of the result for infinite index sets which allows inducing points that are not data points and likelihoods that depend on all function values. We then discuss augmented index sets and show that, contrary to previous works, marginal consistency of augmentation is not enough to guarantee consistency of variational inference with the original model. We then characterize an extra condition where such a guarantee is obtainable. Finally we show how our framework sheds light on interdomain sparse approximations and sparse approximations for Cox processes.},
	journaltitle = {{arXiv}:1504.07027 [stat]},
	author = {Matthews, Alexander G. de G. and Hensman, James and Turner, Richard E. and Ghahramani, Zoubin},
	urldate = {2021-03-09},
	date = {2015-12-04},
	eprinttype = {arxiv},
	eprint = {1504.07027},
	keywords = {tbr},
	file = {Matthews et al_2015_On Sparse variational methods and the Kullback-Leibler divergence between stochastic processes.pdf:/Users/wpq/Dropbox (MIT)/zotero/Matthews et al_2015_On Sparse variational methods and the Kullback-Leibler divergence between stochastic processes.pdf:application/pdf}
}

@article{buiSparseApproximationsNonConjugate2014,
	title = {Sparse Approximations for Non-Conjugate Gaussian Process Regression},
	url = {https://thangbui.github.io/docs/reports/tr_sparseNonConj.pdf},
	abstract = {Notes: This report only shows some preliminary work on scaling Gaussian process models that use non-Gaussian likelihoods. As there are recently arxived papers on the similar idea [1, 2], this report will stay as is, please consult the two papers above for a proper discussion and experiments.},
	pages = {7},
	author = {Bui, Thang and Turner, Richard},
	date = {2014},
	langid = {english},
	keywords = {read},
	file = {Bui and Turner - Sparse Approximations for Non-Conjugate Gaussian P.pdf:/Users/wpq/Zotero/storage/HQ3JL6H8/Bui and Turner - Sparse Approximations for Non-Conjugate Gaussian P.pdf:application/pdf}
}

@article{SparseGaussianProcess2018,
	title = {Sparse Gaussian Process Approximations and Applications},
	url = {https://markvdw.github.io/vanderwilk-thesis.pdf},
	pages = {188},
	date = {2018},
	langid = {english},
	keywords = {read},
	file = {2018_Sparse Gaussian Process Approximations and Applications.pdf:/Users/wpq/Dropbox (MIT)/zotero/2018_Sparse Gaussian Process Approximations and Applications.pdf:application/pdf}
}